/****************************************************************************
*
*    Copyright (c) 2005 - 2019 by Vivante Corp.  All rights reserved.
*
*    The material in this file is confidential and contains trade secrets
*    of Vivante Corporation. This is proprietary information owned by
*    Vivante Corporation. No part of this work may be disclosed,
*    reproduced, copied, transmitted, or used in any way for any purpose,
*    without the express written permission of Vivante Corporation.
*
*****************************************************************************/


#include <VX/vx.h>
#ifdef WIN32
#include <dirent_win.h>
#else
#include <dirent.h>
#endif

#include <gc_vx_common.h>
#include <gc_vx_nn_extension_interface.h>
#include <gc_vx_internal_node_api.h>
#include "gc_hal_types.h"
#include "anchors.h"
#include <gc_vx_nn_util.h>
#include <gc_nn_arch_model.h>
#include <stdio.h>
#include <ops/gc_vx_ops.h>

#define QUANT8_SUPPORT 0

#define VX_GET_DATA_FROM_TENSOR(tensor, index) \
    vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(tensor), TENSOR_QUANT_TYPE(tensor), index, TENSOR_LOGICAL_ADDR(tensor), TENSOR_POS(tensor), TENSOR_TF_ZEROPOINT(tensor), TENSOR_TF_SCALE(tensor))

#define VX_SAVE_DATA_TO_TENSOR(tensor, data, index) \
    vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(tensor), TENSOR_QUANT_TYPE(tensor), index, data, TENSOR_LOGICAL_ADDR(tensor), TENSOR_POS(tensor), TENSOR_TF_ZEROPOINT(tensor), TENSOR_TF_SCALE(tensor), TENSOR_ROUNDING_MODE(tensor))

#define CHECK_LIFETIME_IS_STATIC(tensor) \
    (((tensor != VX_NULL) && TENSOR_DATA_LIFETIME(tensor) == VX_TENSOR_LIFE_TIME_STATIC) ? vx_true_e : vx_false_e)

vx_float32 vxnneActivation(vx_enum func_v, vx_float32 a_v, vx_float32 b_v, vx_float32 value);


#define vxmOPERATION_COUNT(layer)       gcmCOUNTOF(layer->operations)

#define IMG_MAX_WIDTH 65536

vx_status vxnneExecuteSWConvolution(vxnne_operation operation);


VX_PRIVATE_API vx_status vxnneGetTensorMemeory(vx_tensor tensor, vx_ptr_ptr ptr, vx_bool stage, vx_bool zero);

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDilationConvolutionLayerInitializer(vx_node node,
        vx_tensor inputs,
        vx_weights_biases_parameter weights_biases,
        vx_tensor weights,
        vx_tensor biases,
        vx_scalar padXLeft,
        vx_scalar padXRight,
        vx_scalar padYTop,
        vx_scalar padYBottom,
        vx_enum   padMode,
        vx_scalar padConstant,
        vx_scalar dilationX,
        vx_scalar dilationY,
        vx_scalar stridesX,
        vx_scalar stridesY,
        vx_scalar relu_s,
        vx_scalar pooling_s,
        vx_scalar poolingX,
        vx_scalar poolingY,
        vx_scalar downScaleSizeRounding,
        vx_tensor outputs);

VX_API_ENTRY vx_status VX_API_CALL vxReleaseWeightsBiasesParameter(vx_weights_biases_parameter *weights_bias)
{
    gcmDUMP_API("$VX vxReleaseWeightsBiasesParameter: weights_bias=%p", weights_bias);

    if ((*weights_bias) == VX_NULL)
    {
        vxError("%s[%d]: Weights_bias is invalid!\n", __FUNCTION__, __LINE__);
        return VX_ERROR_INVALID_PARAMETERS;
    }

    return vxoReference_Release((vx_reference_ptr)weights_bias, VX_TYPE_WEIGHTS_BIASES_PARAMETER, VX_REF_INTERNAL);
}

VX_API_ENTRY vx_weights_biases_parameter VX_API_CALL
vxCreateWeightsBiasesParameter(
    vx_context  context,
    vx_enum     layer_type,
    vx_uint32   num_of_dims,
    vx_uint32 * inputs_dims,
    vx_uint32   pad_x,
    vx_uint32   pad_y,
    vx_uint32   pooling_size_x,
    vx_uint32   pooling_size_y,
    vx_enum     down_scale_size_rounding,
    vx_uint32 * convolution_outputs_dims,
    vx_uint32 * pool_outputs_dims,
    vx_uint32   weights_num_of_dims,
    vx_uint32 * weights_dims,
    vx_enum     weights_data_format,
    vx_int8     weights_fixed_point_pos,
    vx_uint32   biases_num_of_dims,
    vx_uint32 * biases_dims,
    vx_enum     biases_data_format,
    vx_int8     biases_fixed_point_pos,
    vx_uint32   raw_data_size
    )
{
    vx_weights_biases_parameter wb;
    vx_uint32 padXLeft, padXRight, padYTop, padYBottom;

    gcmDUMP_API("$VX vxCreateWeightsBiasesParameter: context=%p, layer_type=0x%x, num_of_dims=0x%x, inputs_dims=%p, pad_x=0x%x, pad_y=0x%x, pooling_size_x=0x%x, pooling_size_y=0x%x,"\
        " down_scale_size_rounding=0x%x, convolution_outputs_dims=%p, pool_outputs_dims=%p, weights_num_of_dims=0x%x, weights_dims=%p, weights_data_format=0x%x, weights_fixed_point_pos=0x%x,"\
        " biases_num_of_dims=0x%x, biases_dims=%p, biases_data_format=0x%x, biases_fixed_point_pos=0x%x, raw_data_size=0x%x", context, layer_type, num_of_dims, inputs_dims, pad_x, pad_y, pooling_size_x, pooling_size_y,
        down_scale_size_rounding, convolution_outputs_dims, pool_outputs_dims, weights_num_of_dims, weights_dims, weights_data_format, weights_fixed_point_pos, biases_num_of_dims, biases_dims, biases_data_format, biases_fixed_point_pos, raw_data_size);

    if (!vxoContext_IsValid(context)) return VX_NULL;

    padXLeft = padXRight = pad_x;
    padYTop = padYBottom = pad_y;

    wb = _createWeightsBiasesParameterFromParams(
            context,
            layer_type,
            inputs_dims,
            padXLeft,
            padXRight,
            padYTop,
            padYBottom,
            pooling_size_x,
            pooling_size_y,
            down_scale_size_rounding,
            convolution_outputs_dims,
            pool_outputs_dims,
            weights_num_of_dims,
            weights_dims,
            weights_dims,
            weights_data_format,
            VX_QUANT_DYNAMIC_FIXED_POINT,
            weights_fixed_point_pos,
            biases_num_of_dims,
            biases_dims,
            biases_dims,
            biases_data_format,
            VX_QUANT_DYNAMIC_FIXED_POINT,
            biases_fixed_point_pos);

    return wb;
}

VX_API_ENTRY vx_weights_biases_parameter VX_API_CALL vxCreateWeightsBiasesParameterFromTensors(
    vx_enum     layer_type,
    vx_uint32   num_of_dims,
    vx_uint32 * inputs_dims,
    vx_uint32   pad_x,
    vx_uint32   pad_y,
    vx_uint32   pooling_size_x,
    vx_uint32   pooling_size_y,
    vx_enum     down_scale_size_rounding,
    vx_uint32 * convolution_outputs_dims,
    vx_uint32 * pool_outputs_dims,
    vx_weights_biases_parameter_optimizations_t *optimizations,
    vx_tensor   weights,
    vx_tensor   biases
    )
{
    vx_weights_biases_parameter wb;
    vx_context context = vxGetContext((vx_reference)weights);
    vx_uint32 padXLeft, padXRight, padYTop, padYBottom;
    vx_enum outType = TENSOR_DATA_TYPE(weights);

    gcmDUMP_API("$VX vxCreateWeightsBiasesParameterFromTensors: layer_type=0x%x, num_of_dims=0x%x, inputs_dims=%p, pad_x=0x%x, pad_y=0x%x, pooling_size_x=0x%x, pooling_size_y=0x%x,"\
        " down_scale_size_rounding=0x%x, convolution_outputs_dims=%p, pool_outputs_dims=%p, optimizations=%p, weights=%p, biases=%p",
        layer_type, num_of_dims, inputs_dims, pad_x, pad_y, pooling_size_x, pooling_size_y, down_scale_size_rounding, convolution_outputs_dims, pool_outputs_dims, optimizations, weights, biases);

    if (!vxoContext_IsValid(context)) return VX_NULL;


    padXLeft = padXRight = pad_x;
    padYTop = padYBottom = pad_y;

    if (optimizations)
        outType = optimizations->outputFormat;

    wb = _createWeightsBiasesParameterFromTensors(
            context,
            layer_type,
            inputs_dims,
            num_of_dims,
            num_of_dims,
            padXLeft,
            padXRight,
            padYTop,
            padYBottom,
            pooling_size_x,
            pooling_size_y,
            0,
            0,
            down_scale_size_rounding,
            convolution_outputs_dims,
            pool_outputs_dims,
            optimizations,
            outType,
            0,
            VX_TENSOR_RANK_WHCN,
            weights,
            biases,
            VX_NULL,
            vx_false_e,
            vx_true_e);

    return wb;
}

VX_API_ENTRY vx_weights_biases_parameter VX_API_CALL vxCreateWeightsBiasesParameterFromTensors2(
    vx_enum     layer_type,
    vx_uint32   num_of_dims,
    vx_uint32 * inputs_dims,
    vx_uint32 * convolution_outputs_dims,
    vx_uint32 * pool_outputs_dims,
    vx_enum     output_format,
    const vx_nn_convolution_relu_pooling_params convolution_relu_pooling_params,
    vx_size size_of_convlution_relu_pooling_params,
    vx_weights_biases_parameter_optimizations_t *optimizations,
    vx_tensor   weights,
    vx_tensor   biases)
{
    vx_weights_biases_parameter wb;
    vx_context context = vxGetContext((vx_reference)weights);
    vx_uint32 stride_x = 0;
    vx_uint32 stride_y = 0;
    vx_enum convert_format = 0;
    vx_enum rank_mode = VX_TENSOR_RANK_WHCN;
    vx_enum layer = layer_type;

    gcmDUMP_API("$VX vxCreateWeightsBiasesParameterFromTensors2: layer_type=0x%x, num_of_dims=0x%x, inputs_dims=%p, convolution_outputs_dims=%p, pool_outputs_dims=%p, output_format=0x%x,"\
        " convolution_relu_pooling_params=%p, size_of_convlution_relu_pooling_paramss=0x%lx, optimizations=%p, weights=%p, biases=%p",
        layer_type, num_of_dims, inputs_dims, convolution_outputs_dims, pool_outputs_dims, output_format, convolution_relu_pooling_params, size_of_convlution_relu_pooling_params, optimizations, weights, biases);


    if (!vxoContext_IsValid(context)) return VX_NULL;

    if (size_of_convlution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext_t))
    {
        vx_nn_convolution_relu_pooling_params_ext_t conv_ext = *((vx_nn_convolution_relu_pooling_params_ext_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext.stride_x;
        stride_y = conv_ext.stride_y;
    }
    else if (size_of_convlution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext2_t))
    {
        vx_nn_convolution_relu_pooling_params_ext2_t conv_ext2 = *((vx_nn_convolution_relu_pooling_params_ext2_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext2.ext.stride_x;
        stride_y = conv_ext2.ext.stride_y;
        rank_mode = conv_ext2.src_rank_mode;
        convert_format = conv_ext2.convert_dst_format;

        if (conv_ext2.depth_multiplier == 1 &&
            (TENSOR_DATA_TYPE(weights) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(weights) == VX_TYPE_UINT8) &&
            vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_NN_DEPTHWISE_SUPPORT))
            layer = VX_NN_DEPTH_WISE_CONVOLUTION_LAYER;
    }
    else if (size_of_convlution_relu_pooling_params != sizeof(vx_nn_convolution_relu_pooling_params_t))
    {
        vxError("Invalid parameter convolution_relu_pooling_params");
        return VX_NULL;
    }

    wb = _createWeightsBiasesParameterFromTensors(
            context,
            layer,
            inputs_dims,
            num_of_dims,
            num_of_dims,
            convolution_relu_pooling_params->pad_x_left,
            convolution_relu_pooling_params->pad_x_right,
            convolution_relu_pooling_params->pad_y_top,
            convolution_relu_pooling_params->pad_y_bottom,
            convolution_relu_pooling_params->pool_size_x,
            convolution_relu_pooling_params->pool_size_y,
            stride_x,
            stride_y,
            convolution_relu_pooling_params->down_scale_size_rounding,
            convolution_outputs_dims,
            pool_outputs_dims,
            optimizations,
            output_format,
            convert_format,
            rank_mode,
            weights,
            biases,
            VX_NULL,
            vx_false_e,
            vx_true_e);

    return wb;
}


VX_API_ENTRY vx_weights_biases_parameter VX_API_CALL vxCreateWeightsBiasesParameterFromTensors3(
    vx_enum     layer_type,
    vx_uint32 * inputs_dims,
    vx_uint32 * convolution_outputs_dims,
    vx_uint32 * pool_outputs_dims,
    const vx_nn_convolution_relu_pooling_params convolution_relu_pooling_params,
    vx_size size_of_convolution_relu_pooling_params,
    vx_weights_biases_parameter_optimizations_t *optimizations,
    vx_size size_of_optimizations,
    vx_tensor   weights,
    vx_tensor   biases)
{
    vx_weights_biases_parameter wb;
    vx_context context = vxGetContext((vx_reference)weights);
    vx_uint32 stride_x = 0;
    vx_uint32 stride_y = 0;
    vx_uint32 num_of_input_dims = 0;
    vx_uint32 num_of_output_dims = 0;
    vx_enum output_format = TENSOR_DATA_TYPE(weights);
    vx_enum convert_format = 0;
    vx_enum rank_mode = VX_TENSOR_RANK_WHCN;

    vx_enum layer = layer_type;

    gcmDUMP_API("$VX vxCreateWeightsBiasesParameterFromTensors3: layer_type=0x%x, inputs_dims=%p, "
        "convolution_outputs_dims=%p, pool_outputs_dims=%p, convolution_relu_pooling_params=%p, "
        "size_of_convolution_relu_pooling_params=0x%lx, optimizations=%p, size_of_optimizations=0x%lx, weights=%p, biases=%p",
        layer_type, inputs_dims, convolution_outputs_dims, pool_outputs_dims, convolution_relu_pooling_params, size_of_convolution_relu_pooling_params, optimizations, size_of_optimizations, weights, biases);

    if (!vxoContext_IsValid(context)) return VX_NULL;

    if (size_of_convolution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext_t))
    {
        vx_nn_convolution_relu_pooling_params_ext_t conv_ext = *((vx_nn_convolution_relu_pooling_params_ext_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext.stride_x;
        stride_y = conv_ext.stride_y;
    }
    else if (size_of_convolution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext2_t))
    {
        vx_nn_convolution_relu_pooling_params_ext2_t conv_ext2 = *((vx_nn_convolution_relu_pooling_params_ext2_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext2.ext.stride_x;
        stride_y = conv_ext2.ext.stride_y;
        rank_mode = conv_ext2.src_rank_mode;
        convert_format = conv_ext2.convert_dst_format;

        if (conv_ext2.depth_multiplier == 1 &&
            (TENSOR_DATA_TYPE(weights) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(weights) == VX_TYPE_UINT8) &&
            vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_NN_DEPTHWISE_SUPPORT))
            layer = VX_NN_DEPTH_WISE_CONVOLUTION_LAYER;
    }
    else if (size_of_convolution_relu_pooling_params != sizeof(vx_nn_convolution_relu_pooling_params_t))
    {
        vxError("Invalid parameter convolution_relu_pooling_params");
        return VX_NULL;
    }

    if (optimizations)
    {
        if (size_of_optimizations == sizeof(vx_weights_biases_parameter_optimizations_t))
            output_format = optimizations->outputFormat;
        else if (size_of_optimizations == sizeof(vx_weights_biases_parameter_optimizations_ext_t))
        {
            vx_weights_biases_parameter_optimizations_ext_t opt = *((vx_weights_biases_parameter_optimizations_ext_t *)optimizations);
            output_format = opt.outputFormat;
            num_of_input_dims = opt.num_of_input_dims;
            num_of_output_dims = opt.num_of_output_dims;
        }
        else
        {
            vxError("Invalid parameter convolution_relu_pooling_params");
            return VX_NULL;
        }
    }

    wb = _createWeightsBiasesParameterFromTensors(
            context,
            layer,
            inputs_dims,
            num_of_input_dims,
            num_of_output_dims,
            convolution_relu_pooling_params->pad_x_left,
            convolution_relu_pooling_params->pad_x_right,
            convolution_relu_pooling_params->pad_y_top,
            convolution_relu_pooling_params->pad_y_bottom,
            convolution_relu_pooling_params->pool_size_x,
            convolution_relu_pooling_params->pool_size_y,
            stride_x,
            stride_y,
            convolution_relu_pooling_params->down_scale_size_rounding,
            convolution_outputs_dims,
            pool_outputs_dims,
            optimizations,
            output_format,
            convert_format,
            rank_mode,
            weights,
            biases,
            VX_NULL,
            vx_false_e,
            vx_true_e);

    return wb;
}

VX_INTERNAL_API vx_weights_biases_parameter vxoCreateWeightsBiasesParameterFromTensorsPRelu(
    vx_enum     layer_type,
    vx_uint32 * inputs_dims,
    vx_uint32 * convolution_outputs_dims,
    vx_uint32 * pool_outputs_dims,
    const vx_nn_convolution_relu_pooling_params convolution_relu_pooling_params,
    vx_size size_of_convolution_relu_pooling_params,
    vx_weights_biases_parameter_optimizations_t *optimizations,
    vx_size size_of_optimizations,
    vx_tensor   weights,
    vx_tensor   biases,
    vx_tensor   alpha)
{
    vx_weights_biases_parameter wb;
    vx_context context = vxGetContext((vx_reference)weights);
    vx_uint32 stride_x = 0;
    vx_uint32 stride_y = 0;
    vx_uint32 num_of_input_dims = 0;
    vx_uint32 num_of_output_dims = 0;
    vx_enum output_format = TENSOR_DATA_TYPE(weights);
    vx_enum convert_format = 0;
    vx_enum rank_mode = VX_TENSOR_RANK_WHCN;

    vx_enum layer = layer_type;

    gcmDUMP_API("$VX vxCreateWeightsBiasesParameterFromTensors3: layer_type=0x%x, inputs_dims=%p, "
        "convolution_outputs_dims=%p, pool_outputs_dims=%p, convolution_relu_pooling_params=%p, "
        "size_of_convolution_relu_pooling_params=0x%lx, optimizations=%p, size_of_optimizations=0x%lx, weights=%p, biases=%p",
        layer_type, inputs_dims, convolution_outputs_dims, pool_outputs_dims, convolution_relu_pooling_params, size_of_convolution_relu_pooling_params, optimizations, size_of_optimizations, weights, biases);

    if (!vxoContext_IsValid(context)) return VX_NULL;

    if (size_of_convolution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext_t))
    {
        vx_nn_convolution_relu_pooling_params_ext_t conv_ext = *((vx_nn_convolution_relu_pooling_params_ext_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext.stride_x;
        stride_y = conv_ext.stride_y;
    }
    else if (size_of_convolution_relu_pooling_params == sizeof(vx_nn_convolution_relu_pooling_params_ext2_t))
    {
        vx_nn_convolution_relu_pooling_params_ext2_t conv_ext2 = *((vx_nn_convolution_relu_pooling_params_ext2_t*)(convolution_relu_pooling_params));
        stride_x = conv_ext2.ext.stride_x;
        stride_y = conv_ext2.ext.stride_y;
        rank_mode = conv_ext2.src_rank_mode;
        convert_format = conv_ext2.convert_dst_format;

        if (conv_ext2.depth_multiplier == 1 &&
            (TENSOR_DATA_TYPE(weights) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(weights) == VX_TYPE_UINT8) &&
            vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_NN_DEPTHWISE_SUPPORT))
            layer = VX_NN_DEPTH_WISE_CONVOLUTION_LAYER;
    }
    else if (size_of_convolution_relu_pooling_params != sizeof(vx_nn_convolution_relu_pooling_params_t))
    {
        vxError("Invalid parameter convolution_relu_pooling_params");
        return VX_NULL;
    }

    if (optimizations)
    {
        if (size_of_optimizations == sizeof(vx_weights_biases_parameter_optimizations_t))
            output_format = optimizations->outputFormat;
        else if (size_of_optimizations == sizeof(vx_weights_biases_parameter_optimizations_ext_t))
        {
            vx_weights_biases_parameter_optimizations_ext_t opt = *((vx_weights_biases_parameter_optimizations_ext_t *)optimizations);
            output_format = opt.outputFormat;
            num_of_input_dims = opt.num_of_input_dims;
            num_of_output_dims = opt.num_of_output_dims;
        }
        else
        {
            vxError("Invalid parameter convolution_relu_pooling_params");
            return VX_NULL;
        }
    }

    wb = _createWeightsBiasesParameterFromTensors(
            context,
            layer,
            inputs_dims,
            num_of_input_dims,
            num_of_output_dims,
            convolution_relu_pooling_params->pad_x_left,
            convolution_relu_pooling_params->pad_x_right,
            convolution_relu_pooling_params->pad_y_top,
            convolution_relu_pooling_params->pad_y_bottom,
            convolution_relu_pooling_params->pool_size_x,
            convolution_relu_pooling_params->pool_size_y,
            stride_x,
            stride_y,
            convolution_relu_pooling_params->down_scale_size_rounding,
            convolution_outputs_dims,
            pool_outputs_dims,
            optimizations,
            output_format,
            convert_format,
            rank_mode,
            weights,
            biases,
            alpha,
            vx_true_e,
            vx_true_e);

    return wb;
}

VX_API_ENTRY vx_status VX_API_CALL
vxMapWeightsBiasesParameter(
    vx_weights_biases_parameter     weights_biases,
    vx_map_id *                     map_id,
    vx_uint32 *                     stride,
    void **                         ptr,
    vx_enum                         usage,
    vx_enum                         mem_type,
    vx_uint32                       flags
    )
{
    gcmDUMP_API("$VX vxMapWeightsBiasesParameter: weights_biases=%p, map_id=%p, stride=%p, ptr=%p, usage=0x%x, mem_type=0x%x, flags=0x%x",
        weights_biases, map_id, stride, ptr, usage, mem_type, flags);

    if (!vxoWeightsBiasesParameter_IsValid(weights_biases)) return VX_ERROR_INVALID_REFERENCE;

    return vxoWeightsBiasesParameter_Map(weights_biases, map_id, stride, ptr, usage, mem_type, flags);
}

VX_API_ENTRY vx_status VX_API_CALL
vxUnmapWeightsBiasesParameter(
    vx_weights_biases_parameter     weights_biases,
    vx_map_id                       map_id
    )
{
    gcmDUMP_API("$VX vxUnmapWeightsBiasesParameter: weights_biases=%p, map_id=0x%x", weights_biases, map_id);

    if (!vxoWeightsBiasesParameter_IsValid(weights_biases)) return VX_ERROR_INVALID_REFERENCE;

    return vxoWeightsBiasesParameter_Unmap(weights_biases, map_id);
}

VX_API_ENTRY vx_status VX_API_CALL vxConfigTarget(
    vx_context context,
    vx_int32 dp_amount,
    vx_int32 mac_per_core,
    vx_int32 conv_cores,
    vx_int32 in_buffer_depth,
    vx_int32 accum_buffer_height,
    vx_int32 l2_cache_size,
    vx_int32 tp_cores
)
{
    gcmDUMP_API("$VX vxConfigTarget: context=%p, dp_amount=0x%x, mac_per_core=0x%x, conv_cores=0x%x, in_buffer_depth=0x%x, accum_buffer_height=0x%x, l2_cache_size=0x%x, tp_cores=0x%x",
        context, dp_amount, mac_per_core, conv_cores, in_buffer_depth, accum_buffer_height, l2_cache_size, tp_cores);

    if (!vxoContext_IsValid(context)) return VX_ERROR_INVALID_REFERENCE;

    context->nnConfig.fixedFeature.nnMadPerCore = mac_per_core;
    context->nnConfig.fixedFeature.nnCoreCount = conv_cores;
    context->nnConfig.fixedFeature.tpCoreCount = tp_cores;
    context->nnConfig.fixedFeature.nnInputBufferDepth = in_buffer_depth;
    context->nnConfig.fixedFeature.nnAccumBufferDepth = accum_buffer_height;
    context->nnConfig.derivedFeature.nnDPAmount = dp_amount;
    context->nnConfig.customizedFeature.vipSRAMSize = l2_cache_size;
    context->nnConfig.isSet = gcvTRUE;
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_uint32 vxoWeightsBiasesParameterStreamSize(
    vx_weights_biases_parameter weights_biases_parameter,
    vx_bool onlyHeaderStream
)
{
    vx_weights_biases_parameter wb = weights_biases_parameter;
    vx_uint32 bufferSize = 0;

    /*calculate stream buffer size*/
    if (onlyHeaderStream)
    {
        bufferSize = sizeof(vx_uint32) * 32
            + sizeof(vx_uint32) * VX_CONTEXT_TENSOR_MAX_DIMENSION * 3
            + sizeof(vx_uint32) * wb->slice_num * 10
            + sizeof(vx_uint32) * VX_CONTEXT_TENSOR_MAX_DIMENSION * 2;
    }
    else
    {
        bufferSize = (vx_uint32)WB_RAW_DATA_SIZE(wb)
            + sizeof(vx_uint32) * 32
            + sizeof(vx_uint32) * VX_CONTEXT_TENSOR_MAX_DIMENSION * 3
            + sizeof(vx_uint32) * wb->slice_num * 10
            + sizeof(vx_uint32) * VX_CONTEXT_TENSOR_MAX_DIMENSION * 2;
    }

    bufferSize += sizeof(vx_uint32);
    if (wb->sub_wb_vdata != VX_NULL)
    {
        bufferSize += sizeof(vx_uint32);
        bufferSize += sizeof(vx_uint32) * 1;
        bufferSize += (vx_uint32)wb->sub_wb_vdata->wb_memory_size;
        bufferSize += sizeof(vx_uint32) * 1;
        bufferSize += sizeof(vx_uint32) * wb->sub_wb_vdata->slice_num * 10;
    }

    bufferSize += sizeof(vx_uint32);
    if (weights_biases_parameter->archPerfHandle != VX_NULL) bufferSize += 89 * sizeof(vx_uint32);

    /* support multiVIP, save mGpuWBCount to stream*/
    bufferSize += sizeof(vx_uint32);

    return bufferSize;
}

VX_API_ENTRY vx_uint32* VX_API_CALL vxWeightsBiasesParameterToStream(
    vx_context context,
    vx_weights_biases_parameter weights_biases_parameter,
    vx_uint32 *weights_biases_stream_size,
    vx_bool onlyHeaderStream
)
{
    vx_weights_biases_parameter wb = weights_biases_parameter;
    vx_weights_biases_parameter_base wb_base = wb->wb_base;

    vx_uint32 bufferSize;
    vx_uint32 checkSize;
    vx_uint32 bitOffset;
    vx_uint32* kernelBufferPtr = NULL;
    vx_uint32* base = NULL;
    vx_uint32 i;

    gcmDUMP_API("$VX vxWeightsBiasesParameterToStream: context=%p, weights_biases_parameter=%p, weights_biases_stream_size=%p, onlyHeaderStream=0x%x",
        context, weights_biases_parameter, weights_biases_stream_size, onlyHeaderStream);

    gcmASSERT(context);

    bufferSize = vxoWeightsBiasesParameterStreamSize(wb, onlyHeaderStream);

    kernelBufferPtr = (vx_uint32*)vxAllocateAndZeroMemory(bufferSize);

    if (kernelBufferPtr == VX_NULL)
    {
        vxError("vxWeightsBiasesParameterToStream: OUT OF MEMORY");
        *weights_biases_stream_size = 0;
        return VX_NULL;
    }

    base = kernelBufferPtr;

    /* Write weights biases parameter to the head of data*/
    bitOffset = 0;

    /* wb base */
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pad_x_left, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pad_x_right, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pad_y_top, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pad_y_bottom, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pooling_size_x, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pooling_size_y, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->down_scale_size_rounding, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->weights_num_of_dims, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        if (i < wb_base->weights_num_of_dims)
        {
            writeBits(&kernelBufferPtr, &bitOffset, wb_base->weights_sizes[i], 32);
            writeBits(&kernelBufferPtr, &bitOffset, wb_base->org_weights_sizes[i], 32);
        }
        else
        {
            writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
            writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
        }
    }
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->weights_data_format, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->weights_quant_format, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->weights_fixed_point_pos, 32);

    writeBits(&kernelBufferPtr, &bitOffset, wb_base->biases_num_of_dims, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        if (i < wb_base->biases_num_of_dims)
        {
            writeBits(&kernelBufferPtr, &bitOffset, wb_base->biases_sizes[i], 32);
        }
        else
        {
            writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
        }
    }
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->biases_data_format, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->biases_quant_format, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->biases_fixed_point_pos, 32);

    /* wb */
    writeBits(&kernelBufferPtr, &bitOffset, wb->layer_type, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        if (i < WB_WEIGHT_DIM_NUM(wb))
        {
            writeBits(&kernelBufferPtr, &bitOffset, wb->weights_sizes[i], 32);
            writeBits(&kernelBufferPtr, &bitOffset, wb->biases_sizes[i], 32);
        }
        else
        {
            writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
            writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
        }
    }
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)WB_RAW_DATA_SIZE(wb), 32);

    writeBits(&kernelBufferPtr, &bitOffset, wb_base->strideX, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->strideY, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->pooling_stride, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->setZeroLength, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->inputZP, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->coefZP, 32);
    writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb_base->coefScale, 32);
    writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb_base->biasScale, 32);
    writeBits(&kernelBufferPtr, &bitOffset, wb_base->skipValue, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->nn_fc_batch_mode, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->do_fisrt_pixel_pool, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->do_1xN, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb_base->do_zdp_opt, 32);

    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->use_tp_fc, 32);
    writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_num, 32);

    for (i = 0; i < wb->slice_num; i++)
    {
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].kz_count, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].z_count, 32);

        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].memory_offset, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].memory_size, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].kernel_orig_size, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].kernel_stream_size, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->slice_array[i].kernel_align_stream_size, 32);

        writeBits(&kernelBufferPtr, &bitOffset, wb->slice_array[i].non_zero_count, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->slice_array[i].reserve_weight_count, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->slice_array[i].all_count, 32);
    }

    if (wb->sub_wb_vdata != VX_NULL)
    {
        writeBits(&kernelBufferPtr, &bitOffset, 1, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->wb_memory_size, 32);
        kernelBufferPtr++;
        memcpy(kernelBufferPtr, wb->sub_wb_vdata->wb_memory_ptr, wb->sub_wb_vdata->wb_memory_size);
        kernelBufferPtr = (vx_uint32*)((vx_uint8*)kernelBufferPtr + wb->sub_wb_vdata->wb_memory_size);
        bitOffset = 0; /*clear bitOffset flag after memcpy to fix the current kernelBufferPtr in next writeBits() */
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_num, 32);
        for (i = 0; i < wb->sub_wb_vdata->slice_num; i++)
        {
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].kz_count, 32);
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].z_count, 32);

            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].memory_offset, 32);
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].memory_size, 32);
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].kernel_orig_size, 32);
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].kernel_stream_size, 32);
            writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->slice_array[i].kernel_align_stream_size, 32);

            writeBits(&kernelBufferPtr, &bitOffset, wb->sub_wb_vdata->slice_array[i].non_zero_count, 32);
            writeBits(&kernelBufferPtr, &bitOffset, wb->sub_wb_vdata->slice_array[i].reserve_weight_count, 32);
            writeBits(&kernelBufferPtr, &bitOffset, wb->sub_wb_vdata->slice_array[i].all_count, 32);
        }
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->sub_wb_vdata->kernel_per_core, 32);
    }
    else
    {
        writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
    }

    /* for multiVIP */
    writeBits(&kernelBufferPtr, &bitOffset, wb->mGpuWBCount, 32);

    if (wb->archPerfHandle != VX_NULL)
    {
        writeBits(&kernelBufferPtr, &bitOffset, 1, 32);
        /* Copy arch perf data*/
        /*archPerfor base: 7*/
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->calculated, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->coefNonZeroRatio, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->coefNonZeroRatio + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->coefCompressRatio, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->coefCompressRatio + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->imageCompressRatio, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->imageCompressRatio + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->imageNonZeroRatio, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->imageNonZeroRatio + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->opType, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->opTarget, 32);

        /*archPerfor info: 27*/
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.kx, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.ky, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.kz, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.oinx, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.oiny, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.oinz, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.inx, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.iny, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.inz, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.outx, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.outy, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.outz, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.stridex, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.stridey, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.inputDataSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.outputDataSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.poolingSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.poolingStride, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.xOffSet, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.yOffSet, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.inputDataFormat, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.outputDataFormat, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.nnCores, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.convOutFifoDepth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.kernelSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.pix, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.piy, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.p3, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->info.nextKY, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->info.flush, 32);

        /*archPerfor swTilingInfo: 23*/
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.srcBuf, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.dstBuf, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.kernelBuf, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.cacheSpace, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.origInX, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.origInY, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.origOutX, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.origOutY, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.origOutZ, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.outImageStride, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->swTilingInfo.outImageSlice, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.kernelCacheMode, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.imageCacheMode, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.calcNonFirstCmd, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->swTilingInfo.isNonFirstComputeBottleNeck, 32);

        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstCycleCount, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstCycleCount + 1), 32);

        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstKernelReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstKernelReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstInImageReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstInImageReadBandWidth + 1), 32);

        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstWriteBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstWriteBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIWriteBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIWriteBandWidth + 1), 32);

        /*archPerfor resultInfo: 17*/
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->resultInfo.isFirstComputeBottleNeck, 32);
        writeBits(&kernelBufferPtr, &bitOffset, (vx_uint32)wb->archPerfHandle->resultInfo.calculated, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->resultInfo.kernelsPerCore, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->resultInfo.outImageTileXSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->resultInfo.outImageTileYSize, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->resultInfo.interleaveMode, 32);
        writeBits(&kernelBufferPtr, &bitOffset, wb->archPerfHandle->resultInfo.nnCoreCount, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfCycleCount, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfCycleCount + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfWriteBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfWriteBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIWriteBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIWriteBandWidth + 1), 32);

        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfKernelReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfKernelReadBandWidth + 1), 32);
        writeBits(&kernelBufferPtr, &bitOffset, *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfInImageReadBandWidth, 32);
        writeBits(&kernelBufferPtr, &bitOffset, *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfInImageReadBandWidth + 1), 32);
        kernelBufferPtr++;
    }
    else
    {
        writeBits(&kernelBufferPtr, &bitOffset, 0, 32);
        kernelBufferPtr++;
    }

    /* Copy kernel stream data*/
    if (onlyHeaderStream)
        checkSize = (vx_uint32)((vx_uint8*)kernelBufferPtr - (vx_uint8*)base);
    else
    {
        vxMemCopy(kernelBufferPtr, WB_MEM_LOGICAL_BASE_ADDR(wb), WB_RAW_DATA_SIZE(wb));
        checkSize = (vx_uint32)((vx_uint8*)kernelBufferPtr - (vx_uint8*)base) + (vx_uint32)WB_RAW_DATA_SIZE(wb);
    }

    if (checkSize != bufferSize)
    {
        vxmASSERT(0);
    }

    /* support multiVIP */
    if (wb->mGpuWBCount > 0)
    {
        vx_uint8* streamBufferPtr = VX_NULL;
        vx_uint8* streamBase = VX_NULL;
        vx_uint32 streamSize = bufferSize;
        vx_uint32 i = 0;

        /*1. calculate stream size and allocate memory for stream */
        for (i = 0; i < wb->mGpuWBCount; i++)
        {
            streamSize += vxoWeightsBiasesParameterStreamSize(wb->mGpuWBTable[i], vx_false_e);
        }
        streamBufferPtr = (vx_uint8*)vxAllocateAndZeroMemory(streamSize);
        if (streamBufferPtr == VX_NULL)
        {
            vxError("vxWeightsBiasesParameterToStream: OUT OF MEMORY, fail to allocate memory");
            *weights_biases_stream_size = 0;
            vxFree(base);
            return VX_NULL;
        }
        streamBase = streamBufferPtr;

        /*2. copy parent weight biase stream to streamBufferPtr */
        vxMemCopy(streamBufferPtr, base, bufferSize);
        vxFree(base);
        base = VX_NULL;
        streamBufferPtr += bufferSize;

        /*3. convert the weight_biase_parameter of multiVIP to stream */
        for (i = 0; i < wb->mGpuWBCount; i++)
        {
            vx_uint32 size = 0;
            vx_uint32 *buffer = VX_NULL;
            buffer = vxWeightsBiasesParameterToStream(context, wb->mGpuWBTable[i], &size, vx_false_e);
            vxmASSERT((buffer != VX_NULL) && (size > 0));
            vxMemCopy(streamBufferPtr, buffer, size);
            vxFree(buffer);
            buffer = VX_NULL;
            streamBufferPtr += size;

            vxmASSERT((vx_uint32)(streamBufferPtr - streamBase) <= streamSize);
        }

        *weights_biases_stream_size = streamSize;
        return (vx_uint32*)streamBase;
    }
    else
    {
        *weights_biases_stream_size = bufferSize;
        return base;
    }

}

VX_API_ENTRY vx_status VX_API_CALL vxFreeWeightsBiasesParameterStream(
    vx_uint32 *weights_biases_stream
)
{
    gcmDUMP_API("$VX vxFreeWeightsBiasesParameterStream: weights_biases_stream=%p", weights_biases_stream);
    if (weights_biases_stream != VX_NULL)
        vxFree(weights_biases_stream);
    weights_biases_stream = VX_NULL;

    return VX_SUCCESS;
}

#define DUMP_OFFLINE 0
VX_API_ENTRY vx_weights_biases_parameter VX_API_CALL vxCreateWeightsBiasesParameterFromStream(
    vx_context context,
    vx_uint32 * weights_biases_stream
)
{
    vx_weights_biases_parameter wb;
    vx_uint32 bitOffset = 0;
    vx_uint32* streamBufferPtr = weights_biases_stream;
    vx_uint8* streamBasePtr = (vx_uint8*)weights_biases_stream;
    vx_uint32 i;
    vx_status status = VX_SUCCESS;

    vx_enum     layerType;
    vx_uint32   poolingSizeX, poolingSizeY;
    vx_enum     downScaleSizeRounding;
    vx_uint32   weightsNumOfDims;
    vx_uint32   weightsDims[VX_CONTEXT_TENSOR_MAX_DIMENSION];
    vx_uint32   weightsOrgDims[VX_CONTEXT_TENSOR_MAX_DIMENSION];
    vx_uint32   weightsSubDims[VX_CONTEXT_TENSOR_MAX_DIMENSION];
    vx_enum     weightsDataFormat;
    vx_enum     weightsQuantFormat;
    vx_int8     weightsFixedPointPos;
    vx_uint32   biasesNumOfDims;
    vx_uint32   biasesDims[VX_CONTEXT_TENSOR_MAX_DIMENSION];
    vx_uint32   biasesSubDims[VX_CONTEXT_TENSOR_MAX_DIMENSION];
    vx_enum     biasesDataFormat;
    vx_enum     biasesQuantFormat;
    vx_int8     biasesFixedPointPos;
    vx_uint32   rawDataSize;
    vx_uint32   padXLeft, padXRight, padYTop, padYBottom;
    vx_uint32   sub_wb_memory_size;
#if DUMP_OFFLINE
    char fileName[1024];
    vx_uint32 * base = streamBufferPtr;
    FILE *outputFile = NULL;
    static vx_uint32 idx = 0;
    vx_uint32 size;
#endif
    gcmDUMP_API("$VX vxCreateWeightsBiasesParameterFromStream: context=%p, weights_biases_stream=%p", context, weights_biases_stream);

    if (!vxoContext_IsValid(context)) return VX_NULL;

    padXLeft = readBits(&streamBufferPtr, &bitOffset, 32);
    padXRight = readBits(&streamBufferPtr, &bitOffset, 32);
    padYTop = readBits(&streamBufferPtr, &bitOffset, 32);
    padYBottom = readBits(&streamBufferPtr, &bitOffset, 32);
    poolingSizeX = readBits(&streamBufferPtr, &bitOffset, 32);
    poolingSizeY = readBits(&streamBufferPtr, &bitOffset, 32);
    downScaleSizeRounding = readBits(&streamBufferPtr, &bitOffset, 32);
    weightsNumOfDims = readBits(&streamBufferPtr, &bitOffset, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        weightsDims[i] = readBits(&streamBufferPtr, &bitOffset, 32);
        weightsOrgDims[i] = readBits(&streamBufferPtr, &bitOffset, 32);
    }
    weightsDataFormat = readBits(&streamBufferPtr, &bitOffset, 32);
    weightsQuantFormat = readBits(&streamBufferPtr, &bitOffset, 32);
    weightsFixedPointPos = (vx_int8)readBits(&streamBufferPtr, &bitOffset, 32);
    biasesNumOfDims = readBits(&streamBufferPtr, &bitOffset, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        biasesDims[i] = readBits(&streamBufferPtr, &bitOffset, 32);
    }
    biasesDataFormat = readBits(&streamBufferPtr, &bitOffset, 32);
    biasesQuantFormat = readBits(&streamBufferPtr, &bitOffset, 32);
    biasesFixedPointPos = (vx_int8)readBits(&streamBufferPtr, &bitOffset, 32);

    layerType = readBits(&streamBufferPtr, &bitOffset, 32);
    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        weightsSubDims[i] = readBits(&streamBufferPtr, &bitOffset, 32);
        biasesSubDims[i] = readBits(&streamBufferPtr, &bitOffset, 32);
    }
    rawDataSize = readBits(&streamBufferPtr, &bitOffset, 32);

    wb = _createWeightsBiasesParameterFromParams(
            context,
            layerType,
            VX_NULL,
            padXLeft,
            padXRight,
            padYTop,
            padYBottom,
            poolingSizeX,
            poolingSizeY,
            downScaleSizeRounding,
            VX_NULL,
            VX_NULL,
            weightsNumOfDims,
            weightsDims,
            weightsSubDims,
            weightsDataFormat,
            weightsQuantFormat,
            weightsFixedPointPos,
            biasesNumOfDims,
            biasesDims,
            biasesSubDims,
            biasesDataFormat,
            biasesQuantFormat,
            biasesFixedPointPos);

    if (vxoReference_GetStatus((vx_reference)wb) != VX_SUCCESS) return VX_NULL;

    wb->wb_base->strideX = readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->strideY = readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->pooling_stride = readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->setZeroLength = (vx_int8)readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->inputZP = readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->coefZP = readBits(&streamBufferPtr, &bitOffset, 32);
    *(vx_uint32 *)&wb->wb_base->coefScale = readBits(&streamBufferPtr, &bitOffset, 32);
    *(vx_uint32 *)&wb->wb_base->biasScale = readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->skipValue = readBits(&streamBufferPtr, &bitOffset, 32);

    wb->wb_base->nn_fc_batch_mode = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->do_fisrt_pixel_pool = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->do_1xN = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
    wb->wb_base->do_zdp_opt = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);

    wb->use_tp_fc = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
    wb->slice_num = readBits(&streamBufferPtr, &bitOffset, 32);

    wb->slice_array = (vx_weights_biases_slice)vxAllocateAndZeroMemory(sizeof(vx_weights_biases_slice_s) * wb->slice_num);
    if (wb->slice_array == VX_NULL)
    {
        status = VX_ERROR_NO_MEMORY;
        goto exit;
    }

    for (i = 0; i < wb->slice_num; i++)
    {
        wb->slice_array[i].kz_count = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].z_count = readBits(&streamBufferPtr, &bitOffset, 32);

        wb->slice_array[i].memory_offset = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].memory_size = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].kernel_orig_size = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].kernel_stream_size = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].kernel_align_stream_size = readBits(&streamBufferPtr, &bitOffset, 32);

        wb->slice_array[i].non_zero_count = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].reserve_weight_count = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->slice_array[i].all_count = readBits(&streamBufferPtr, &bitOffset, 32);
    }

    if (!WeightBiasBufferAllocate(context, wb, rawDataSize))
    {
        status = VX_ERROR_NO_MEMORY;
        goto exit;
    }

    if (readBits(&streamBufferPtr, &bitOffset, 32) == 1)
    {
        sub_wb_memory_size = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->sub_wb_vdata = (vx_sub_wb_vdata)vxAllocateAndZeroMemory(sizeof(vx_sub_wb_vdata_s));
        if (wb->sub_wb_vdata == VX_NULL)
        {
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        wb->sub_wb_vdata->wb_memory_size = sub_wb_memory_size;
        if ((wb->sub_wb_vdata->wb_memory_ptr = (vx_uint8_ptr)vxAllocate(wb->sub_wb_vdata->wb_memory_size)) == VX_NULL)
        {
            status =  VX_ERROR_NO_MEMORY;
            goto exit;
        }
        streamBufferPtr++;
        memcpy(wb->sub_wb_vdata->wb_memory_ptr, streamBufferPtr, wb->sub_wb_vdata->wb_memory_size);
        streamBufferPtr = (vx_uint32*)((vx_uint8*)streamBufferPtr + wb->sub_wb_vdata->wb_memory_size);
        bitOffset = 0; /*clear bitOffset flag after memcpy to fix the current kernelBufferPtr in next readBits() */
        wb->sub_wb_vdata->slice_num = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->sub_wb_vdata->slice_array = (vx_weights_biases_slice)vxAllocateAndZeroMemory(sizeof(vx_weights_biases_slice_s) * wb->sub_wb_vdata->slice_num);
        if (wb->sub_wb_vdata->slice_array == VX_NULL)
        {
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }
        for (i = 0; i < wb->sub_wb_vdata->slice_num; i++)
        {
            wb->sub_wb_vdata->slice_array[i].kz_count = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].z_count = readBits(&streamBufferPtr, &bitOffset, 32);

            wb->sub_wb_vdata->slice_array[i].memory_offset = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].memory_size = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].kernel_orig_size = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].kernel_stream_size = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].kernel_align_stream_size = readBits(&streamBufferPtr, &bitOffset, 32);

            wb->sub_wb_vdata->slice_array[i].non_zero_count = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].reserve_weight_count = readBits(&streamBufferPtr, &bitOffset, 32);
            wb->sub_wb_vdata->slice_array[i].all_count = readBits(&streamBufferPtr, &bitOffset, 32);
        }
        wb->sub_wb_vdata->kernel_per_core = readBits(&streamBufferPtr, &bitOffset, 32);
    }

    /* support multiVIP */
    wb->mGpuWBCount = readBits(&streamBufferPtr, &bitOffset, 32);

    if (readBits(&streamBufferPtr, &bitOffset, 32) == 1)
    {
        wb->archPerfHandle = (vx_arch_perf)vxAllocateAndZeroMemory(sizeof(vx_arch_perf_s));
        if (wb->archPerfHandle == VX_NULL)
        {
            status =  VX_ERROR_NO_MEMORY;
            goto exit;
        }
        /*archPerfor base: 7*/
        wb->archPerfHandle->calculated = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->coefNonZeroRatio = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->coefNonZeroRatio + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->coefCompressRatio = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->coefCompressRatio + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->imageCompressRatio = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->imageCompressRatio + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->imageNonZeroRatio = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->imageNonZeroRatio + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->opType = (vxnne_operator_e)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->opTarget = (vxnne_operation_target_e)readBits(&streamBufferPtr, &bitOffset, 32);

        /*archPerfor info: 27*/
        wb->archPerfHandle->info.kx = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.ky = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.kz = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.oinx = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.oiny = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.oinz = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.inx = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.iny = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.inz = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.outx = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.outy = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.outz = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.stridex = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.stridey = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.inputDataSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.outputDataSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.poolingSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.poolingStride = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.xOffSet = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.yOffSet = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.inputDataFormat = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.outputDataFormat = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.nnCores = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.convOutFifoDepth = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.kernelSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.pix = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.piy = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.p3 = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.nextKY = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->info.flush = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);

        /*archPerfor swTilingInfo: 23*/
        wb->archPerfHandle->swTilingInfo.srcBuf = (vx_uint8)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.dstBuf = (vx_uint8)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.kernelBuf = (vx_uint8)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.cacheSpace = (vx_int32)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.origInX = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.origInY = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.origOutX = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.origOutY = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.origOutZ = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.outImageStride = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.outImageSlice = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.kernelCacheMode = (vx_enum)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.imageCacheMode = (vx_enum)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.calcNonFirstCmd = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->swTilingInfo.isNonFirstComputeBottleNeck = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstCycleCount = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstCycleCount + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstKernelReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstKernelReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstInImageReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstInImageReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstWriteBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstWriteBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIWriteBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->swTilingInfo.perfNonFirstAXIWriteBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);

        /*archPerfor resultInfo: 17*/
        wb->archPerfHandle->resultInfo.isFirstComputeBottleNeck = (vx_bool)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.calculated = (vx_uint8)readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.kernelsPerCore = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.outImageTileXSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.outImageTileYSize = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.interleaveMode = readBits(&streamBufferPtr, &bitOffset, 32);
        wb->archPerfHandle->resultInfo.nnCoreCount = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfCycleCount = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfCycleCount + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfWriteBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfWriteBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIWriteBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfAXIWriteBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfKernelReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfKernelReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        *(vx_uint32 *)&wb->archPerfHandle->resultInfo.perfInImageReadBandWidth = readBits(&streamBufferPtr, &bitOffset, 32);
        *((vx_uint32 *)&wb->archPerfHandle->resultInfo.perfInImageReadBandWidth + 1) = readBits(&streamBufferPtr, &bitOffset, 32);
        streamBufferPtr++;
        wb->non_zero_ratio = wb->archPerfHandle->coefNonZeroRatio;
        wb->general_compression_ratio = wb->archPerfHandle->coefCompressRatio;
    }
    else
        streamBufferPtr++;

    /* Copy kernel stream data*/
    memcpy(WB_MEM_LOGICAL_BASE_ADDR(wb), streamBufferPtr, WB_RAW_DATA_SIZE(wb));

#if gcdDUMP
    gcmDUMP(gcvNULL, "#[weights and biases]\n");
    gcmDUMP_BUFFER(gcvNULL,
                   gcvDUMP_BUFFER_MEMORY,
                   WB_MEM_PHYSICAL_BASE_ADDR(wb),
                   (gctPOINTER)WB_MEM_LOGICAL_BASE_ADDR(wb),
                   0,
                   WB_RAW_DATA_SIZE(wb));
#endif

#if DUMP_OFFLINE
    sprintf(fileName, "weightsBias_buffer_offline_lev%d.dat", idx++);
    outputFile = fopen(fileName,"wb");
    if (outputFile == NULL)
    {
        vxError("can't find file %s", fileName);
    }
    size = (vx_uint32)((vx_uint8*)streamBufferPtr - (vx_uint8*)base) + (vx_uint32)wb->sub_wb_memory_size;
    fwrite(base, size, 1, outputFile);
    fclose(outputFile);
#endif

    /* support multiVIP */
    if (wb->mGpuWBCount > 0)
    {
        vx_weights_biases_parameter WBTable[gcdMAX_3DGPU_COUNT];
        vx_uint32 i = 0;
        vx_uint32 size = vxoWeightsBiasesParameterStreamSize(wb, vx_false_e);
        vx_uint32 allocateSize = sizeof(vx_weights_biases_parameter) * wb->mGpuWBCount;
        /* this memeory be released in WeightsBiases_Destroy*/
        wb->mGpuWBTable = (vx_weights_biases_parameter*)vxAllocateAndZeroMemory((vx_size)allocateSize);

        streamBasePtr += size;
        vxmASSERT(wb->mGpuWBCount <= gcdMAX_3DGPU_COUNT);
        for (i = 0; i < wb->mGpuWBCount; i++)
        {
            WBTable[i] = vxCreateWeightsBiasesParameterFromStream(context, (vx_uint32*)(streamBasePtr));
            size = vxoWeightsBiasesParameterStreamSize(WBTable[i], vx_false_e);
            streamBasePtr += size;
        }

        for (i = 0; i < wb->mGpuWBCount; i++)
        {
            wb->mGpuWBTable[i] = WBTable[i];
        }
    }

exit:
    if (status == VX_SUCCESS)
        return wb;
    else
    {
        vxError("vxCreateWeightsBiasesParameterFromStream failed: status is 0x%x", status);
        return VX_NULL;
    }
}

vx_status vxnneLayer_Deinitialize(struct _vxnne_layer_s* layer)
{
    vx_uint32 i;

    for (i = 0; i < VX_MAX_TEMP_TENSORS; i++)
    {
        if (layer->temp_tensors[i] != VX_NULL)
        {
            vxoTensor_ReleaseTensor(&layer->temp_tensors[i]);
        }
    }

    for (i = 0; i < VX_MAX_TEMP_ARRAYS; i++)
    {
        if (layer->temp_arrays[i] != VX_NULL)
        {
            vxReleaseArray(&layer->temp_arrays[i]);
        }
    }

    for (i = 0; i < layer->num_operations; i++)
    {
        if (layer->operations[i]->deinitialize != VX_NULL)
        {
            layer->operations[i]->deinitialize(layer->operations[i]);
        }
    }

    return VX_SUCCESS;
}

vx_status vxnneLayer_Free(struct _vxnne_layer_s* layer)
{
    layer->deinitialize(layer);

    gcoOS_Free(gcvNULL, layer);

    return VX_SUCCESS;
}

vx_status vxnneLayer_Initialize(
    vxnne_layer                 layer,
    vx_char                     *name,
    vx_node                     node,
    vx_uint32                   max_num_operations,
    vxnne_operation             *operations,
    vxnne_layer_deinitialize_f  deinitialize
    )
{
    layer->name         = name;
    layer->node         = node;
    layer->operations   =  operations;
    layer->num_temp_tensors      = 0;
    layer->dump                  = VX_NULL;
    layer->deinitialize          = (deinitialize ? deinitialize :  vxnneLayer_Deinitialize);
    layer->num_operations        = 0;
    layer->max_num_operations    = max_num_operations;

    return VX_SUCCESS;
}

vx_status vxnneLayer_SetOperation(
    vxnne_layer layer,
    vxnne_operation operation,
    vx_uint32 index
    )
{
    vxmASSERT(index < layer->max_num_operations);
    if (layer->operations[index])
    {
        vxError("layer[%d] %dth operation is overwritten", layer->node->id, index);
    }
    if (layer->num_operations < (index + 1))
    {
        layer->num_operations = index + 1;
    }
    layer->operations[index] = operation;
    operation->id = index;

    return VX_SUCCESS;
}

vx_status vxnneOperation_Deinitialize(vxnne_operation_s *operation)
{

    return VX_SUCCESS;
}

vx_status vxnneOperation_TP_Deinitialize(vxnne_operation_s *operation)
{
    if (operation->parameter.data_buff != VX_NULL)
    {
        vxoTensor_ReleaseTensor(&operation->parameter.data_buff);
    }

    if (operation->parameter.tp_value != VX_NULL)
    {
        if (operation->parameter.tp_value->p8[0] != VX_NULL)
        {
            vxFree(operation->parameter.tp_value->p8[0]);
            operation->parameter.tp_value->p8[0] = VX_NULL;
        }

        vxFree(operation->parameter.tp_value);
        operation->parameter.tp_value = VX_NULL;
    }

    vxnneOperation_Deinitialize(operation);
    return VX_SUCCESS;
}

vx_status vxnneOperation_ConvolutionReluPooling_Deinitialize(vxnne_operation_s *operation)
{
    vxnne_convolution_relu_pooling_operation op = (vxnne_convolution_relu_pooling_operation)operation;

    if (op->reshape_weights_biases != VX_NULL)
    {
        vxReleaseWeightsBiasesParameter(&op->reshape_weights_biases);
    }

    if (op->sub_wb != VX_NULL)
    {
        vxReleaseWeightsBiasesParameter(&op->sub_wb);
    }

    if (op->reshape_inputs != VX_NULL)
    {
        vxoTensor_ReleaseTensor(&op->reshape_inputs);
    }

    if (op->reshape_outputs != VX_NULL)
    {
        vxoTensor_ReleaseTensor(&op->reshape_outputs);
    }

    if (op->swtWeightBiases != VX_NULL)
    {
        vxReleaseWeightsBiasesParameter(&op->swtWeightBiases);
    }

    vxnneOperation_Deinitialize(operation);
    return VX_SUCCESS;
}

vx_status vxnneOperation_DeConvoulutionNNE_Deinitialize(vxnne_operation_s *operation)
{
    vxnne_convolution_relu_pooling_operation op = (vxnne_convolution_relu_pooling_operation)operation;

    if (op->weights_biases != VX_NULL)
    {
        vxReleaseWeightsBiasesParameter(&op->weights_biases);
    }

    vxnneOperation_Deinitialize(operation);
    return VX_SUCCESS;
}

vx_status vxnneOperation_Initialize(
                vxnne_operation_s               *operation,
                vxnne_layer                     layer,
                vxnne_operation_target_e        target,
                vxnne_operator_e                operatorType,
                vxnne_operation_execute_f       execute,
                vxnne_operation_deinitialize_f  deinitialize,
                vx_uint32                       batchCount,
                vx_uint32                       cmdBuffSize
                )
{
    vx_context context       = layer->node->base.context;

    operation->layer         = layer;
    operation->target        = target;
    operation->operatorType = operatorType;
    operation->execute       = execute;
    operation->initialize    = VX_NULL;
    operation->deinitialize  = (deinitialize ? deinitialize :  vxnneOperation_Deinitialize);
    operation->dump          = VX_NULL;

    operation->inputs        = &operation->references[0];
    operation->outputs       = &operation->references[VX_MAX_OPERTAION_INPUTS_OUTPUTS];
    operation->generics      = &operation->references[VX_MAX_OPERTAION_PARAMETERS-VX_MAX_OPERTAION_GENERICS];

    if (target == VXNNE_OPERATION_TARGET_SW)
    {
        layer->hasCPUFunction = vx_true_e;
    }

    operation->batchCount = batchCount;

    gcmASSERT(operation->batchCount > 0);

    memset(&operation->parameter, 0, sizeof(vx_op_param_s));


    if (context->options.enablePrintOperaTarget)
    {
        vxInfo("layer name %s, operation type %s, operation target %s\n", layer->name, vxnneGetOperatorTypeName(operatorType), vxnneGetOperatorTargetName(target));
    }

    return VX_SUCCESS;
}

vx_status vxnneOperation_AddReference(
    vxnne_operation_s*            operation,
    vx_reference                  reference,
    vxnne_operation_reference_e   refType
    )
{
    if (reference == VX_NULL || (reference->type != VX_TYPE_TENSOR && reference->type != VX_TYPE_IMAGE))
    {
        return VX_FAILURE;
    }

    if (refType == VXNNE_OPERATION_REFENRENCE_ONETIME)
    {
        if (operation->onetimeRefsNum == VX_MAX_OPERTAION_GENERICS)
            return VX_ERROR_NO_RESOURCES;
        operation->onetimeRefs[operation->onetimeRefsNum++] = reference;
    }

    switch (refType)
    {
        case VXNNE_OPERATION_REFENRENCE_INPUT:
            if (operation->inputsNum == VX_MAX_OPERTAION_INPUTS_OUTPUTS)
                return VX_ERROR_NO_RESOURCES;
            operation->inputs[operation->inputsNum++] = reference;
            break;

        case VXNNE_OPERATION_REFENRENCE_OUTPUT:
            if (operation->outputsNum == VX_MAX_OPERTAION_INPUTS_OUTPUTS)
                return VX_ERROR_NO_RESOURCES;
            operation->outputs[operation->outputsNum++] = reference;
            break;

        case VXNNE_OPERATION_REFENRENCE_GENERIC:
            if (operation->genericNum == VX_MAX_OPERTAION_GENERICS)
                return VX_ERROR_NO_RESOURCES;
            operation->generics[operation->genericNum++] = reference;
            break;

        default:
            return VX_ERROR_NO_RESOURCES;
    }

    operation->refNum++;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneShaderOperation_Execute(vxnne_operation_s *operation)
{
    vx_status status;
    vx_uint32 i;
    vxnne_shader_operation shaderOperation  = (vxnne_shader_operation)operation;
    vx_shader kernelShader;
    vx_graph graph = shaderOperation->base.layer->node->graph;
    gctUINT8 *stateBuffer = VX_NULL;

    kernelShader = shaderOperation->shaderExecutable->kernelShader;

    vxmONERROR(vxoShader_SetParameters(kernelShader,
                                       shaderOperation->shaderExecutable->params != VX_NULL ?
                                           shaderOperation->shaderExecutable->params : shaderOperation->shaderExecutable->param,
                                       shaderOperation->shaderExecutable->paramNum,
                                       shaderOperation->shaderExecutable->datatypes != VX_NULL ?
                                           shaderOperation->shaderExecutable->datatypes : VX_NULL,
                                       shaderOperation->shaderExecutable->attribue));

    for(i = 0; i < shaderOperation->shaderExecutable->uniformCount; i++)
    {
        vxmONERROR(vxoShader_SetUniform(
                        kernelShader,
                        shaderOperation->shaderExecutable->uniforms[i].name,
                        shaderOperation->shaderExecutable->uniforms[i].count,
                        shaderOperation->shaderExecutable->uniforms[i].data));
    }

    if (graph->binarySave)
    {
        gctPOINTER pointer;
        vx_binary_save binarySave = graph->binarySave;
        vxmONERROR(gcoOS_Allocate(gcvNULL, VX_MAX_SH_OPERATION_STATE_SIZE, (gctPOINTER *)&pointer));
        stateBuffer = (gctUINT8_PTR)pointer;
        if (binarySave->waitCommandsSize > 0) {
            /* append wait commands */
            vxMemCopy(stateBuffer, binarySave->waitCommands, binarySave->waitCommandsSize);
        }
        status = gcfVX_CaptureState(stateBuffer + binarySave->waitCommandsSize,
                                    VX_MAX_SH_OPERATION_STATE_SIZE,
                                    gcvNULL,
                                    gcvTRUE, gcvFALSE);
        if (status != VX_SUCCESS)
        {
            vxError("fail to capture shader states\n");
            vxmONERROR(VX_FAILURE);
        }
    }

    status = vxoShader_Execute(shaderOperation->base.layer->node,
                               kernelShader,
                               &shaderOperation->shaderExecutable->borderMode,
                               &shaderOperation->shaderExecutable->shaderParam,
                               operation->currBatchIndex);

    if (graph->binarySave)
    {
        vx_node node = shaderOperation->base.layer->node;
        vx_reference *shParams = VX_NULL;
        gctUINT32 actualSize = 0;
        vx_binary_save binarySave = graph->binarySave;

        status = gcfVX_CaptureState(gcvNULL, 0, &actualSize, gcvFALSE, gcvFALSE);
        if (actualSize <= 0)
        {
            vxError("error: fail to save layer name : %s to binary in shader operation\n", node->layer->name);
            vxmONERROR(VX_FAILURE);
        }

        if (VXNNE_OPERATOR_USER_VXC == operation->operatorType)
        {
            shParams = shaderOperation->shaderExecutable->params;
        }
        else
        {
            shParams = shaderOperation->shaderExecutable->param;
        }
        vxmONERROR(vxoBinaryGraph_SaveShaderOperation(node, &shaderOperation->base, kernelShader,
                                                    shParams,
                                                    shaderOperation->shaderExecutable->paramNum,
                                                    stateBuffer, actualSize + binarySave->waitCommandsSize,
                                                    operation->currBatchIndex));
        binarySave->waitCommandsSize = 0;
    }

OnError:
    if (stateBuffer != VX_NULL)
    {
        gcmVERIFY_OK(gcmOS_SAFE_FREE(gcvNULL, stateBuffer));
    }

    return status;
}

vx_status vxnneShaderExecutable_Destroy(vxnne_shader_executable shaderExecutable)
{
    vx_uint32 i;

    for (i = 0; i < shaderExecutable->paramNum; i++)
    {
        if (shaderExecutable->param[i] != VX_NULL)
        {
            vxoReference_Release(&shaderExecutable->param[i], shaderExecutable->param[i]->type, VX_REF_INTERNAL);
        }
    }

    if (shaderExecutable->uniforms)
    {
        for(i = 0 ; i < shaderExecutable->uniformCount; i++)
        {
            gcoOS_Free(gcvNULL, shaderExecutable->uniforms[i].data);
        }

        gcoOS_Free(gcvNULL, shaderExecutable->uniforms);
    }

    gcoOS_Free(gcvNULL, shaderExecutable);

    return VX_SUCCESS;
}

vx_status vxnneShaderOperation_Deinitialize(vxnne_operation_s *operation)
{
    vxnne_shader_operation shader_operation = (vxnne_shader_operation)operation;
    if (shader_operation->shaderExecutable)
    {
        vxnneShaderExecutable_Destroy(shader_operation->shaderExecutable);

        shader_operation->shaderExecutable = VX_NULL;
    }

    return VX_SUCCESS;
}

vx_status vxnneShaderOperation_Initialize(
    vxnne_shader_operation_s            *operation,
    vxnne_layer                         layer,
    vxnne_operator_e                    operatorType,
    vx_uint32                           batchCount,
    vxnne_shader_executable             shaderExecutable
    )
{
    vx_context context              = layer->node->base.context;
    operation->base.layer           = layer;
    operation->base.dump            = VX_NULL;
    operation->base.execute         = vxnneShaderOperation_Execute;
    operation->base.operatorType    = operatorType;
    operation->base.deinitialize    = vxnneShaderOperation_Deinitialize;
    operation->base.target          = VXNNE_OPERATION_TARGET_SH;
    operation->base.batchCount      = batchCount;
    operation->shaderExecutable     = shaderExecutable;

    operation->base.inputs        = &operation->base.references[0];
    operation->base.outputs       = &operation->base.references[VX_MAX_OPERTAION_INPUTS_OUTPUTS];
    operation->base.generics      = &operation->base.references[VX_MAX_OPERTAION_PARAMETERS-VX_MAX_OPERTAION_GENERICS];

    gcmASSERT(operation->base.batchCount > 0);


    if (context->options.enablePrintOperaTarget)
    {
        vxInfo("layer name %s, operation type %s, operation target %s", layer->name, vxnneGetOperatorTypeName(operatorType), vxnneGetOperatorTargetName(VXNNE_OPERATION_TARGET_SH));
    }

    return VX_SUCCESS;
}


vx_status vxnneShaderExecutable_SetParameters(vxnne_shader_executable shaderExecutable, vx_reference parameters[], vx_uint32 paramNum)
{
    vx_uint32 i;

    if (paramNum > VX_MAX_SHADER_PARAMETERS) goto error;

    for (i = 0; i < paramNum; i++)
    {
        shaderExecutable->param[i] = parameters[i];
        vxoReference_Increment(shaderExecutable->param[i], VX_REF_INTERNAL);
    }

    shaderExecutable->paramNum = paramNum;

    return VX_SUCCESS;
error:
    return VX_FAILURE;
}

vx_status vxnneShaderExecutable_SetParametersEx(vxnne_shader_executable shaderExecutable, vx_reference parameters[], vx_enum datatypes[], vx_uint32 paramNum)
{
    shaderExecutable->params = parameters;
    shaderExecutable->datatypes = datatypes;
    shaderExecutable->paramNum = paramNum;

    return VX_SUCCESS;
}

vx_status vxnneShaderExecutable_SetParametersAttribute(vxnne_shader_executable shaderExecutable, vx_uint32 index, vx_bitfield attrib)
{
    if (index < VX_MAX_SHADER_PARAMETERS)
    {
        if (!(shaderExecutable->attribue[index] & VXNNE_SHADER_PARAMETERS_ATTRIBUTE_INPUT_BIT)
          && (attrib & VXNNE_SHADER_PARAMETERS_ATTRIBUTE_INPUT_BIT))
        {
            shaderExecutable->inputNum++;
        }

        if (!(shaderExecutable->attribue[index] & VXNNE_SHADER_PARAMETERS_ATTRIBUTE_OUTPUT_BIT)
          && (attrib & VXNNE_SHADER_PARAMETERS_ATTRIBUTE_OUTPUT_BIT))
        {
            shaderExecutable->outputNum++;
        }
        shaderExecutable->attribue[index] |= attrib;
        return VX_SUCCESS;
    }

    return VX_FAILURE;
}

vx_status vxnneShaderExecutable_SetParametersAttributes(vxnne_shader_executable shaderExecutable, vx_uint32 index[], vx_uint32 count, vx_bitfield attrib)
{
    vx_uint32 i;
    vx_status status;

    for (i = 0; i < count; i++)
    {
        status = vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, index[i], attrib);
        if (VX_FAILURE == status)
            return status;
    }

    return VX_SUCCESS;
}


vx_status vxnneShaderExecutable_SetExecutionParameters(vxnne_shader_executable shaderExecutable, vx_kernel_execution_parameters_t *shaderParam)
{
    vx_uint32 i;

    shaderExecutable->shaderParam = *shaderParam;

    for (i = 0; i < shaderExecutable->shaderParam.workDim; i++)
    {
        shaderExecutable->shaderParam.globalWorkScale[i] = gcmMAX(1, shaderExecutable->shaderParam.globalWorkScale[i]);
    }

    return VX_SUCCESS;
}

vx_status vxnneShaderExecutable_GetMaxWorkGroupSize(
    vxnne_shader_executable shaderExecutable,
    vx_uint32               *maxWorkGroupSize
    )
{
    if (shaderExecutable == NULL)
        return VX_FAILURE;

    *maxWorkGroupSize = (vx_uint32)shaderExecutable->kernelShader->maxWorkGroupSize;

    return VX_SUCCESS;
}

vx_status vxnneShaderExecutable_SetUniform(vxnne_shader_executable shaderExecutable, vx_char *name, vx_uint32 count, void * value)
{
    vx_uint32 size;
    vx_status vStatus = VX_FAILURE;
    gceSTATUS status;

    if (shaderExecutable->uniformCount >= shaderExecutable->kernelShader->numArgs) goto error;

    if (!shaderExecutable->uniforms)
    {
        /*allocat the maximum number uniforms */
        status = gcoOS_Allocate(gcvNULL, shaderExecutable->kernelShader->numArgs * gcmSIZEOF(vx_node_s), (gctPOINTER*)&shaderExecutable->uniforms);
        if (gcmIS_ERROR(status))
        {
            vStatus = VX_FAILURE;
            goto error;
        }
    }

    vStatus = vxoShader_GetUniformSize(shaderExecutable->kernelShader, name, &size);
    if (vStatus != VX_SUCCESS) goto error;

    status = gcoOS_Allocate(gcvNULL, size, (gctPOINTER*)&shaderExecutable->uniforms[shaderExecutable->uniformCount].data);
    if (gcmIS_ERROR(status))
    {
        vStatus = VX_FAILURE;
        goto error;
    }

    gcoOS_MemCopy(shaderExecutable->uniforms[shaderExecutable->uniformCount].data, value, size);

    shaderExecutable->uniforms[shaderExecutable->uniformCount].count = count;
    shaderExecutable->uniforms[shaderExecutable->uniformCount].name  = name;
    shaderExecutable->uniforms[shaderExecutable->uniformCount].size  = size;

    shaderExecutable->uniformCount++;

error:
    return vStatus;
}

vxnne_shader_executable  vxnneKernelShaders_CreateShaderExecutable(vxnne_kernel_shaders kernel, vx_char * subName, vx_border_mode_t *borderMode)
{
    vxnne_shader_executable shaderExecutable = VX_NULL;
    vx_char     kernelName[256]     = {0};
    vx_uint32   i, shaderID;

    gceSTATUS status = gcoOS_Allocate(gcvNULL, gcmSIZEOF(vxnne_shader_executable_s), (gctPOINTER*)&shaderExecutable);
    if (gcmIS_ERROR(status)) goto error;

    gcoOS_ZeroMemory((gctPOINTER)shaderExecutable, gcmSIZEOF(vxnne_shader_executable_s));

    shaderExecutable->borderMode = *borderMode;

    gcoOS_StrCopySafe(kernelName, 256, kernel->kernelName);

    if (subName)
    {
        gcoOS_StrCatSafe(kernelName, 256, subName);
    }

    for(i = 0; i < kernel->kernelShaderCount; i++)
    {
        if (gcoOS_StrCmp(kernel->kernelShader[i*2]->name, kernelName) == 0)
            break;
    }

    if (i == kernel->kernelShaderCount) goto error;

    shaderID = ((shaderExecutable->borderMode.mode == VX_BORDER_MODE_CONSTANT) ? 1 : 0);

    shaderExecutable->kernelShader = kernel->kernelShader[i*2 + shaderID];

    return shaderExecutable;

error:
    if (shaderExecutable) gcoOS_Free(gcvNULL, (gctPOINTER)shaderExecutable);

    return VX_NULL;
}

vxnne_kernel_shaders vxnneGetKernelShadersByEnum(vx_context context, vx_enum kernelEnum)
{
    if (context->kernels[kernelEnum].kernelShader)
    {
        return &context->kernels[kernelEnum];
    }
    else
    {
        return VX_NULL;
    }
}

vxnne_kernel_shaders vxnneAddKernelShadersInProgram(vx_context context, vx_char* kernelName, vx_program program, vx_uint32  paramNum, vx_enum kernelEnum)
{
    vxnne_kernel_shaders kernel = &context->kernels[kernelEnum];

    /* if exists then failed to add */
    if (kernel->kernelShader) return VX_NULL;

    kernel->kernelName  = kernelName;
    kernel->kernelEnum  = kernelEnum;
    kernel->paramNum    = paramNum;

    vxoKernel_CreateShaders(
            program,
            kernelName,
            &kernel->kernelShaderCount,
            &kernel->kernelShader);

    return kernel;
}

vx_status vxnneExecuteSWReshuffle(struct _vxnne_operation_s *operation)
{
    vxnne_reshuffle_operation           reshuffleOperation   = (vxnne_reshuffle_operation)operation;

    vx_tensor inputs = (vx_tensor)reshuffleOperation->inputs;
    vx_weights_biases_parameter weights_biases = (vx_weights_biases_parameter)reshuffleOperation->weights_biases;
    vx_enum   padMode = reshuffleOperation->pad_mode;
    vx_scalar padConst = reshuffleOperation->pad_const;
    vx_tensor outputs = (vx_tensor)reshuffleOperation->outputs;
    vx_uint32 stride_x, stride_y;
    vx_uint32 padXLeft;
    vx_uint32 padXRight;
    vx_uint32 padYTop;
    vx_uint32 padYBottom;
    void * padConstPtr = VX_NULL;
    vx_uint32 kx, ky;

    vx_status status = VX_SUCCESS;


    padXLeft   = reshuffleOperation->pad_x_left;
    padXRight  = reshuffleOperation->pad_x_right;
    padYTop    = reshuffleOperation->pad_y_top;
    padYBottom = reshuffleOperation->pad_y_bottom;

    stride_x = WB_STRIDE_X(weights_biases);
    stride_y = WB_STRIDE_Y(weights_biases);

    kx = WB_ORG_KERNEL_X(weights_biases);
    ky = WB_ORG_KERNEL_Y(weights_biases);

    padConstPtr = (void*)vxAllocateAndZeroMemory(sizeof(vx_int32));
    if (padConstPtr == NULL)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    if (padConst != VX_NULL)
    {
        vxReadScalarValue(padConst, padConstPtr);
        vxWriteScalarValue(padConst, padConstPtr);
    }

    *(vx_int32*)padConstPtr += TENSOR_PAD_ZERO_VALUE(inputs);

    /* if stride > 1, need do reshuffle with input buffer */
    gcmASSERT ((WB_STRIDE_X(weights_biases) > 1) || (WB_STRIDE_Y(weights_biases) > 1));

    {
        vxoNNExternsionDoReshuffle(
            operation->currBatchIndex,
            inputs,
            outputs,
            padXLeft,
            padXRight,
            padYTop,
            padYBottom,
            padMode,
            padConstPtr,
            stride_x,
            stride_y,
            kx,
            ky);
    }

    if (padConstPtr != VX_NULL)
    {
        vxFree(padConstPtr);
    }

    return status;
}

vx_status vxnneExecuteSWFullyConnected(struct _vxnne_operation_s *operation)
{
    vxnne_fully_connected_sw_operation           fullyConnectedOperation   = (vxnne_fully_connected_sw_operation)operation;

    vx_tensor inputs  = (vx_tensor)fullyConnectedOperation->inputs;
    vx_tensor weights = (vx_tensor)fullyConnectedOperation->weights;
    vx_tensor biases  = (vx_tensor)fullyConnectedOperation->biases;
    vx_tensor outputs = (vx_tensor)fullyConnectedOperation->outputs;
    gctPOINTER inputsBaseLogicalAddr = VX_NULL, outputsBaseLogicalAddr = VX_NULL;
    gctPOINTER weightsBaseLogicalAddr = VX_NULL, biasesBaseLogicalAddr = VX_NULL;
    vx_uint32 i = 0, j = 0, b = 0;
    vx_uint32 inputCount, outputCount;
    vx_float32 madValue, inputValue, weightValue, biasValue = 0.0f;
    vx_enum srcType, dstType, weightsType, biasesType, outputRoundingMode;
    vx_int8 inputFpPos = 0, weightFpPos = 0, biasFpPos = 0, outputFpPos = 0;
    vx_float32 result = 0.0f;
    vx_status status = VX_SUCCESS;
    vx_uint32 dims = TENSOR_VIEW_DIM_NUM(inputs);
    vx_uint32  width         = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32  height        = dims > 1 ? TENSOR_VIEW_SIZE_INDEX(inputs, 1) : 1;
    vx_uint32  depth         = dims > 2 ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
    vx_uint32  batch         = dims > 3 ? TENSOR_VIEW_SIZE_INDEX(inputs, 3) : 1;
    vx_uint32  batchCount    = 1;

    if (TENSOR_DIM_NUM(inputs) == 2)
    {
        batchCount = TENSOR_SIZE_INDEX(inputs, 1);
    }
    else if (TENSOR_DIM_NUM(inputs) == 4)
    {
        batchCount = TENSOR_SIZE_INDEX(inputs, 3);
    }

    srcType = TENSOR_DATA_TYPE(inputs);
    dstType = TENSOR_DATA_TYPE(outputs);
    weightsType = TENSOR_DATA_TYPE(weights);
    biasesType = TENSOR_DATA_TYPE(biases);

    vxoTensor_GetTensorViewMemory(inputs, &inputsBaseLogicalAddr, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputsBaseLogicalAddr, VX_NULL);
    vxoTensor_GetTensorViewMemory(weights, &weightsBaseLogicalAddr, VX_NULL);
    vxoTensor_GetTensorViewMemory(biases, &biasesBaseLogicalAddr, VX_NULL);

    inputCount = (vx_uint32)(width * height * depth * batch) / batchCount;
    dims          = TENSOR_VIEW_DIM_NUM(outputs);
    width         = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    height        = dims > 1 ? TENSOR_VIEW_SIZE_INDEX(outputs, 1) : 1;
    depth         = dims > 2 ? TENSOR_VIEW_SIZE_INDEX(outputs, 2) : 1;
    batch         = dims > 3 ? TENSOR_VIEW_SIZE_INDEX(outputs, 3) : 1;
    outputCount = (vx_uint32)(width * height * depth * batch) / batchCount;

    inputFpPos = TENSOR_POS(inputs);
    weightFpPos = TENSOR_POS(weights);
    biasFpPos = TENSOR_POS(biases);
    outputFpPos = TENSOR_POS(outputs);
    outputRoundingMode = TENSOR_ROUNDING_MODE(outputs);

    for (b = 0; b < batchCount; b ++)
    {
        for (i = 0; i < outputCount; i++)
        {
            madValue = 0.0;
            for (j = 0; j < inputCount; j++)
            {
                if (((srcType == VX_TYPE_FLOAT16) && (weightsType == VX_TYPE_FLOAT16) && (biasesType == VX_TYPE_FLOAT32)) ||
                    ((srcType == VX_TYPE_FLOAT32) && (weightsType == VX_TYPE_FLOAT32) && (biasesType ==  VX_TYPE_FLOAT32)) ||
                    ((srcType == VX_TYPE_INT8) && (weightsType == VX_TYPE_INT8) && (biasesType == VX_TYPE_INT32 || biasesType == VX_TYPE_FLOAT32)) ||
                    ((srcType == VX_TYPE_INT16) && (weightsType == VX_TYPE_INT16) && (biasesType == VX_TYPE_INT32 || biasesType == VX_TYPE_FLOAT32)))
                {
                    inputValue  = vxnneGetDataExt((vx_type_e)srcType, TENSOR_QUANT_TYPE(inputs), j + b * inputCount, (vx_uint8_ptr)inputsBaseLogicalAddr, inputFpPos, TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                    weightValue = vxnneGetDataExt((vx_type_e)weightsType, TENSOR_QUANT_TYPE(weights), inputCount * i + j, (vx_uint8_ptr)weightsBaseLogicalAddr, weightFpPos, TENSOR_TF_ZEROPOINT(weights), TENSOR_TF_SCALE(weights));

                    madValue += inputValue * weightValue;
                }
                else if((srcType == VX_TYPE_UINT8) && (weightsType == VX_TYPE_UINT8) && (biasesType == VX_TYPE_INT32 || biasesType == VX_TYPE_FLOAT32) && TENSOR_QUANT_TYPE(inputs) == VX_QUANT_AFFINE_SCALE)
                {
                    inputValue  = vxnneGetDataQuant((vx_type_e)srcType, j + b * inputCount, (vx_uint8_ptr)inputsBaseLogicalAddr, TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                    weightValue = vxnneGetDataQuant((vx_type_e)weightsType, inputCount * i + j, (vx_uint8_ptr)weightsBaseLogicalAddr, TENSOR_TF_ZEROPOINT(weights), TENSOR_TF_SCALE(weights));

                    madValue += inputValue * weightValue;
                }
                else
                {
                    /* other format not surpport now */
                    vxError("can't support this input data format\n");
                    gcmASSERT(0);
                }
            }

            if (biasesType == VX_TYPE_FLOAT32 || biasesType == VX_TYPE_INT32)
            {
                biasValue = vxnneGetDataExt((vx_type_e)biasesType, TENSOR_QUANT_TYPE(biases), i, (vx_uint8_ptr)biasesBaseLogicalAddr, biasFpPos, TENSOR_TF_ZEROPOINT(biases), TENSOR_TF_SCALE(biases));
            }
            else
            {
                vxError("can't support this bias data format\n");
                gcmASSERT(0);
            }

            result = madValue + biasValue;

            vxnneSaveDataExt((vx_type_e)dstType, TENSOR_QUANT_TYPE(outputs), i + b * outputCount, result, outputsBaseLogicalAddr, outputFpPos, TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), outputRoundingMode);
        }
    }

    return status;
}

vx_status vxnneExecuteSWActivation(struct _vxnne_operation_s *operation)
{
    vxnne_activation_sw_operation           activationOperation   = (vxnne_activation_sw_operation)operation;

    vx_tensor inputs  = (vx_tensor)activationOperation->inputs;
    vx_scalar func = (vx_scalar)activationOperation->func;
    vx_scalar a  = (vx_scalar)activationOperation->a;
    vx_scalar b = (vx_scalar)activationOperation->b;
    vx_tensor outputs = (vx_tensor)activationOperation->outputs;

    vx_enum   func_v = func->value->e;
    vx_int32  a_v = a->value->n32;
    vx_int32  b_v = b->value->n32;

    vx_uint32 elementCount = 0;
    vx_uint32 i;
    vx_float32 value = 0.0f, result = 0.0f;
    gctPOINTER inputBase;
    gctPOINTER outputBase;


    vx_status status = VX_SUCCESS;

    elementCount = (vx_uint32)vxoMemory_ComputeElementCount(&inputs->tensorBuffer->memory, 0);
    vxoTensor_GetTensorViewMemory(inputs, &inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputBase, VX_NULL);

    for (i = 0; i < elementCount; i++)
    {
        value = VX_GET_DATA_FROM_TENSOR(inputs, i);

        switch (func_v)
        {
        case VX_NN_ACTIVATION_LOGISTIC:
            {
                result = 1.0f / (1 + gcoMATH_Exp(value * (-1)));
            }
            break;

        case VX_NN_ACTIVATION_HYPERBOLIC_TAN:
            {
                result = a_v * gcoMATH_TangentH(b_v * value);
            }
            break;

        case VX_NN_ACTIVATION_RELU:
            {
                result = gcoMATH_MAX(0.0f, value);
            }
            break;

        case VX_NN_ACTIVATION_BRELU:
            {
                result = gcoMATH_MIN(a_v, gcoMATH_MAX(0.0f, value));
            }
            break;

        case VX_NN_ACTIVATION_SOFTRELU:
            {
                result = gcoMATH_Log(1 + gcoMATH_Exp(value));
            }
            break;

        case VX_NN_ACTIVATION_ABS:
            {
                result = gcoMATH_Absolute(value);
            }
            break;

        case VX_NN_ACTIVATION_SQUARE:
            {
                result = gcoMATH_Power(value, 2);
            }
            break;

        case VX_NN_ACTIVATION_SQRT:
            {
                result = gcoMATH_SquareRoot(value);
            }
            break;

        case VX_NN_ACTIVATION_LINEAR:
            {
                result = a_v * value + b_v;
            }
            break;

        case VX_NN_ACTIVATION_LEAKYRELU:
            {
                result = (value > 0.0f) ? value : 0.1f * value;
            }
            break;

        case VX_NN_ACTIVATION_RELU6:
            {
                result = gcoMATH_MIN(gcoMATH_MAX(value, 0), 6);
            }
            break;

        case VX_NN_ACTIVATION_RELU1:
            {
                result = gcoMATH_MIN(gcoMATH_MAX(value, -1), 1);
            }
            break;

        case VX_NN_ACTIVATION_RSQRT:
            {
                result = gcoMATH_ReciprocalSquareRoot(value);
            }
            break;

        default:
            vxError("this activation func not support");
            status = VX_ERROR_NOT_SUPPORTED;
            return status;
        }

        VX_SAVE_DATA_TO_TENSOR(outputs, result, i);
    }

    return status;

}

vx_status vxnneExecuteSWSoftmax(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_softmax_operation softmaxOperation   = (vxnne_softmax_operation)operation;

    vx_tensor  input           = softmaxOperation->inputs;
    vx_tensor  output          = softmaxOperation->outputs;
    vx_scalar  betas           = softmaxOperation->beta;

    vx_type_e  input_format    = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e  output_format   = (vx_type_e)TENSOR_DATA_TYPE(output);

    vx_int8    input_fp        = TENSOR_POS(input);
    vx_int8    output_fp       = TENSOR_POS(output);
    vx_enum    outputRMode     = TENSOR_ROUNDING_MODE(output);
    vx_float32 beta;

    vx_uint32 width,height,channel,dims;
    vx_uint32 i,c,index,Dim,ItemCount;
    vx_float32_ptr p_pfProSum, pfProSum = NULL;
    vx_float32_ptr p_pfMax, pfMax = NULL;
    vx_float32_ptr p_pfProbFP32, pfProbFP32 = VX_NULL;
    vx_float32_ptr p_pfInput, pfInput = VX_NULL;
    vx_uint8_ptr input_data_ptr = NULL;
    vx_uint8_ptr output_data_ptr = NULL;

    vxoTensor_GetTensorBatchArrayViewMemory(input, operation->currBatchIndex, (gctPOINTER *)&input_data_ptr, VX_NULL);
    vxoTensor_GetTensorBatchArrayViewMemory(output, operation->currBatchIndex, (gctPOINTER *)&output_data_ptr, VX_NULL);

    dims = TENSOR_DIM_NUM(input);
    switch(dims)
    {
        case 1:
            channel = TENSOR_VIEW_SIZE_INDEX(input, 0);
            width   = 1;
            height  = 1;
            break;
        case 2:
            channel = TENSOR_VIEW_SIZE_INDEX(input, 0);
            width   = 1;
            height  = 1;
            break;
        case 3:
            width   = TENSOR_VIEW_SIZE_INDEX(input, 0);
            height  = TENSOR_VIEW_SIZE_INDEX(input, 1);
            channel = TENSOR_VIEW_SIZE_INDEX(input, 2);
            break;
        case 4:
            width   = TENSOR_VIEW_SIZE_INDEX(input, 0);
            height  = TENSOR_VIEW_SIZE_INDEX(input, 1);
            channel = TENSOR_VIEW_SIZE_INDEX(input, 2);
            break;
        default:
            vxError("Input tensor error dimension[%u]\n", dims);
            return VX_ERROR_INVALID_DIMENSION;
    }

    ItemCount = width * height;
    Dim       = channel * width * height; /* default axis = 3, so softmax it in channel */
    pfMax       = (vx_float32_ptr)vxAllocateAndZeroMemory(ItemCount * sizeof(vx_float32));
    pfProSum    = (vx_float32_ptr)vxAllocateAndZeroMemory(ItemCount * sizeof(vx_float32));
    pfInput     = (vx_float32_ptr)vxAllocateAndZeroMemory(channel * ItemCount * sizeof(vx_float32));
    pfProbFP32  = (vx_float32_ptr)vxAllocateAndZeroMemory(channel * ItemCount * sizeof(vx_float32));

    index = 0;
    p_pfInput = pfInput;
    p_pfMax = pfMax;

    for(c = 0; c < channel; c++)
    {
        for(i = 0; i < ItemCount; i++)
        {
            index = c * ItemCount + i;
            p_pfInput[i] = vxnneGetDataExt(input_format, TENSOR_QUANT_TYPE(input), index, (vx_uint8_ptr)input_data_ptr, input_fp, TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input));
            p_pfMax[i]   = gcmMAX(p_pfMax[i], p_pfInput[i]);
        }
        p_pfInput += ItemCount;
    }
    p_pfMax += ItemCount;


    p_pfProbFP32 = pfProbFP32;
    p_pfInput = pfInput;
    p_pfMax = pfMax;
    p_pfProSum = pfProSum;

    for(c = 0; c < channel; c++)
    {
        for(i = 0; i < ItemCount; i++)
        {
            if (betas != VX_NULL)
            {
                beta = betas->value->f32;
                p_pfProbFP32[i] = gcoMATH_Exp((p_pfInput[i] - p_pfMax[i]) * beta);
            }
            else
                p_pfProbFP32[i] = gcoMATH_Exp(p_pfInput[i] - p_pfMax[i]);
        }
        p_pfProbFP32 += ItemCount;
        p_pfInput += ItemCount;
    }

    p_pfProbFP32 = pfProbFP32;
    p_pfProSum = pfProSum;

    for(c = 0; c < channel; c++)
    {
        for(i = 0; i < ItemCount; i++)
        {
            index = c * ItemCount + i;
            p_pfProSum[i] += pfProbFP32[index];
        }
    }
    p_pfProSum += ItemCount;


    p_pfProbFP32 = pfProbFP32;
    p_pfProSum = pfProSum;
    index = 0;

    for(c = 0; c < channel; c++)
    {
        for(i = 0; i < ItemCount; i++)
        {
            vx_float32 div = p_pfProbFP32[i] / p_pfProSum[i];


            vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(output), index++, div, (vx_uint8_ptr)output_data_ptr, output_fp, TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), outputRMode);
        }
        p_pfProbFP32 += ItemCount;
    }
    p_pfProSum += ItemCount;

    if(pfMax)
    {
        vxFree(pfMax);
    }

    if(pfProSum)
    {
        vxFree(pfProSum);
    }

    if(pfInput)
    {
        vxFree(pfInput);
    }

    if(pfProbFP32)
    {
        vxFree(pfProbFP32);
    }
    return status;
}

vx_status vxnneExecuteSWConcat2(struct _vxnne_operation_s *operation)
{
    vxnne_concat2_sw_operation           concatOperation   = (vxnne_concat2_sw_operation)operation;

    vx_tensor input0  = (vx_tensor)concatOperation->inputs0;
    vx_tensor input1  = (vx_tensor)concatOperation->inputs1;
    vx_tensor output = (vx_tensor)concatOperation->outputs;
    vx_type_e   input0_format = (vx_type_e)TENSOR_DATA_TYPE(input0);
    vx_type_e   input1_format = (vx_type_e)TENSOR_DATA_TYPE(input1);
    vx_type_e   output_format = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_int8   src0FixPointPos  = TENSOR_POS(input0);
    vx_int8   src1FixPointPos  = TENSOR_POS(input1);
    vx_int8   dstFixPointPos  = TENSOR_POS(output);
    vx_enum   dstRoundingMode = TENSOR_ROUNDING_MODE(output);
    vx_uint32  m = 0;
    vx_uint32  index = 0;

    vx_uint8_ptr pInput0Buf = NULL;
    vx_uint8_ptr pInput1Buf = NULL;
    vx_uint8_ptr pOutputBuf = NULL;

    vx_uint32 input0Size = 0, input1Size = 0;

    vxoTensor_GetTensorViewMemory(input0, (gctPOINTER *)&pInput0Buf, VX_NULL);
    vxoTensor_GetTensorViewMemory(input1, (gctPOINTER *)&pInput1Buf, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER *)&pOutputBuf, VX_NULL);

    if (input0->isViewed)
    {
        vxmASSERT(vxoTensor_GetTensorSize(input0, &input0Size) == VX_SUCCESS);
    }
    else
    {
        input0Size = (vx_uint32)vxoMemory_ComputeSize(&input0->tensorBuffer->memory, 0);
    }

    if (input1->isViewed)
    {
        vxmASSERT(vxoTensor_GetTensorSize(input1, &input1Size) == VX_SUCCESS);
    }
    else
    {
        input1Size = (vx_uint32)vxoMemory_ComputeSize(&input1->tensorBuffer->memory, 0);
    }

    if((input0_format == VX_TYPE_FLOAT16 && input1_format == VX_TYPE_FLOAT16 && output_format == VX_TYPE_FLOAT16)
    ||(input0_format == VX_TYPE_INT8 && input1_format == VX_TYPE_INT8 && output_format == VX_TYPE_INT8 && src0FixPointPos == src1FixPointPos && src0FixPointPos == dstFixPointPos))
    {
        vxMemCopy(pOutputBuf, pInput0Buf, input0Size);
        vxMemCopy(&pOutputBuf[input0Size], pInput1Buf, input1Size);
    }
    else
    {
        input0Size = input0Size / TENSOR_DATA_SIZE(input0);
        input1Size = input1Size / TENSOR_DATA_SIZE(input1);
        for (m = 0; m < input0Size; m ++, index ++)
        {
            vx_float32 src0 = vxnneGetDataExt(input0_format, TENSOR_QUANT_TYPE(input0), m, (vx_uint8_ptr)pInput0Buf, src0FixPointPos, TENSOR_TF_ZEROPOINT(input0), TENSOR_TF_SCALE(input0));
            vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(output), index, src0, (vx_uint8_ptr)pOutputBuf, dstFixPointPos, TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), dstRoundingMode);
        }

        for (m = 0; m < input1Size; m ++, index ++)
        {
            vx_float32 src1 = vxnneGetDataExt(input1_format, TENSOR_QUANT_TYPE(input1), m, (vx_uint8_ptr)pInput1Buf, src1FixPointPos, TENSOR_TF_ZEROPOINT(input1), TENSOR_TF_SCALE(input1));
            vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(output), index, src1, (vx_uint8_ptr)pOutputBuf, dstFixPointPos, TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), dstRoundingMode);
        }
    }

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorCopy(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_copy_sw_operation           copyOperation   = (vxnne_tensor_copy_sw_operation)operation;

    vx_tensor src  = (vx_tensor)copyOperation->src;
    vx_tensor dst = (vx_tensor)copyOperation->dst;
    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_uint32 dstSize, srcSize, copySize;
    vx_uint32 dstCount, srcCount, copyCount;
    vx_int8 srcFp = TENSOR_POS(src);
    vx_int8 dstFp = TENSOR_POS(dst);

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    if (TENSOR_DATA_TYPE(src) == TENSOR_DATA_TYPE(dst) && srcFp == dstFp)
    {
        vxoTensor_GetTensorSize(src,&srcSize);
        vxoTensor_GetTensorSize(dst,&dstSize);

        copySize = gcmMIN(srcSize, dstSize);
        memcpy(dstLogical, srcLogical, copySize);
    }
    else
    {
        vx_uint32 i;
        vx_float32 src0 = 0;

        if (src->isViewed)
        {
            srcCount = 1;

            for (i = 0; i < TENSOR_VIEW_DIM_NUM(src); i++)
            {
                srcCount *= TENSOR_VIEW_SIZE_INDEX(src, i);
            }
        }
        else
            vxoTensor_GetTensorElementCount(src, &srcCount);

        if (dst->isViewed)
        {
            dstCount = 1;

            for (i = 0; i < TENSOR_VIEW_DIM_NUM(dst); i++)
            {
                dstCount *= TENSOR_VIEW_SIZE_INDEX(dst, i);
            }
        }
        else
            vxoTensor_GetTensorElementCount(dst, &dstCount);

        copyCount = gcmMIN(srcCount, dstCount);

        for (i = 0; i < copyCount; i++)
        {
            src0 = VX_GET_DATA_FROM_TENSOR(src, i);

            VX_SAVE_DATA_TO_TENSOR(dst, src0, i);

        }
    }

    return VX_SUCCESS;
}

/* Greatest Common Divisor*/
VX_PRIVATE_API vx_bool vxoGetDataDivisors(vx_uint32 input_value, vx_uint32 *divisors, vx_uint32 gcd)
{
    vx_uint32 i                 = 0;

    for (i = gcmMIN(input_value, IMG_MAX_WIDTH - 1); i > 0; i --)
    {
        if ((i % gcd == 0) && (input_value % i == 0))
        {
            *divisors = i;

            return vx_true_e;
        }
    }

    return vx_false_e;
}

VX_PRIVATE_API vx_bool vxoElementOptimization_GetTensorShape(vx_tensor input, vx_uint32 sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION], vx_uint32 * num_of_dims)
{
    vx_status status            = VX_SUCCESS;
    vx_uint32 element_count     = 0;
    vx_uint32 i                 = 0;

    status = vxoTensor_GetTensorElementCount(input, &element_count);
    if (status != VX_SUCCESS) return vx_false_e;

    for (i = 0; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
    {
        sizes[i] = 1;
    }

    if (element_count < IMG_MAX_WIDTH)
    {
        sizes[0] = element_count;

        *num_of_dims = 2;
    }
    else
    {
        vx_uint32 divisors = 1;
        for (i = 0; i < 2; i++)
        {
            divisors = 1;
            vxoGetDataDivisors(element_count, &divisors, 1);

            sizes[i] = divisors;
            element_count = element_count / divisors;
        }

        sizes[2] = element_count;
        *num_of_dims = 3;
    }

    return vx_true_e;
}

vx_status CPUTensorReverse(vx_tensor src, vx_tensor dst, vx_uint32 axis)
{
    vx_type_e srcFormat = (vx_type_e)TENSOR_DATA_TYPE(src);
    vx_enum srcQuantFormat = TENSOR_QUANT_TYPE(src);
    vx_int8 srcFp = TENSOR_POS(src);
    vx_int32 srcZp = TENSOR_TF_ZEROPOINT(src);
    vx_float32 srcScale = TENSOR_TF_SCALE(src);

    vx_type_e dstFormat = (vx_type_e)TENSOR_DATA_TYPE(dst);
    vx_enum dstQuantFormat = TENSOR_QUANT_TYPE(dst);
    vx_int8 dstFp = TENSOR_POS(dst);
    vx_int32 dstZp = TENSOR_TF_ZEROPOINT(dst);
    vx_float32 dstScale = TENSOR_TF_SCALE(dst);
    vx_enum   dstRoundingMode = TENSOR_ROUNDING_MODE(dst);

    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_uint32 batch = src->dimCount < 4 ? 1 : TENSOR_VIEW_SIZE_INDEX(src, 3);
    vx_uint32 channel = src->dimCount < 3 ? 1 : TENSOR_VIEW_SIZE_INDEX(src, 2);
    vx_uint32 height = src->dimCount < 2 ? 1 : TENSOR_VIEW_SIZE_INDEX(src, 1);
    vx_uint32 width = TENSOR_VIEW_SIZE_INDEX(src, 0);

    vx_uint32 i, j, m, n;

    vxmASSERT(TENSOR_VIEW_SIZE_INDEX(src, 0) == TENSOR_VIEW_SIZE_INDEX(dst, 0));
    vxmASSERT(axis < src->dimCount);

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    switch (axis)
    {
    case 0:
        for (n = 0; n < batch; n++)
        {
            for (m = 0; m < channel; m++)
            {
                for (j = 0; j < height; j++)
                {
                    for (i = 0; i < width; i++)
                    {
                        vx_uint32 srcIdx = n * channel * height * width + m * height * width + j * width + i;
                        vx_uint32 dstIdx = n * channel * height * width + m * height * width + j * width + width - i - 1;

                        vx_float32 value = vxnneGetDataExt(srcFormat, srcQuantFormat, srcIdx, (vx_uint8_ptr)srcLogical, srcFp, srcZp, srcScale);
                        vxnneSaveDataExt(dstFormat, dstQuantFormat, dstIdx, value, (vx_uint8_ptr)dstLogical, dstFp, dstZp, dstScale, dstRoundingMode);
                    }
                }
            }
        }
        break;

    case 1:
        for (n = 0; n < batch; n++)
        {
            for (m = 0; m < channel; m++)
            {
                for (j = 0; j < height; j++)
                {
                    for (i = 0; i < width; i++)
                    {
                        vx_uint32 srcIdx = n * channel * height * width + m * height * width + j * width + i;
                        vx_uint32 dstIdx = n * channel * height * width + m * height * width + (height - j - 1) * width + i;

                        vx_float32 value = vxnneGetDataExt(srcFormat, srcQuantFormat, srcIdx, (vx_uint8_ptr)srcLogical, srcFp, srcZp, srcScale);
                        vxnneSaveDataExt(dstFormat, dstQuantFormat, dstIdx, value, (vx_uint8_ptr)dstLogical, dstFp, dstZp, dstScale, dstRoundingMode);
                    }
                }
            }
        }
        break;

    case 2:
        for (n = 0; n < batch; n++)
        {
            for (m = 0; m < channel; m++)
            {
                for (j = 0; j < height; j++)
                {
                    for (i = 0; i < width; i++)
                    {
                        vx_uint32 srcIdx = n * channel * height * width + m * height * width + j * width + i;
                        vx_uint32 dstIdx = n * channel * height * width + (channel - m - 1) * height * width + j * width + i;

                        vx_float32 value = vxnneGetDataExt(srcFormat, srcQuantFormat, srcIdx, (vx_uint8_ptr)srcLogical, srcFp, srcZp, srcScale);
                        vxnneSaveDataExt(dstFormat, dstQuantFormat, dstIdx, value, (vx_uint8_ptr)dstLogical, dstFp, dstZp, dstScale, dstRoundingMode);
                    }
                }
            }
        }
        break;

    case 3:
        for (n = 0; n < batch; n++)
        {
            for (m = 0; m < channel; m++)
            {
                for (j = 0; j < height; j++)
                {
                    for (i = 0; i < width; i++)
                    {
                        vx_uint32 srcIdx = n * channel * height * width + m * height * width + j * width + i;
                        vx_uint32 dstIdx = (batch - n - 1) * channel * height * width + m  * height * width + j * width + i;

                        vx_float32 value = vxnneGetDataExt(srcFormat, srcQuantFormat, srcIdx, (vx_uint8_ptr)srcLogical, srcFp, srcZp, srcScale);
                        vxnneSaveDataExt(dstFormat, dstQuantFormat, dstIdx, value, (vx_uint8_ptr)dstLogical, dstFp, dstZp, dstScale, dstRoundingMode);
                    }
                }
            }
        }
        break;

    default:
        vxError("Tensor Reverse: invalid axis");
        return VX_ERROR_INVALID_VALUE;
    }

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorReverse(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_reverse_sw_operation reverseOperation = (vxnne_tensor_reverse_sw_operation)operation;

    vx_tensor input = reverseOperation->input;
    vx_tensor output = reverseOperation->output;
    vx_uint32 numOfAxis = reverseOperation->numOfAxis;

    vx_context ctx = vxGetContext((vx_reference)input);
    vx_graph graph = input->tensorBuffer->memory.graph;

    vx_tensor tmp[2] = {VX_NULL};
    vx_uint32 sizes[4] = {0};
    vx_status status = VX_SUCCESS;
    vx_uint32 i;
    vx_tensor_create_params_t tensor_create_params;

    for (i = 0; i < 4; i++)
    {
        sizes[i] = TENSOR_VIEW_SIZE_INDEX(input, i);
    }

    gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
    tensor_create_params.num_of_dims = TENSOR_DIM_NUM(input);
    tensor_create_params.sizes = sizes;
    tensor_create_params.data_format = TENSOR_DATA_TYPE(input);
    tensor_create_params.quant_format = TENSOR_QUANT_TYPE(input);
    if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
    {
        tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(input);
    }
    else
    {
        tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(input);
        tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input);
    }

    switch (numOfAxis)
    {
    case 1:
        status = CPUTensorReverse(input, output, ((vx_scalar)reverseOperation->axis[0])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        break;

    case 2:
        tmp[0] = vxoTensor_CreateTensor(ctx, graph, &tensor_create_params, vx_false_e);
        vxoTensor_AllocateMemory(tmp[0]);

        status = CPUTensorReverse(input, tmp[0], ((vx_scalar)reverseOperation->axis[0])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[0], output, ((vx_scalar)reverseOperation->axis[1])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        break;

    case 3:
        tmp[0] = vxoTensor_CreateTensor(ctx, graph, &tensor_create_params, vx_false_e);
        vxoTensor_AllocateMemory(tmp[0]);
        tmp[1] = vxoTensor_CreateTensor(ctx, graph, &tensor_create_params, vx_false_e);
        vxoTensor_AllocateMemory(tmp[1]);

        status = CPUTensorReverse(input, tmp[0], ((vx_scalar)reverseOperation->axis[0])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[0], tmp[1], ((vx_scalar)reverseOperation->axis[1])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[1], output, ((vx_scalar)reverseOperation->axis[2])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        break;

    case 4:
        tmp[0] = vxoTensor_CreateTensor(ctx, graph, &tensor_create_params, vx_false_e);
        vxoTensor_AllocateMemory(tmp[0]);
        tmp[1] = vxoTensor_CreateTensor(ctx, graph, &tensor_create_params, vx_false_e);
        vxoTensor_AllocateMemory(tmp[1]);

        status = CPUTensorReverse(input, tmp[0], ((vx_scalar)reverseOperation->axis[0])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[0], tmp[1], ((vx_scalar)reverseOperation->axis[1])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[1], tmp[0], ((vx_scalar)reverseOperation->axis[2])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        status = CPUTensorReverse(tmp[0], output, ((vx_scalar)reverseOperation->axis[3])->value->n32);
        if (status != VX_SUCCESS)
            goto OnError;

        break;
    default:
        vxError("Tensor Reverse: invalid axis");
        return VX_ERROR_INVALID_VALUE;
    }

OnError:
    if (tmp[0] != VX_NULL)
        vxoTensor_ReleaseTensor(&tmp[0]);
    if (tmp[1] != VX_NULL)
        vxoTensor_ReleaseTensor(&tmp[1]);
    return status;
}

vx_status vxnneExecuteSWTensorReduceSum(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_reduce_sum_sw_operation           reduceOperation   = (vxnne_tensor_reduce_sum_sw_operation)operation;

    vx_tensor src  = (vx_tensor)reduceOperation->src;
    vx_tensor dst = (vx_tensor)reduceOperation->dst;
    vx_scalar rDim = (vx_scalar)reduceOperation->reduceDim;
    vx_scalar keep = (vx_scalar)reduceOperation->keepDim;

    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_uint32 srcCount;
    vx_uint32 axis = 1024;
    vx_uint32 i;
    vx_bool keepDim = keep->value->b;
    vx_int8 srcFp = TENSOR_POS(src);
    vx_int8 dstFp = TENSOR_POS(dst);
    vx_enum   dstRoundingMode = TENSOR_ROUNDING_MODE(dst);
    vx_uint32 srcSizes[VX_CONTEXT_TENSOR_MAX_DIMENSION];

    if (src->isViewed)
    {
        for (i = 0; i < src->viewRegion.dimCount; i++)
        {
            srcSizes[i] = src->viewRegion.viewEnds[i] - src->viewRegion.viewStarts[i];
        }

    }
    else
    {
        for (i = 0; i < src->viewRegion.dimCount; i++)
        {
            srcSizes[i] = src->dims[i];
        }
    }

    if (rDim)
        axis = rDim->value->u32;

    if (keepDim)
    {
        vxmASSERT(src->viewRegion.dimCount == dst->viewRegion.dimCount);
    }
    else if (rDim && src->viewRegion.dimCount > 1)
    {
        vxmASSERT(src->viewRegion.dimCount == dst->viewRegion.dimCount + 1);
    }

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    if (!rDim)
    {
        /* When axis is NULL, if don't keep dimension, output is 1D tensor, otherwise output is 1x(dim count-2)x1 tensor*/
        vx_float32 sum = 0;
        vxoTensor_GetTensorElementCount(src, &srcCount);
        for (i = 0; i < srcCount; i++)
        {
            vx_float32 src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), i, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
            sum += src0;

        }
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), 0, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
    }
    else if (axis == 0)
    {
        /* Reduce dimension[0]*/
        vx_uint32 srcOffset, dstOffset;
        vx_uint32 x, y, z, w;
        vx_float32 sum = 0;
        vx_float32 src0;

        switch (src->viewRegion.dimCount)
        {
        case 4:
            {
                for (w = 0; w < srcSizes[3]; w++)
                {
                    for (z = 0; z < srcSizes[2]; z++)
                    {
                        for (y = 0; y < srcSizes[1]; y++)
                        {
                            for (x = 0; x < srcSizes[0]; x++)
                            {
                                srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z + srcSizes[0] * srcSizes[1] * srcSizes[2] * w;
                                src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                sum += src0;
                            }
                            dstOffset = y + srcSizes[1] * z + srcSizes[1] * srcSizes[2] * w;
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            sum = 0;
                        }

                    }
                }
            }
            break;

        case 3:
            {
                for (z = 0; z < srcSizes[2]; z++)
                {
                    for (y = 0; y < srcSizes[1]; y++)
                    {
                        for (x = 0; x < srcSizes[0]; x++)
                        {
                            srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            sum += src0;
                        }
                        dstOffset = y + srcSizes[1] * z;
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        sum = 0;
                    }
                }
            }
            break;

        case 2:
            {
                for (y = 0; y < srcSizes[1]; y++)
                {
                    for (x = 0; x < srcSizes[0]; x++)
                    {
                        srcOffset = x + srcSizes[0] * y;
                        src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                        sum += src0;
                    }
                    vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), y, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                    sum = 0;
                }
            }
            break;

        case 1:
            {
                for (x = 0; x < srcSizes[0]; x++)
                {
                    src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), x, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                    sum += src0;
                }
                vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), 0, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
            }
            break;

        default:
            vxError("Invalid input dimention count");
            return VX_ERROR_INVALID_PARAMETERS;
        }
    }
    else if (axis == 1)
    {
        /* Reduce dimension[1], dimention count should lager than 1*/
        vx_uint32 srcOffset, dstOffset;
        vx_uint32 x, y, z, w;
        vx_float32 sum = 0;
        vx_float32 src0;

        switch (src->viewRegion.dimCount)
        {
        case 4:
            {
                for (w = 0; w < srcSizes[3]; w++)
                {
                    for (z = 0; z < srcSizes[2]; z++)
                    {
                        for (x = 0; x < srcSizes[0]; x++)
                        {
                            for (y = 0; y < srcSizes[1]; y++)
                            {
                                srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z + srcSizes[0] * srcSizes[1] * srcSizes[2] * w;
                                src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                sum += src0;
                            }
                            dstOffset = x + srcSizes[0] * z + srcSizes[0] * srcSizes[2] * w;
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            sum = 0;
                        }

                    }
                }
            }
            break;

        case 3:
            {
                for (z = 0; z < srcSizes[2]; z++)
                {
                    for (x = 0; x < srcSizes[0]; x++)
                    {
                        for (y = 0; y < srcSizes[1]; y++)
                        {
                            srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            sum += src0;
                        }
                        dstOffset = x + srcSizes[0] * z;
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        sum = 0;
                    }
                }
            }
            break;

        case 2:
            {
                for (x = 0; x < srcSizes[0]; x++)
                {
                    for (y = 0; y < srcSizes[1]; y++)
                    {
                        srcOffset = x + srcSizes[0] * y;
                        src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                        sum += src0;
                    }
                    vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), x, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                    sum = 0;
                }
            }
            break;

        default:
            vxError("Invalid input dimention count");
            return VX_ERROR_INVALID_PARAMETERS;
        }
    }
    else if (axis == 2)
    {
        /* Reduce dimension[2], dimention count should lager than 2*/
        vx_uint32 srcOffset, dstOffset;
        vx_uint32 x, y, z, w;
        vx_float32 sum = 0;
        vx_float32 src0;

        switch (src->viewRegion.dimCount)
        {
        case 4:
            {
                for (w = 0; w < srcSizes[3]; w++)
                {
                    for (y = 0; y < srcSizes[1]; y++)
                    {
                        for (x = 0; x < srcSizes[0]; x++)
                        {
                            for (z = 0; z < srcSizes[2]; z++)
                            {
                                srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z + srcSizes[0] * srcSizes[1] * srcSizes[2] * w;
                                src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                sum += src0;
                            }
                            dstOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * w;
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            sum = 0;
                        }

                    }
                }
            }
            break;

        case 3:
            {
                for (y = 0; y < srcSizes[1]; y++)
                {
                    for (x = 0; x < srcSizes[0]; x++)
                    {
                        for (z = 0; z < srcSizes[2]; z++)
                        {
                            srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            sum += src0;
                        }
                        dstOffset = x + srcSizes[0] * y;
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        sum = 0;
                    }
                }
            }
            break;

        default:
            vxError("Invalid input dimention count");
            return VX_ERROR_INVALID_PARAMETERS;
        }
    }
    else if (axis == 3)
    {
        /* Reduce dimension[3], dimention count should lager than 3*/
        vx_uint32 srcOffset, dstOffset;
        vx_uint32 x, y, z, w;
        vx_float32 sum = 0;
        vx_float32 src0;

        switch (src->viewRegion.dimCount)
        {
        case 4:
            {
                for (z = 0; z < srcSizes[2]; z++)
                {
                    for (y = 0; y < srcSizes[1]; y++)
                    {
                        for (x = 0; x < srcSizes[0]; x++)
                        {
                            for (w = 0; w < srcSizes[3]; w++)
                            {
                                srcOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z + srcSizes[0] * srcSizes[1] * srcSizes[2] * w;
                                src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                sum += src0;
                            }
                            dstOffset = x + srcSizes[0] * y + srcSizes[0] * srcSizes[1] * z;
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, sum, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            sum = 0;
                        }

                    }
                }
            }
            break;

        default:
            vxError("Invalid input dimention count");
            return VX_ERROR_INVALID_PARAMETERS;
        }
    }

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorPad(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_pad_sw_operation           padOperation   = (vxnne_tensor_pad_sw_operation)operation;

    vx_tensor src  = (vx_tensor)padOperation->src;
    vx_tensor dst = (vx_tensor)padOperation->dst;

    vx_scalar padLeft = (vx_scalar)padOperation->padLeft;
    vx_scalar padRight = (vx_scalar)padOperation->padRight;
    vx_scalar padTop = (vx_scalar)padOperation->padTop;
    vx_scalar padBottom = (vx_scalar)padOperation->padBottom;
    vx_scalar padMode = (vx_scalar)padOperation->padMode;
    vx_scalar padConst = (vx_scalar)padOperation->padConst;

    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_int8 srcFp = TENSOR_POS(src);
    vx_int8 dstFp = TENSOR_POS(dst);
    vx_enum  dstRoundingMode = TENSOR_ROUNDING_MODE(dst);

    vx_uint32 dstSize;

    vx_uint32 left = padLeft->value->u32;
    vx_uint32 right = padRight->value->u32;
    vx_uint32 top = padTop->value->u32;
    vx_uint32 bottom = padBottom->value->u32;

    vx_uint32 inWidth, inHeight;
    vx_uint32 outWidth, outHeight, ofm, batch;
    vx_uint32 x, y, z, w;

    vx_enum mode = padMode->value->e;
    vx_uint32 constant = padConst->value->u32;

    if (src->isViewed)
    {
        inWidth = src->viewRegion.viewEnds[0] - src->viewRegion.viewStarts[0];
        inHeight = src->viewRegion.viewEnds[1] - src->viewRegion.viewStarts[1];
    }
    else
    {
        inWidth = TENSOR_SIZE_INDEX(src, 0);
        inHeight = TENSOR_SIZE_INDEX(src, 1);
    }

    if (dst->isViewed)
    {
        outWidth = dst->viewRegion.viewEnds[0] - dst->viewRegion.viewStarts[0];
        outHeight = dst->viewRegion.viewEnds[1] - dst->viewRegion.viewStarts[1];
        ofm = dst->viewRegion.viewEnds[2] - dst->viewRegion.viewStarts[2];
        batch = dst->viewRegion.viewEnds[3] - dst->viewRegion.viewStarts[3];
    }
    else
    {
        outWidth = TENSOR_SIZE_INDEX(dst, 0);
        outHeight = TENSOR_SIZE_INDEX(dst, 1);
        ofm = TENSOR_SIZE_INDEX(dst, 2);
        batch = TENSOR_SIZE_INDEX(dst, 3);
    }

    if (inHeight + top + bottom != outHeight ||
        inWidth + left + right != outWidth)
    {
        vxmASSERT(gcvFALSE);
    }

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    vxoTensor_GetTensorSize(dst,&dstSize);

    switch (mode)
    {
    case VX_PAD_CONSTANT:
        {
            if (TENSOR_DATA_TYPE(dst) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(dst) == VX_TYPE_UINT8)
            {
                /*Set const value to dst*/
                memset(dstLogical,
                    constant,
                    dstSize);

                /* Copy non-padding part*/
                for (w = 0; w < batch; w++)
                {
                    for (z = 0; z < ofm; z++)
                    {
                        for (y = top; y < outHeight - bottom; y++)
                        {
                            for (x = left; x < outWidth - right; x++)
                            {
                                vx_float32 src0;
                                vx_uint32 srcOffset = (x - left) + inWidth * (y - top) + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                                vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                                src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            }
                        }
                    }
                }
            }
            else
            {
                for (w = 0; w < batch; w++)
                {
                    for (z = 0; z < ofm; z++)
                    {
                        for (y = 0; y < outHeight; y++)
                        {
                            for (x = 0; x < outWidth; x++)
                            {
                                vx_float32 src0;
                                vx_uint32 srcOffset;
                                vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                                if (x < left
                                    || x >= outWidth - right
                                    || y < top
                                    || y >= outHeight - bottom)
                                {
                                    src0 = (vx_float32)constant;
                                }
                                else
                                {
                                    srcOffset = (x - left) + inWidth * (y - top) + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                                    src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                                }
                                vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                            }
                        }
                    }
                }
            }
        }
        break;

    case VX_PAD_REPLICATE:
        {
            for (w = 0; w < batch; w++)
            {
                for (z = 0; z < ofm; z++)
                {
                    for (y = 0; y < outHeight; y++)
                    {
                        for (x = 0; x < outWidth; x++)
                        {
                            vx_float32 src0;
                            vx_uint32 srcX, srcY;
                            vx_uint32 srcOffset;
                            vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                            if (x < left)
                                srcX = 0;
                            else if (x >= outWidth - right)
                                srcX = inWidth - 1;
                            else
                                srcX = x - left;

                            if (y < top)
                                srcY = 0;
                            else if (y >= outHeight - bottom)
                                srcY = inHeight - 1;
                            else
                                srcY = y - top;

                            srcOffset = srcX + inWidth * srcY + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        }
                    }
                }
            }
        }
        break;

    case VX_PAD_MIRROR_SYMMETRIC:
        {
            for (w = 0; w < batch; w++)
            {
                for (z = 0; z < ofm; z++)
                {
                    for (y = 0; y < outHeight; y++)
                    {
                        for (x = 0; x < outWidth; x++)
                        {
                            vx_float32 src0;
                            vx_uint32 srcX, srcY;
                            vx_uint32 srcOffset;
                            vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                            if (x < left)
                                srcX = left - x - 1;
                            else if (x >= outWidth - right)
                                srcX = inWidth - (x - inWidth - left + 1);
                            else
                                srcX = x - left;

                            if (y < top)
                                srcY = top - y - 1;
                            else if (y >= outHeight - bottom)
                                srcY = inHeight - (y - inHeight - top + 1);
                            else
                                srcY = y - top;

                            srcOffset = srcX + inWidth * srcY + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        }
                    }
                }
            }
        }
        break;

    case VX_PAD_MIRROR_REFLECT:
        {
            for (w = 0; w < batch; w++)
            {
                for (z = 0; z < ofm; z++)
                {
                    for (y = 0; y < outHeight; y++)
                    {
                        for (x = 0; x < outWidth; x++)
                        {
                            vx_float32 src0;
                            vx_uint32 srcX, srcY;
                            vx_uint32 srcOffset;
                            vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                            if (x < left)
                                srcX = left - x;
                            else if (x >= outWidth - right)
                                srcX = inWidth - (x - inWidth - left + 2);
                            else
                                srcX = x - left;

                            if (y < top)
                                srcY = top - y;
                            else if (y >= outHeight - bottom)
                                srcY = inHeight - (y - inHeight - top + 2);
                            else
                                srcY = y - top;

                            srcOffset = srcX + inWidth * srcY + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                        }
                    }
                }
            }
        }
        break;

    default:
        vxError("Invalid pad mode");
        return VX_ERROR_INVALID_PARAMETERS;
    }

    return VX_SUCCESS;
}

vx_status vxnnePAD(vx_int32 pad, vx_int32 slice, vx_uint8_ptr* dst_ptr, vx_int32 item_size, vx_enum mode, vx_int32 constant, vx_uint8_ptr src_ptr)
{

    if (pad > 0)
    {
        vx_int32 s = 0;
        for (s = 0; s < slice * pad; s++)
        {
            switch (mode)
            {
            case VX_PAD_CONSTANT:
                memset(*dst_ptr, constant, item_size);
                break;
            case VX_PAD_REPLICATE:
                break;
            case VX_PAD_MIRROR_SYMMETRIC:
                break;
            case VX_PAD_MIRROR_REFLECT:
                break;
            }

            *dst_ptr += item_size;
        }

    }
    return VX_SUCCESS;

}

vx_status vxnneExecuteSWTensorPad2(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_pad_sw_operation           padOperation = (vxnne_tensor_pad_sw_operation)operation;

    vx_tensor src = (vx_tensor)padOperation->src;
    vx_tensor dst = (vx_tensor)padOperation->dst;

    vx_tensor pad_dims = (vx_tensor)padOperation->pad_dims;
    vx_scalar padMode = (vx_scalar)padOperation->padMode;
    vx_scalar padConst = (vx_scalar)padOperation->padConst;
    vx_uint8_ptr src_base = VX_NULL, dst_base = VX_NULL, dst_ptr = VX_NULL, src_ptr = VX_NULL;
    vx_uint32 constant = padConst != VX_NULL?padConst->value->u32:0;
    vx_enum mode = padMode->value->e;
    vx_int32 left_n_padding = 0, left_c_padding = 0, left_h_padding = 0, left_w_padding = 0;
    vx_int32 right_n_padding = 0, right_c_padding = 0, right_h_padding = 0, right_w_padding = 0;
    vx_int32 inWidth = 0, inHeight = 0, inDepth = 0, inBatch = 0;
    vx_int32 outWidth = 0, outHeight = 0, outDepth = 0, outBatch = 0;
    vx_int32 item_size = vxnneGetTypeSize(TENSOR_DATA_TYPE(src));
    vx_int32 b = 0, c = 0, w = 0, h = 0;
    vx_int32_ptr pad_base = VX_NULL;

    vxoTensor_GetTensorViewMemory(pad_dims, (gctPOINTER*)&pad_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(src, (gctPOINTER*)&src_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, (gctPOINTER*)&dst_base, VX_NULL);

    /*
     *     w      h       c       n
     *  0   1   2   3   4   5   6   7
     * w_s w_e h_s h_e c_s c_e n_s n_e
     */
    left_n_padding = pad_base[6];
    left_c_padding = pad_base[4];
    left_h_padding = pad_base[2];
    left_w_padding = pad_base[0];

    right_n_padding = pad_base[7];
    right_c_padding = pad_base[5];
    right_h_padding = pad_base[3];
    right_w_padding = pad_base[1];

    if (src->isViewed)
    {
        inWidth  = src->viewRegion.viewEnds[0] - src->viewRegion.viewStarts[0];
        inHeight = src->viewRegion.viewEnds[1] - src->viewRegion.viewStarts[1];
        inDepth  = src->viewRegion.viewEnds[2] - src->viewRegion.viewStarts[2];
        inBatch  = src->viewRegion.viewEnds[3] - src->viewRegion.viewStarts[3];
    }
    else
    {
        inWidth  = TENSOR_SIZE_INDEX(src, 0);
        inHeight = TENSOR_SIZE_INDEX(src, 1);
        inDepth  = TENSOR_SIZE_INDEX(src, 2);
        inBatch  = TENSOR_SIZE_INDEX(src, 3);
    }

    if (dst->isViewed)
    {
        outWidth  = dst->viewRegion.viewEnds[0] - dst->viewRegion.viewStarts[0];
        outHeight = dst->viewRegion.viewEnds[1] - dst->viewRegion.viewStarts[1];
        outDepth  = dst->viewRegion.viewEnds[2] - dst->viewRegion.viewStarts[2];
        outBatch  = dst->viewRegion.viewEnds[3] - dst->viewRegion.viewStarts[3];
    }
    else
    {
        outWidth = TENSOR_SIZE_INDEX(dst, 0);
        outHeight = TENSOR_SIZE_INDEX(dst, 1);
        outDepth = TENSOR_SIZE_INDEX(dst, 2);
        outBatch = TENSOR_SIZE_INDEX(dst, 3);
    }

    gcmASSERT(inHeight + left_h_padding + right_h_padding == outHeight);
    gcmASSERT(inWidth  + left_w_padding + right_w_padding == outWidth);
    gcmASSERT(inDepth  + left_c_padding + right_c_padding == outDepth);
    gcmASSERT(inBatch  + left_n_padding + right_n_padding == outBatch);

    dst_ptr = dst_base;
    src_ptr = src_base;

    vxnnePAD(left_n_padding, outDepth * outWidth * outHeight, &dst_ptr, item_size, mode, constant, VX_NULL);

    for (b = 0; b < inBatch; b ++)
    {

        vxnnePAD(left_c_padding, outWidth * outHeight, &dst_ptr, item_size, mode, constant, VX_NULL);

        for (c = 0; c < inDepth; c++)
        {
            vxnnePAD(left_h_padding, outWidth, &dst_ptr, item_size, mode, constant, VX_NULL);

            for (h = 0; h < inHeight; h++)
            {
                vxnnePAD(left_w_padding, 1, &dst_ptr, item_size, mode, constant, VX_NULL);

                for (w = 0; w < inWidth; w++)
                {
                    memcpy(dst_ptr, src_ptr, item_size);
                    src_ptr += item_size;

                    dst_ptr += item_size;
                }

                vxnnePAD(right_w_padding, 1, &dst_ptr, item_size, mode, constant, VX_NULL);
            }
            vxnnePAD(right_h_padding, outWidth, &dst_ptr, item_size, mode, constant, VX_NULL);
        }
        vxnnePAD(right_c_padding, outWidth * outHeight, &dst_ptr, item_size, mode, constant, VX_NULL);
    }

    vxnnePAD(right_n_padding, outDepth * outWidth * outHeight, &dst_ptr, item_size, mode, constant, VX_NULL);

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWConvertFormat(struct _vxnne_operation_s *operation)
{
    vxnne_convert_format_operation           convertOperation   = (vxnne_convert_format_operation)operation;

    vx_tensor src  = (vx_tensor)convertOperation->inputs;
    vx_tensor dst = (vx_tensor)convertOperation->outputs;
    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_uint32 dstSize;
    vx_uint32 dstCount;
    vx_int8 srcFp = TENSOR_POS(src);
    vx_int8 dstFp = TENSOR_POS(dst);
    vx_enum   dstRoundingMode = TENSOR_ROUNDING_MODE(dst);
    vx_uint32 i;

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    vxoTensor_GetTensorSize(dst, &dstSize);

    memset(dstLogical,
           0,
           dstSize);

    vxoTensor_GetTensorElementCount(dst, &dstCount);
    for (i = 0; i < dstCount; i++)
    {
        vx_float32 src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), i, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), i, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
    }


    return VX_SUCCESS;
}

vx_status vxnneExecuteSWBrickMode(struct _vxnne_operation_s *operation)
{
    vxnne_brick_operation           brickModeOperation   = (vxnne_brick_operation)operation;

    vx_tensor inputs = (vx_tensor)brickModeOperation->inputs;
    vx_tensor outputs = (vx_tensor)brickModeOperation->outputs;
    vx_uint32 kernel_x, kernel_y;
    vx_uint32 padXLeft;
    vx_uint32 padXRight;
    vx_uint32 padYTop;
    vx_uint32 padYBottom;
    vx_uint32 inImageTileSizeX, inImageTileSizeY;
    vx_uint32 numOfImageTileX, numOfImageTileY;
    vx_uint32 distSize;
    vx_uint32 input_width, input_height, input_z;
    vx_uint32 i, j, x, y, z;
    vx_uint32 outTileX, outTileY;
    vx_uint8_ptr temp_buffer = VX_NULL;
    vx_context context = vxGetContext((vx_reference)inputs);

    padXLeft   = brickModeOperation->pad_x_left;
    padXRight  = brickModeOperation->pad_x_right;
    padYTop    = brickModeOperation->pad_y_top;
    padYBottom = brickModeOperation->pad_y_bottom;

    kernel_x = brickModeOperation->kernel_x;
    kernel_y = brickModeOperation->kernel_y;
    outTileX = brickModeOperation->outTileX;
    outTileY = brickModeOperation->outTileY;

    if (inputs->isViewed)
    {
        input_width = inputs->viewRegion.viewEnds[0] - inputs->viewRegion.viewStarts[0];
        input_height = inputs->viewRegion.viewEnds[1] - inputs->viewRegion.viewStarts[1];
        input_z = inputs->viewRegion.viewEnds[2] - inputs->viewRegion.viewStarts[2];
    }
    else
    {
        input_width = TENSOR_SIZE_INDEX(inputs, 0);
        input_height = TENSOR_SIZE_INDEX(inputs, 1);
        input_z = TENSOR_SIZE_INDEX(inputs, 2);
    }

    inImageTileSizeX = outTileX - 1 + kernel_x;
    inImageTileSizeY = outTileY - 1 + kernel_y;

    distSize = inImageTileSizeX * inImageTileSizeY * input_z * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
    numOfImageTileX = brickModeOperation->num_tile_x;
    numOfImageTileY = brickModeOperation->num_tile_y;

    temp_buffer = (vx_uint8_ptr)vxAllocateAndZeroMemory(distSize * numOfImageTileX * numOfImageTileY);

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP))
        gcfVX_Flush(gcvTRUE);

    if (padXLeft != 0 || padXRight != 0 || padYBottom != 0 || padYTop != 0)
    {
        for (i = 0; i < numOfImageTileY; i++)
        {
            for (j = 0; j < numOfImageTileX; j++)
            {
                vx_uint32 offsetDist = (j + i * numOfImageTileX) * distSize;

                for (z = 0; z < input_z; z++)
                {
                    vx_uint32 dstX = 0, dstY = 0;
                    vx_int32 inX = 0, inY = 0;

                    /* (Top, Left) */
                    if (i == 0 && j == 0)
                    {
                        vx_uint32 tempX = inImageTileSizeX - padXLeft;
                        vx_uint32 tempY = inImageTileSizeY - padYTop;

                        if (tempX > input_width)
                            tempX = input_width;
                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* (Bottom, Right) */
                    else if (i == numOfImageTileY - 1 && j == numOfImageTileX - 1)
                    {
                        vx_uint32 tempX = input_width - (numOfImageTileX - 1) * outTileX + padXLeft;
                        vx_uint32 tempY = input_height - (numOfImageTileY - 1) * outTileY + padYTop;

                        if (tempX > input_width)
                            tempX = input_width;
                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                if (j != 0)
                                    inX = x + j * outTileX - padXLeft;
                                else
                                    inX = x;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* (Bottom, Left) */
                    else if (i == numOfImageTileY - 1 && j == 0)
                    {
                        vx_uint32 tempX = inImageTileSizeX - padXLeft;
                        vx_uint32 tempY = input_height - (numOfImageTileY - 1) * outTileY + padYTop;

                        if (tempX > input_width)
                            tempX = input_width;
                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* (Top, Right) */
                    else if (j == numOfImageTileX - 1 && i == 0)
                    {
                        vx_uint32 tempX = input_width - (numOfImageTileX - 1) * outTileX + padXLeft;
                        vx_uint32 tempY = inImageTileSizeY - padYTop;

                        if (tempX > input_width)
                            tempX = input_width;
                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX - padXLeft;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* Other Top */
                    else if (i == 0)
                    {
                        vx_uint32 tempY = inImageTileSizeY - padYTop;

                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < inImageTileSizeX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= inImageTileSizeX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX - padXLeft;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + inImageTileSizeX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* Other Bottom */
                    else if (i == numOfImageTileY - 1)
                    {
                        vx_uint32 tempY = input_height - (numOfImageTileY - 1) * outTileY + padYTop;

                        if (tempY > input_height)
                            tempY = input_height;

                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < inImageTileSizeX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= inImageTileSizeX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX - padXLeft;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + inImageTileSizeX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* Other Left */
                    else if (j == 0)
                    {
                        vx_uint32 tempX = inImageTileSizeX - padXLeft;

                        if (tempX > input_width)
                            tempX = input_width;

                        for (y = 0; y < inImageTileSizeY; y++)
                        {

                            if (dstY >= inImageTileSizeY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + inImageTileSizeY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    /* Other Right */
                    else if (j == numOfImageTileX - 1)
                    {
                        vx_uint32 tempX = input_width - (numOfImageTileX - 1) * outTileX + padXLeft;

                        if (tempX > input_width)
                            tempX = input_width;

                        for (y = 0; y < inImageTileSizeY; y++)
                        {

                            if (dstY >= inImageTileSizeY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX - padXLeft;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + inImageTileSizeY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    else
                    {
                        for (y = 0; y < inImageTileSizeY; y++)
                        {

                            if (dstY >= inImageTileSizeY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < inImageTileSizeX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= inImageTileSizeX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX - padXLeft;
                                inY = y + i * outTileY - padYTop;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                if (inImageTileSizeX > input_width && inImageTileSizeY > input_height)
                                    dstOffserSize = (dstX + input_width * (dstY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                else
                                    dstOffserSize = (dstX + inImageTileSizeX * (dstY + inImageTileSizeY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                }
            }
        }
    }
    else /* no padding*/
    {
        for (i = 0; i < numOfImageTileY; i++)
        {
            for (j = 0; j < numOfImageTileX; j++)
            {
                vx_uint32 offsetDist = (j + i * numOfImageTileX) * distSize;

                for (z = 0; z < input_z; z++)
                {
                    vx_uint32 dstX = 0, dstY = 0;
                    vx_int32 inX = 0, inY = 0;
                    if (i == numOfImageTileY - 1 && j == numOfImageTileX -1 && input_width % outTileX && input_height % outTileY && inImageTileSizeX < input_width && inImageTileSizeY < input_height)
                    {
                        vx_uint32 tempX = input_width - (numOfImageTileX - 1) * outTileX;
                        vx_uint32 tempY = input_height - (numOfImageTileY - 1) * outTileY;
                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    else if (i == numOfImageTileY - 1 && input_height % outTileY && inImageTileSizeY < input_height)
                    {
                        vx_uint32 tempY = input_height - (numOfImageTileY - 1) * outTileY;
                        for (y = 0; y < tempY; y++)
                        {

                            if (dstY >= tempY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < inImageTileSizeX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= inImageTileSizeX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + inImageTileSizeX * (dstY + tempY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    else if (j == numOfImageTileX - 1 && input_width % outTileX && inImageTileSizeX < input_width && inImageTileSizeY < input_height)
                    {
                        vx_uint32 tempX = input_width - (numOfImageTileX - 1) * outTileX;
                        for (y = 0; y < inImageTileSizeY; y++)
                        {

                            if (dstY >= inImageTileSizeY || dstY >= input_height)
                                dstY = 0;

                            for (x = 0; x < tempX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= tempX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                dstOffserSize = (dstX + tempX * (dstY + inImageTileSizeY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                    else
                    {
                        for (y = 0; y < inImageTileSizeY; y++)
                        {

                            if (dstY >= inImageTileSizeY || dstY >= input_height)
                                    dstY = 0;

                            for (x = 0; x < inImageTileSizeX; x++)
                            {

                                vx_uint32 srcOffsetSize, dstOffserSize;
                                vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

                                if (dstX >= inImageTileSizeX || dstX >= input_width)
                                    dstX = 0;

                                inX = x + j * outTileX;
                                inY = y + i * outTileY;
                                /* Skip out of border value */
                                if (inX < 0 || inY < 0 || inX >= (vx_int32)input_width || inY >= (vx_int32)input_height)
                                    continue;

                                if (inputs->isViewed)
                                    srcOffsetSize = ((inX + inputs->viewRegion.viewStarts[0]) + input_width * ((inY + inputs->viewRegion.viewStarts[1]) + input_height * (z + inputs->viewRegion.viewStarts[2]))) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));

                                else
                                    srcOffsetSize = (inX + input_width * (inY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                if (inImageTileSizeX > input_width && inImageTileSizeY > input_height)
                                    dstOffserSize = (dstX + input_width * (dstY + input_height * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                else
                                    dstOffserSize = (dstX + inImageTileSizeX * (dstY + inImageTileSizeY * z)) * vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs));
                                input_ptr = inputs->tensorBuffer->memory.logicals[0] + srcOffsetSize;
                                output_ptr = temp_buffer + dstOffserSize + offsetDist;

                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16)
                                    *(vx_uint16*)output_ptr = *(vx_uint16*)input_ptr;
                                else if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_INT8)
                                    *(vx_int8*)output_ptr = *(vx_int8*)input_ptr;
                                else
                                    *(vx_int32*)output_ptr = *(vx_int32*)input_ptr;
                                dstX++;
                            }
                            if (inY < 0)
                                continue;
                            dstY++;
                        }
                    }
                }
            }
        }
    }

    gcoVX_FreeMemory((gcsSURF_NODE_PTR)outputs->tensorBuffer->memory.nodePtrs[0]);
    outputs->tensorBuffer->memory.logicals[0]    = VX_NULL;
    outputs->tensorBuffer->memory.nodePtrs[0]    = VX_NULL;

    gcoVX_AllocateMemory(distSize * numOfImageTileX * numOfImageTileY, (gctPOINTER*)&outputs->tensorBuffer->memory.logicals[0], &outputs->tensorBuffer->memory.physicals[0], &outputs->tensorBuffer->memory.nodePtrs[0]);
    vxMemCopy(outputs->tensorBuffer->memory.logicals[0], temp_buffer, distSize * numOfImageTileX * numOfImageTileY);
    vxFree(temp_buffer);
    temp_buffer = VX_NULL;

    return VX_SUCCESS;

}

vx_status vxnneExecuteSWPooling(struct _vxnne_operation_s *operation)
{
    vxnne_pooling_operation           poolingOperation   = (vxnne_pooling_operation)operation;

    vx_tensor inputs  = (vx_tensor)poolingOperation->inputs;
    vx_tensor outputs = (vx_tensor)poolingOperation->outputs;

    vx_enum  poolType_v = poolingOperation->pool_type;
    vx_uint32 poolSizeX_v = poolingOperation->pool_size_x;
    vx_uint32 poolSizeY_v = poolingOperation->pool_size_y;
    vx_uint32 poolPadXLeft_v = poolingOperation->pool_pad_x_left;
    vx_uint32 poolPadXRight_v = poolingOperation->pool_pad_x_right;
    vx_uint32 poolPadYTop_v = poolingOperation->pool_pad_y_top;
    vx_uint32 poolPadYBottom_v = poolingOperation->pool_pad_y_bottom;
    vx_enum rounding_v = poolingOperation->rounding;

    vx_status status = VX_SUCCESS;

    gctPOINTER inputsBaseLogicalAddr = VX_NULL;
    gctPOINTER outputsBaseLogicalAddr = VX_NULL;

    vx_int32 inputs_width, inputs_height, depth, outputs_width, outputs_height, out_w, out_h, batch;
    vx_uint32 stride_x, stride_y;
    vx_int32 type;

    vxoTensor_GetTensorViewMemory(inputs, &inputsBaseLogicalAddr, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputsBaseLogicalAddr, VX_NULL);

    inputs_width   = TENSOR_SIZE_INDEX(inputs, 0);
    inputs_height  = TENSOR_SIZE_INDEX(inputs, 1);
    depth          = TENSOR_SIZE_INDEX(inputs, 2);
    batch          = TENSOR_SIZE_INDEX(inputs, 3);
    outputs_width  = TENSOR_SIZE_INDEX(outputs, 0);
    outputs_height = TENSOR_SIZE_INDEX(outputs, 1);

    switch (poolType_v)
    {
    case VX_NN_POOLING_MAX:
        type = 1;
        break;
    case VX_NN_POOLING_AVG:
    case VX_NN_POOLING_AVG_ANDROID:
        type = 2;
        break;
    default:
        vxError("not support this pool type");
        return VX_ERROR_INVALID_PARAMETERS;
    }

    if (outputs_width == 1 && outputs_height == 1)
    {
        stride_x = 1;
        stride_y = 1;
    }
    else if (outputs_width == 1)
    {
        stride_x = 1;
        stride_y = vxoNNExternsionConvlutionRound((vx_float32)(inputs_height + poolPadYTop_v + poolPadYBottom_v - poolSizeY_v) / (outputs_height - 1), rounding_v);
    }
    else if(outputs_height == 1)
    {
        stride_x = vxoNNExternsionConvlutionRound((vx_float32)(inputs_width + poolPadXLeft_v + poolPadXRight_v - poolSizeX_v) / (outputs_width - 1), rounding_v);
        stride_y = 1;
    }
    else
    {
        /* Calculate stride = (w + 2*pad - weight)/(output_w - 1) */
        stride_x = vxoNNExternsionConvlutionRound((vx_float32)(inputs_width + poolPadXLeft_v + poolPadXRight_v - poolSizeX_v) / (outputs_width - 1), rounding_v);
        stride_y = vxoNNExternsionConvlutionRound((vx_float32)(inputs_height + poolPadYTop_v + poolPadYBottom_v - poolSizeY_v) / (outputs_height - 1), rounding_v);
    }

    status = vxnnePoolingCpu((vx_uint8_ptr)inputsBaseLogicalAddr,
                             TENSOR_POS(inputs),
                             type,
                             (vx_type_e)TENSOR_DATA_TYPE(inputs),
                             inputs_width,
                             inputs_height,
                             batch,
                             depth,
                             &out_w,
                             &out_h,
                             stride_x,
                             stride_y,
                             poolSizeX_v,
                             poolSizeY_v,
                             poolPadXLeft_v,
                             poolPadXRight_v,
                             poolPadYTop_v,
                             poolPadYBottom_v,
                             rounding_v,
                             (vx_uint8_ptr)outputsBaseLogicalAddr,
                             TENSOR_POS(outputs),
                             TENSOR_ROUNDING_MODE(outputs),
                             (vx_type_e)TENSOR_DATA_TYPE(outputs),
                             (vx_type_e)TENSOR_QUANT_TYPE(inputs),
                             (vx_type_e)TENSOR_QUANT_TYPE(outputs),
                             TENSOR_TF_ZEROPOINT(inputs),
                             TENSOR_TF_SCALE(inputs),
                             TENSOR_TF_ZEROPOINT(outputs),
                             TENSOR_TF_SCALE(outputs));
    gcmASSERT((out_w == outputs_width) && (out_h == outputs_height));

    /* make release build happy */
    outputs_height = outputs_height;

    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConvolutionReluPoolingLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API void vxnneConvolutionReluPoolingOperation_Initialize(
    vxnne_convolution_relu_pooling_operation operation,
    vx_tensor inputs,
    vx_weights_biases_parameter weights_biases,
    vx_size dilationX,
    vx_size dilationY,
    vx_uint32 pad_x_left,
    vx_uint32 pad_x_right,
    vx_uint32 pad_y_top,
    vx_uint32 pad_y_bottom,
    vx_enum conv_rounding_type,
    vx_bool enable_relu,
    vx_bool enable_pooling,
    vx_enum pool_type,
    vx_uint32 pool_size_x,
    vx_uint32 pool_size_y,
    vx_enum padMode,
    vx_scalar padConst,
    vx_tensor outputs
    )
{
    operation->inputs           = inputs;
    operation->weights_biases   = weights_biases;
    operation->dilationX        = dilationX;
    operation->dilationY        = dilationY;
    operation->pad_x_left       = pad_x_left;
    operation->pad_x_right      = pad_x_right;
    operation->pad_y_top        = pad_y_top;
    operation->pad_y_bottom     = pad_y_bottom;
    operation->conv_rounding_type = conv_rounding_type;
    operation->enable_relu      = enable_relu;
    operation->enable_pooling   = enable_pooling;
    operation->pool_type        = pool_type;
    operation->pool_size_x      = pool_size_x;
    operation->pool_size_y      = pool_size_y;
    operation->padMode          = padMode;
    operation->padConst         = padConst;
    operation->outputs          = outputs;
}

VX_PRIVATE_API vx_status vxnneConvolutionReluPoolingInitializer(
    vx_node node,
    char* name,
    vx_tensor inputs,
    vx_weights_biases_parameter weights_biases,
    vx_size dilationX,
    vx_size dilationY,
    vx_uint32 pad_x_left,
    vx_uint32 pad_x_right,
    vx_uint32 pad_y_top,
    vx_uint32 pad_y_bottom,
    vx_enum conv_rounding_type,
    vx_bool enable_relu,
    vx_bool enable_pooling,
    vx_enum pool_type,
    vx_uint32 pool_size_x,
    vx_uint32 pool_size_y,
    vx_enum padMode,
    vx_scalar padConst,
    vx_tensor outputs
    )
{
    vx_status status = VX_SUCCESS;

    vx_context context = vxGetContext((vx_reference)node);
    vx_uint32 padConstValue = padConst != VX_NULL ? padConst->value->u32 : 0;
    vx_tensor convertTensor = VX_NULL;
    vx_uint32 batchCount = (TENSOR_SIZE_INDEX(inputs, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(inputs, 3);
    vx_tensor interTensor = inputs;
    vx_uint32 tmpTensorIndex = 0;
    vx_bool enableBrickMode = vx_false_e;
    vx_bool needExtraBrickOp = vx_true_e;

    vx_weights_biases_parameter reshapeWb = VX_NULL;

    vxnne_convolution_relu_pooling_layer convolutionReluPoolingLayer = gcvNULL;

    vx_uint32 operationIndex = 0;

    if (!vxnneIsNNSupportFormat(context, inputs, weights_biases, outputs))
    {
        status = VX_ERROR_NOT_SUPPORTED;
        vxError("hw not support this format. function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_convolution_relu_pooling_layer_s), (gctPOINTER*)&convolutionReluPoolingLayer);
    if (!convolutionReluPoolingLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(convolutionReluPoolingLayer, sizeof(vxnne_convolution_relu_pooling_layer_s));

    vxnneLayer_Initialize(&convolutionReluPoolingLayer->base,
                            name,
                            node,
                            vxmOPERATION_COUNT(convolutionReluPoolingLayer),
                            convolutionReluPoolingLayer->operations,
                            VX_NULL);

    if (weights_biases->wb_base->hw_depth_wise)
    {
        status = vxnneOperation_Initialize(&convolutionReluPoolingLayer->convolution_operation.base,
                                           &convolutionReluPoolingLayer->base,
                                           VXNNE_OPERATION_TARGET_NN,
                                           VXNNE_OPERATOR_DEPTH_WISE_CONV,
                                           VX_NULL,
                                           vxnneOperation_ConvolutionReluPooling_Deinitialize,
                                           batchCount,
                                           NNE_COMMAND_SIZE);
    }
    else
    {
        status = vxnneOperation_Initialize(&convolutionReluPoolingLayer->convolution_operation.base,
                                           &convolutionReluPoolingLayer->base,
                                           VXNNE_OPERATION_TARGET_NN,
                                           VXNNE_OPERATOR_CONVOLUTION,
                                           VX_NULL,
                                           vxnneOperation_ConvolutionReluPooling_Deinitialize,
                                           batchCount,
                                           NNE_COMMAND_SIZE);
    }
    if (status != VX_SUCCESS) goto exit;


    if ((WB_STRIDE_X(weights_biases) > 1) || (WB_STRIDE_Y(weights_biases) > 1))
    {
        vx_uint32 sizes[4];
        vx_tensor reshuffleTensor = VX_NULL;
        vx_tensor specialTensorFor1x1 = VX_NULL;
        vx_tensor_create_params_t tensor_create_params;

        sizes[0] = ComputeInputSize(TENSOR_VIEW_SIZE_INDEX(outputs, 0), WB_KERNEL_X(weights_biases), 0, 0, pool_size_x, 2, VX_NULL, 1);
        sizes[1] = ComputeInputSize(TENSOR_VIEW_SIZE_INDEX(outputs, 1), WB_KERNEL_Y(weights_biases), 0, 0, pool_size_y, 2, VX_NULL, 1);
        sizes[2] = TENSOR_VIEW_SIZE_INDEX(inputs, 2) * WB_STRIDE_X(weights_biases) * WB_STRIDE_Y(weights_biases);
        sizes[3] = TENSOR_SIZE_INDEX(inputs, 3);

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = TENSOR_DIM_NUM(inputs);
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = TENSOR_DATA_TYPE(inputs);
        tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
        if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
        {
            tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(inputs);
        }
        else
        {
            tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
            tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
        }

        reshuffleTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (reshuffleTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_RESHUFFLE))
        {
            vx_op_param_s conv = {0};

            reshuffleTensor->brickMode = enableBrickMode;
            status = vxnneOperation_Initialize(&convolutionReluPoolingLayer->reshuffle_tp_operation.base,
                                               &convolutionReluPoolingLayer->base,
                                               VXNNE_OPERATION_TARGET_TP,
                                               VXNNE_OPERATOR_RESHUFFLE,
                                               VX_NULL,
                                               vxnneOperation_TP_Deinitialize,
                                               batchCount,
                                               0);
            if (status != VX_SUCCESS) goto exit;

            conv.pad_x_left = pad_x_left;
            conv.pad_y_top = pad_y_top;
            conv.pad_x_right = pad_x_right;
            conv.pad_y_bottom = pad_y_bottom;
            conv.pool_size_x = conv.pool_size_y = 0;
            conv.pool_stride = 1;
            conv.enable_relu = vx_false_e;
            conv.pad_mode = padMode;
            conv.pad_const = (vx_int32)padConstValue + TENSOR_PAD_ZERO_VALUE(inputs);
            conv.tpType = TP_RESHUFFLE;
            conv.other_ref = (vx_reference)weights_biases;
            conv.data_buff = gcvNULL;

            memcpy(&convolutionReluPoolingLayer->reshuffle_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));
            convolutionReluPoolingLayer->reshuffle_tp_operation.input          = inputs;
            convolutionReluPoolingLayer->reshuffle_tp_operation.weights_biases = weights_biases;
            convolutionReluPoolingLayer->reshuffle_tp_operation.output         = reshuffleTensor;

            vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_tp_operation.base, (vx_reference)reshuffleTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &convolutionReluPoolingLayer->base,
                &convolutionReluPoolingLayer->reshuffle_tp_operation.base,
                operationIndex++);

            needExtraBrickOp = vx_false_e;
        }
        else
        {
            vx_uint32      kx                      = WB_ORG_KERNEL_X(weights_biases);
            vx_uint32      ky                      = WB_ORG_KERNEL_Y(weights_biases);
            vx_uint32      stride_x                = WB_STRIDE_X(weights_biases);
            vx_uint32      stride_y                = WB_STRIDE_Y(weights_biases);
            vx_uint32      dstWidth                = TENSOR_VIEW_SIZE_INDEX(reshuffleTensor, 0);
            vx_uint32      dstHeight               = TENSOR_VIEW_SIZE_INDEX(reshuffleTensor, 1);
            vx_uint32      dstDepth                = TENSOR_VIEW_SIZE_INDEX(reshuffleTensor, 2);
            vx_bool        shExe_flag              = vx_true_e;
            vx_enum        inputFormat             = TENSOR_DATA_TYPE(inputs);
            vx_enum        outputFormat            = TENSOR_DATA_TYPE(reshuffleTensor);
            vx_bool        padMode_flag            = (vx_bool)(padMode == VX_PAD_CONSTANT || padMode == VX_PAD_REPLICATE || (pad_x_left == 0 && pad_x_right == 0 && pad_y_top == 0 && pad_y_bottom == 0));

            shExe_flag    = (vx_bool)(((_IsSameType(inputs, reshuffleTensor) && stride_x == 2 && stride_y == 2)
                                    ||(inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16 && stride_x == 4 && stride_y == 4)
                                    ||(inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16 && stride_x == 3 && stride_y == 3))
                                    && (kx != 1 || ky != 1));
            if(shExe_flag && padMode_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;
                vx_tensor                outTensor = NULL;
                if(outTensor == NULL)
                {
                    vx_int32   new_size[3] = {dstWidth, dstHeight, dstDepth};
                    vx_uint32  outputs_dims = 3;
                    outTensor = vxoTensor_ReshapeTensor(reshuffleTensor, new_size, outputs_dims);
                }

                shaderExecutable = vxnneReshuffleShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_RESHUFFLE, &node->kernelAttributes.borderMode, inputs, stride_x, stride_y, padMode, padConstValue, pad_x_left, pad_x_right, pad_y_top, pad_y_bottom, outTensor);


                if(outTensor)
                    vxoTensor_ReleaseTensor(&outTensor);

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&convolutionReluPoolingLayer->reshuffle_shader_operation,
                    &convolutionReluPoolingLayer->base,
                    VXNNE_OPERATOR_RESHUFFLE,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_shader_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_shader_operation.base, (vx_reference)reshuffleTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &convolutionReluPoolingLayer->base,
                    &convolutionReluPoolingLayer->reshuffle_shader_operation.base,
                    operationIndex++);

            }
            else
            {
                vxnneOperation_Initialize(&convolutionReluPoolingLayer->reshuffle_operation.base,
                    &convolutionReluPoolingLayer->base,
                    VXNNE_OPERATION_TARGET_SW,
                    VXNNE_OPERATOR_RESHUFFLE,
                    vxnneExecuteSWReshuffle,
                    VX_NULL,
                    batchCount,
                    0);

                convolutionReluPoolingLayer->reshuffle_operation.inputs         = inputs;
                convolutionReluPoolingLayer->reshuffle_operation.weights_biases = weights_biases;
                convolutionReluPoolingLayer->reshuffle_operation.pad_x_left     = pad_x_left;
                convolutionReluPoolingLayer->reshuffle_operation.pad_x_right    = pad_x_right;
                convolutionReluPoolingLayer->reshuffle_operation.pad_y_top      = pad_y_top;
                convolutionReluPoolingLayer->reshuffle_operation.pad_y_bottom   = pad_y_bottom;
                convolutionReluPoolingLayer->reshuffle_operation.pad_mode       = padMode;
                convolutionReluPoolingLayer->reshuffle_operation.pad_const      = padConst;
                convolutionReluPoolingLayer->reshuffle_operation.outputs        = reshuffleTensor;

                vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionReluPoolingLayer->reshuffle_operation.base, (vx_reference)reshuffleTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &convolutionReluPoolingLayer->base,
                    &convolutionReluPoolingLayer->reshuffle_operation.base,
                    operationIndex++);
            }
        }

        convolutionReluPoolingLayer->base.temp_tensors[tmpTensorIndex++] = reshuffleTensor;

        if (weights_biases->wb_base->org_weights_sizes[0] == 1 && weights_biases->wb_base->org_weights_sizes[1] == 1)
        {
            /*Since in kx=1, ky=1 situation, weight won't do reshuffle,
            TP has special opt for input, only first num of kz input channels are valid*/
            vx_uint32 smallSizeStart[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
            vx_uint32 smallSizeEnd[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
            vx_tensor_view smallView = VX_NULL;

            smallSizeEnd[0] = reshuffleTensor->dims[0];
            smallSizeEnd[1] = reshuffleTensor->dims[1];
            smallSizeEnd[2] = weights_biases->weights_sizes[2];
            smallSizeEnd[3] = reshuffleTensor->dims[3];

            smallView = vxCreateTensorView(node->base.context, smallSizeStart, smallSizeEnd, (vx_uint8)reshuffleTensor->dimCount);
            specialTensorFor1x1 = vxoTensor_CreateTensorFromView(reshuffleTensor, smallView);
            if (smallView != VX_NULL) vxReleaseTensorView(&smallView);
            convolutionReluPoolingLayer->base.temp_tensors[tmpTensorIndex++] = specialTensorFor1x1;
            interTensor = specialTensorFor1x1;
        }
        else
            interTensor = reshuffleTensor;

        /* stride > 1, set pad value to 0 */
        pad_x_left   = 0;
        pad_x_right  = 0;
        pad_y_top    = 0;
        pad_y_bottom = 0;
    }
    else if (TENSOR_DATA_TYPE(interTensor) != WB_WEIGHT_DATA_FORMAT(weights_biases))
    {
        vx_enum        inputFormat             = TENSOR_DATA_TYPE(interTensor);
        vx_enum        weightFormat            = WB_WEIGHT_DATA_FORMAT(weights_biases);
        vx_bool        shExe_flag              = (vx_bool)(inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_INT8);
        vx_uint32 sizes[3] =
        {
            TENSOR_VIEW_SIZE_INDEX(inputs, 0),
            TENSOR_VIEW_SIZE_INDEX(inputs, 1),
            TENSOR_VIEW_SIZE_INDEX(inputs, 2)
        };

        vx_tensor_create_params_t tensor_create_params;

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = TENSOR_DIM_NUM(interTensor);
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = WB_WEIGHT_DATA_FORMAT(weights_biases);
        tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
        if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
        {
            tensor_create_params.quant_data.dfp.fixed_point_pos = WB_BIAS_FPP(weights_biases)- WB_WEIGHT_FPP(weights_biases);
        }
        else
        {
            tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
            tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
        }

        convertTensor = vxoTensor_CreateTensor(
                        context,
                        node->graph,
                        &tensor_create_params,
                        vx_true_e
                        );
        if (convertTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;

            shaderExecutable = vxnneTensorConvFormatShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_CONVFORMAT, &node->kernelAttributes.borderMode, interTensor, convertTensor);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&convolutionReluPoolingLayer->convert_format_shader_operation,
                &convolutionReluPoolingLayer->base,
                VXNNE_OPERATOR_CONVERT_FORMAT,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&convolutionReluPoolingLayer->convert_format_shader_operation.base, (vx_reference)interTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&convolutionReluPoolingLayer->convert_format_shader_operation.base, (vx_reference)convertTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &convolutionReluPoolingLayer->base,
                &convolutionReluPoolingLayer->convert_format_shader_operation.base,
                operationIndex++);
        }
        else
        {
            vxnneOperation_Initialize(&convolutionReluPoolingLayer->convert_format_operation.base,
                &convolutionReluPoolingLayer->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_CONVERT_FORMAT,
                vxnneExecuteSWConvertFormat,
                VX_NULL,
                batchCount,
                0);
            convolutionReluPoolingLayer->convert_format_operation.inputs         = interTensor;
            convolutionReluPoolingLayer->convert_format_operation.outputs        = convertTensor;
            vxnneOperation_AddReference(&convolutionReluPoolingLayer->convert_format_operation.base, (vx_reference)interTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&convolutionReluPoolingLayer->convert_format_operation.base, (vx_reference)convertTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                &convolutionReluPoolingLayer->base,
                &convolutionReluPoolingLayer->convert_format_operation.base,
                operationIndex++);
        }

        convolutionReluPoolingLayer->base.temp_tensors[tmpTensorIndex++] = convertTensor;

        interTensor = convertTensor;
    }

    /* TODO. */
    needExtraBrickOp = vx_false_e;


    if (weights_biases->wb_base->do_zdp_opt &&
        !context->options.do1xnAfterSwtiling)
    {
        vx_uint32 fitN = 0;
        vx_uint32 fitOutN = 0;
        vx_uint32 i;
        vx_int32 reshapeInputSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
        vx_int32 reshapeOutputSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
        vx_uint32 stride = 1;

        calcFitZdp3N(context, TENSOR_VIEW_SIZE_INDEX(interTensor, 0), TENSOR_VIEW_SIZE_INDEX(interTensor, 1), &fitN, stride, pool_size_x);
        fitOutN = fitN / stride;

        if (fitN == 0)
        {
            vxmASSERT(0 && "fitN = 0");
            status = VX_FAILURE;
            goto exit;
        }
        reshapeInputSize[0] = TENSOR_VIEW_SIZE_INDEX(interTensor, 0) * TENSOR_VIEW_SIZE_INDEX(interTensor, 1) / fitN;
        reshapeInputSize[1] = fitN;

        reshapeOutputSize[0] = TENSOR_VIEW_SIZE_INDEX(outputs, 0) * TENSOR_VIEW_SIZE_INDEX(outputs, 1) / fitOutN;
        reshapeOutputSize[1] = fitOutN;

        for (i = 2; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
        {
            reshapeInputSize[i] = TENSOR_VIEW_SIZE_INDEX(interTensor, i);
            reshapeOutputSize[i] = TENSOR_VIEW_SIZE_INDEX(outputs, i);
        }

        convolutionReluPoolingLayer->convolution_operation.reshape_inputs = vxoTensor_ReshapeTensor(interTensor, reshapeInputSize, interTensor->dimCount);
        convolutionReluPoolingLayer->convolution_operation.reshape_outputs = vxoTensor_ReshapeTensor(outputs, reshapeOutputSize, outputs->dimCount);

        vxmASSERT(convolutionReluPoolingLayer->convolution_operation.reshape_inputs != VX_NULL && convolutionReluPoolingLayer->convolution_operation.reshape_outputs != VX_NULL);
    }
    else if (weights_biases->wb_base->do_1xN &&
        !context->options.do1xnAfterSwtiling)
    {
        vx_uint32 fitN = calcFit1xN(context, TENSOR_VIEW_SIZE_INDEX(interTensor, 2), TENSOR_VIEW_SIZE_INDEX(interTensor, 0), TENSOR_VIEW_SIZE_INDEX(interTensor, 1));

        vx_uint32 i;
        vx_int32 reshapeInputSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
        vx_int32 reshapeOutputSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};

        /* Need reshape input[x, y, kz] --> [x*y, fitN, kz/fitN] */
        /* Need reshape output[x, y, vz] --> [x*y, 1, vz] */
        reshapeInputSize[0] = TENSOR_VIEW_SIZE_INDEX(interTensor, 0) * TENSOR_VIEW_SIZE_INDEX(interTensor, 1);
        reshapeInputSize[1] = fitN;
        reshapeInputSize[2] = TENSOR_VIEW_SIZE_INDEX(interTensor, 2) / fitN;

        reshapeOutputSize[0] = TENSOR_VIEW_SIZE_INDEX(outputs, 0) * TENSOR_VIEW_SIZE_INDEX(outputs, 1);
        reshapeOutputSize[1] = 1;
        reshapeOutputSize[2] = TENSOR_VIEW_SIZE_INDEX(outputs, 2);

        for (i = 3; i < VX_CONTEXT_TENSOR_MAX_DIMENSION; i++)
        {
            reshapeInputSize[i] = TENSOR_VIEW_SIZE_INDEX(interTensor, i);
            reshapeOutputSize[i] = TENSOR_VIEW_SIZE_INDEX(outputs, i);
        }

        convolutionReluPoolingLayer->convolution_operation.reshape_inputs = vxoTensor_ReshapeTensor(interTensor, reshapeInputSize, interTensor->dimCount);
        convolutionReluPoolingLayer->convolution_operation.reshape_outputs = vxoTensor_ReshapeTensor(outputs, reshapeOutputSize, outputs->dimCount);

        vxmASSERT(convolutionReluPoolingLayer->convolution_operation.reshape_inputs != VX_NULL && convolutionReluPoolingLayer->convolution_operation.reshape_outputs != VX_NULL);
    }

    /* Convolution operation. */
    if ((weights_biases->wb_base->do_zdp_opt || weights_biases->wb_base->do_1xN) &&
        convolutionReluPoolingLayer->convolution_operation.reshape_inputs != VX_NULL &&
        convolutionReluPoolingLayer->convolution_operation.reshape_outputs != VX_NULL &&
        !context->options.do1xnAfterSwtiling)
    {
        convolutionReluPoolingLayer->convolution_operation.inputs         = convolutionReluPoolingLayer->convolution_operation.reshape_inputs;
        convolutionReluPoolingLayer->convolution_operation.outputs        = convolutionReluPoolingLayer->convolution_operation.reshape_outputs;
    }
    else
    {
        convolutionReluPoolingLayer->convolution_operation.inputs         = interTensor;
        convolutionReluPoolingLayer->convolution_operation.outputs        = outputs;
    }
    convolutionReluPoolingLayer->convolution_operation.orig_inputs    = inputs;
    convolutionReluPoolingLayer->convolution_operation.weights_biases = weights_biases;
    convolutionReluPoolingLayer->convolution_operation.reshape_weights_biases = reshapeWb;


    vxnneLayer_SetOperation(
        &convolutionReluPoolingLayer->base,
        &convolutionReluPoolingLayer->convolution_operation.base,
        operationIndex++);

    convolutionReluPoolingLayer->base.num_temp_tensors        = tmpTensorIndex;

    inputs->brickMode = interTensor->brickMode;

    if (weights_biases->wb_base->do_fisrt_pixel_pool)
    {
        vxnneConvolutionReluPoolingOperation_Initialize(
                &convolutionReluPoolingLayer->convolution_operation,
                convolutionReluPoolingLayer->convolution_operation.inputs,
                weights_biases,
                dilationX,
                dilationY,
                pad_x_left,
                pad_x_right,
                pad_y_top,
                pad_y_bottom,
                conv_rounding_type,
                enable_relu,
                vx_true_e,
                VX_NN_POOLING_FFP,
                2,
                2,
                padMode,
                padConst,
                convolutionReluPoolingLayer->convolution_operation.outputs);
    }
    else
    {
        vxnneConvolutionReluPoolingOperation_Initialize(
                &convolutionReluPoolingLayer->convolution_operation,
                convolutionReluPoolingLayer->convolution_operation.inputs,
                weights_biases,
                dilationX,
                dilationY,
                pad_x_left,
                pad_x_right,
                pad_y_top,
                pad_y_bottom,
                conv_rounding_type,
                enable_relu,
                enable_pooling,
                pool_type,
                pool_size_x,
                pool_size_y,
                padMode,
                padConst,
                convolutionReluPoolingLayer->convolution_operation.outputs);
    }
    vxnneOperation_AddReference(&convolutionReluPoolingLayer->convolution_operation.base, (vx_reference)convolutionReluPoolingLayer->convolution_operation.inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&convolutionReluPoolingLayer->convolution_operation.base, (vx_reference)convolutionReluPoolingLayer->convolution_operation.outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);



    {
        vx_op_param_s conv = {0};

        conv.pad_x_left = pad_x_left;
        conv.pad_x_right = pad_x_right;
        conv.pad_y_top = pad_y_top;
        conv.pad_y_bottom = pad_y_bottom;
        conv.pad_mode = padMode;
        conv.pad_const = padConst != VX_NULL ? padConst->value->n32 : 0;
        conv.pool_type = pool_type;
        conv.pool_size_x = pool_size_x;
        conv.pool_size_y = pool_size_y;
        conv.pool_stride = 2;
        conv.conv_rounding_type = conv_rounding_type;
        conv.enable_relu = enable_relu;

        if (weights_biases->wb_base->do_fisrt_pixel_pool)
        {
            conv.pool_type = VX_NN_POOLING_FFP;
            conv.pool_size_x = 2;
            conv.pool_size_y = 2;
        }
        else if (enable_pooling)
        {
            conv.pool_size_x = pool_size_x;
            conv.pool_size_y = pool_size_y;
        }
        else
        {
            conv.pool_size_x = conv.pool_size_y = 0;
        }
        memcpy(&convolutionReluPoolingLayer->convolution_operation.base.parameter, &conv, sizeof(vx_op_param_s));
    }

    node->layer = &convolutionReluPoolingLayer->base;
    return status;

exit:
    if (convolutionReluPoolingLayer) gcoOS_Free(gcvNULL, (gctPOINTER)convolutionReluPoolingLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_tensor                   inputs = (vx_tensor)parameters[0];
    vx_weights_biases_parameter weights_biases = (vx_weights_biases_parameter)parameters[1]; // need modify
    vx_scalar                   pad_x_s = (vx_scalar)parameters[2];
    vx_scalar                   pad_y_s = (vx_scalar)parameters[3];
    vx_scalar                   down_scale_size_rounding_s = (vx_scalar)parameters[7];
    vx_scalar                   enable_relu_s = (vx_scalar)parameters[8];
    vx_scalar                   pool_type_s = (vx_scalar)parameters[9];
    vx_scalar                   pool_size_x_s = (vx_scalar)parameters[10];
    vx_scalar                   pool_size_y_s = (vx_scalar)parameters[11];
    vx_tensor                   outputs = (vx_tensor)parameters[12];

    vx_enum                     conv_rounding_type;
    vx_enum                     pool_type;
    vx_uint32                   pool_size_x;
    vx_uint32                   pool_size_y;
    vx_bool                     enable_relu;
    vx_uint32                   pad_x_left;
    vx_uint32                   pad_x_right;
    vx_uint32                   pad_y_top;
    vx_uint32                   pad_y_bottom;

    vx_status                   status = VX_SUCCESS;

    conv_rounding_type   = down_scale_size_rounding_s->value->e;
    pool_type            = pool_type_s->value->e;
    pool_size_x          = pool_size_x_s->value->u32;
    pool_size_y          = pool_size_y_s->value->u32;
    enable_relu          = enable_relu_s->value->b;
    pad_x_left           = pad_x_s->value->u32;
    pad_x_right          = pad_x_left;
    pad_y_top            = pad_y_s->value->u32;
    pad_y_bottom         = pad_y_top;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    status = vxnneConvolutionReluPoolingInitializer(node,
                                                     "ConvolutionReluPoolingLayer",
                                                      inputs,
                                                      weights_biases,
                                                      0,
                                                      0,
                                                      pad_x_left,
                                                      pad_x_right,
                                                      pad_y_top,
                                                      pad_y_bottom,
                                                      conv_rounding_type,
                                                      enable_relu,
                                                      vx_true_e,
                                                      pool_type,
                                                      pool_size_x,
                                                      pool_size_y,
                                                      VX_PAD_CONSTANT,
                                                      VX_NULL,
                                                      outputs);

    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConvolutionReluPoolingLayer2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status                   status = VX_SUCCESS;

    vx_tensor                   inputs = (vx_tensor)parameters[0];
    vx_weights_biases_parameter weights_biases = (vx_weights_biases_parameter)parameters[1]; // need modify
    vx_scalar                   dilation_x_s = (vx_scalar)parameters[2];
    vx_scalar                   dilation_y_s = (vx_scalar)parameters[3];
    vx_scalar                   pad_x_left_s = (vx_scalar)parameters[4];
    vx_scalar                   pad_x_right_s = (vx_scalar)parameters[5];
    vx_scalar                   pad_y_top_s = (vx_scalar)parameters[6];
    vx_scalar                   pad_y_bottom_s = (vx_scalar)parameters[7];
    vx_scalar                   down_scale_size_rounding_s = (vx_scalar)parameters[11];
    vx_scalar                   enable_relu_s = (vx_scalar)parameters[12];
    vx_scalar                   pool_type_s = (vx_scalar)parameters[13];
    vx_scalar                   pool_size_x_s = (vx_scalar)parameters[14];
    vx_scalar                   pool_size_y_s = (vx_scalar)parameters[15];
    vx_scalar                   pad_mode_s = (vx_scalar)parameters[16];
    vx_scalar                   pad_const_s = (vx_scalar)parameters[17];
    vx_tensor                   outputs = (vx_tensor)parameters[num - 1];

    if ((dilation_x_s && (dilation_x_s->value->n32) > 0) || (dilation_y_s && (dilation_y_s->value->n32) > 0))
    {
        gcmASSERT(TENSOR_SIZE_INDEX(inputs, 3) == 1);/* not support batch while dilation enabled */

        status = vxoNNDilationConvolutionLayerInitializer(node,
            inputs,
            weights_biases, VX_NULL, VX_NULL,
            pad_x_left_s, pad_x_right_s, pad_y_top_s, pad_y_bottom_s, VX_PAD_CONSTANT, VX_NULL,
            dilation_x_s, dilation_y_s, VX_NULL, VX_NULL,
            enable_relu_s,
            pool_type_s, pool_size_x_s, pool_size_y_s,
            down_scale_size_rounding_s,
            outputs
            );
    }
    else
    {
        vx_enum                     conv_rounding_type;
        vx_enum                     pool_type;
        vx_uint32                   pool_size_x;
        vx_uint32                   pool_size_y;
        vx_bool                     enable_relu;

        conv_rounding_type   = down_scale_size_rounding_s->value->e;
        pool_type            = pool_type_s->value->e;
        pool_size_x          = pool_size_x_s->value->u32;
        pool_size_y          = pool_size_y_s->value->u32;
        enable_relu          = enable_relu_s->value->b;

        /* destroy the existing layer */
        if (node->layer)
        {
            vxnneLayer_Free(node->layer);
            node->layer = VX_NULL;
        }

        status = vxnneConvolutionReluPoolingInitializer(node,
                                                         "ConvolutionReluPoolingLayer2",
                                                          inputs,
                                                          weights_biases,
                                                          (dilation_x_s == NULL) ? 0 : dilation_x_s->value->s,
                                                          (dilation_y_s == NULL) ? 0 : dilation_y_s->value->s,
                                                          pad_x_left_s->value->u32,
                                                          pad_x_right_s->value->u32,
                                                          pad_y_top_s->value->u32,
                                                          pad_y_bottom_s->value->u32,
                                                          conv_rounding_type,
                                                          enable_relu,
                                                          vx_true_e,
                                                          pool_type,
                                                          pool_size_x,
                                                          pool_size_y,
                                                          pad_mode_s->value->e,
                                                          pad_const_s,
                                                          outputs);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluPoolingLayer2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConvolutionReluLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_ConvolutionReluLayer_params) - 1) return VX_ERROR_INVALID_PARAMETERS;

    ptr->type                 = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{

    vx_tensor                   inputs = (vx_tensor)parameters[0];
    vx_weights_biases_parameter weights_biases = (vx_weights_biases_parameter)parameters[1]; // need modify
    vx_scalar                   pad_x_s = (vx_scalar)parameters[2];
    vx_scalar                   pad_y_s = (vx_scalar)parameters[3];
    vx_scalar                   down_scale_size_rounding_s = (vx_scalar)parameters[7];
    vx_scalar                   enable_relu_s = (vx_scalar)parameters[8];
    vx_tensor                   outputs = (vx_tensor)parameters[9];
    vx_enum                     conv_rounding_type;
    vx_bool                     enable_relu;
    vx_uint32                   pad_x_left;
    vx_uint32                   pad_x_right;
    vx_uint32                   pad_y_top;
    vx_uint32                   pad_y_bottom;

    vx_status                   status = VX_SUCCESS;

    conv_rounding_type   = down_scale_size_rounding_s->value->e;
    enable_relu          = enable_relu_s->value->b;
    pad_x_left           = pad_x_s->value->u32;
    pad_x_right          = pad_x_left;
    pad_y_top            = pad_y_s->value->u32;
    pad_y_bottom         = pad_y_top;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    status = vxnneConvolutionReluPoolingInitializer(node,
                                                      "ConvolutionReluLayer",
                                                      inputs,
                                                      weights_biases,
                                                      0,
                                                      0,
                                                      pad_x_left,
                                                      pad_x_right,
                                                      pad_y_top,
                                                      pad_y_bottom,
                                                      conv_rounding_type,
                                                      enable_relu,
                                                      vx_false_e,
                                                      VIV_NN_POOLING_NON,
                                                      0,
                                                      0,
                                                      VX_PAD_CONSTANT,
                                                      VX_NULL,
                                                      outputs);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionReluLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNFullyConnectedReluLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedReluLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedReluLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_FullyConnectedReluLayer_params) - 1) return VX_ERROR_INVALID_PARAMETERS;

    ptr->type                 = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

vx_status vxoNNFullyConnectedLayerInitializer(
    vx_node node,
    vxnne_layer layer,
    vxnne_tp_operation tp_operation0,
    vxnne_tp_operation tp_operation1,
    vxnne_convolution_relu_pooling_operation nn_operation,
    vxnne_shader_operation sh_operation,
    vx_tensor inputs,
    vx_weights_biases_parameter weights_biases,
    vx_uint32 pad,
    vx_enum conv_rounding_type,
    vx_bool enable_relu,
    vx_int32_ptr count,
    vx_tensor outputs)
{
    vx_status status = VX_SUCCESS;
    vx_context context = vxGetContext((vx_reference)node);
    vx_uint32 i = 0, batchCount = 1;
    vx_uint32 phys;
    vx_bool aligned64 = vx_true_e;
    vx_bool   enable_shader               = vx_false_e;
    vx_bool   supportDataFormat0          = vx_false_e;
    vx_bool   supportDataFormat1          = vx_false_e;
    vx_bool   supportDataFormat2          = vx_false_e;
    vx_bool   supportDataFormat3          = vx_false_e;
    vx_enum   input_dataformat            = TENSOR_DATA_TYPE(inputs);
    vx_enum   weight_dataformat           = TENSOR_DATA_TYPE(weights_biases->wb_base->origWeight);
    vx_enum   bias_dataformat             = weights_biases->wb_base->origBias ? TENSOR_DATA_TYPE(weights_biases->wb_base->origBias) : VX_TYPE_INVALID;
    vx_enum   output_dataformat           = TENSOR_DATA_TYPE(outputs);
    vx_uint32 dims                        = TENSOR_VIEW_DIM_NUM(inputs);
    vx_uint32 width                       = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32 height                      = (dims > 1) ? TENSOR_VIEW_SIZE_INDEX(inputs, 1) : 1;
    vx_uint32 depth                       = (dims > 2) ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
    vx_uint32 inputDims                   = width * height * depth;
    vx_uint32 op_index = 0;

    supportDataFormat0 = (vx_bool)(input_dataformat == VX_TYPE_FLOAT16 && weight_dataformat == VX_TYPE_FLOAT16 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_FLOAT32) && output_dataformat == VX_TYPE_FLOAT16);
    supportDataFormat1 = (vx_bool)(input_dataformat == VX_TYPE_INT8 && weight_dataformat == VX_TYPE_INT8 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_INT8);
    supportDataFormat2 = (vx_bool)(input_dataformat == VX_TYPE_INT16 && weight_dataformat == VX_TYPE_INT16 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_INT16);
    supportDataFormat3 = (vx_bool)(input_dataformat == VX_TYPE_UINT8 && weight_dataformat == VX_TYPE_UINT8 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_UINT8);
    enable_shader      = (supportDataFormat0 || supportDataFormat1 || supportDataFormat2 || supportDataFormat3) && (inputDims < IMG_MAX_WIDTH);

    if (TENSOR_DIM_NUM(inputs) == 2)
    {
        batchCount = TENSOR_SIZE_INDEX(inputs, 1);
        if ((inputs->dims[0] != weights_biases->weights_sizes[2]) ||
            (outputs->dims[0] != weights_biases->weights_sizes[3]))
        {
            vxError("parameter is invalid at function %s, line %d\n", __FUNCTION__, __LINE__);
            vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
        }
    }
    else if (TENSOR_DIM_NUM(inputs) == 4)
    {
        batchCount = TENSOR_SIZE_INDEX(inputs, 3);
        if (((inputs->dims[0] * inputs->dims[1] * inputs->dims[2]) != weights_biases->weights_sizes[2]) ||
            ((outputs->dimCount == 4) && (outputs->dims[2] !=weights_biases->weights_sizes[3])) ||
            ((outputs->dimCount == 2) && (outputs->dims[0] !=weights_biases->weights_sizes[3])))
        {
            vxError("parameter is invalid at function %s, line %d\n", __FUNCTION__, __LINE__);
            vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
        }
    }

    /* TP needs 64-byte aligned inputs. */
    vxoTensor_GetTensorViewMemory(inputs, VX_NULL, &phys);
    if ((phys % 64) ||
        ((batchCount > 1) &&
         (inputs->strides[TENSOR_DIM_NUM(inputs) - 1] % 64)))
    {
        aligned64 = vx_false_e;
    }

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(context, inputs, weights_biases, outputs) &&
        weights_biases->use_tp_fc &&
        WB_IS_NN_FC_BATCH_MODE(weights_biases) == vx_false_e &&
        aligned64)
    {
        vx_op_param conv = VX_NULL;
        vx_uint32 kzgroup = weights_biases->weights_sizes[2] / weights_biases->slice_array[0].kz_count;
        vx_uint32 zoffset = 0;

       vxmONERROR(vxnneOperation_Initialize(&tp_operation0->base,
                                            layer,
                                            VXNNE_OPERATION_TARGET_TP,
                                            VXNNE_OPERATOR_FULLYCONNECTED,
                                            VX_NULL,
                                            vxnneOperation_TP_Deinitialize,
                                            batchCount,
                                            0));

        node->layer = layer;

        conv = &tp_operation0->base.parameter;
        conv->pad_x_left = pad;
        conv->pad_y_top = pad;
        conv->pad_x_right = pad;
        conv->pad_y_bottom = pad;
        conv->pool_size_x = 0;
        conv->pool_size_y = 0;
        conv->pool_stride = 1;
        conv->enable_relu = enable_relu;
        conv->conv_rounding_type = 0;
        conv->pad_mode = VX_PAD_CONSTANT;
        conv->pad_const = 0;

        conv->tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(weights_biases->slice_num * sizeof(vx_tp_value_cmd_s));

        if (kzgroup == 1)
        {
            conv->tpType = TP_SINGLE_FC;
            conv->other_ref = (vx_reference)weights_biases;
            conv->data_buff = gcvNULL;

            for (i = 0; i < weights_biases->slice_num; i++)
            {
                conv->tp_value[i].u32[0] = kzgroup;
                conv->tp_value[i].u32[1] = zoffset;
                zoffset += weights_biases->slice_array[i].z_count;
            }

            tp_operation0->input          = inputs;
            tp_operation0->weights_biases = weights_biases;
            tp_operation0->output         = outputs;

            vxnneOperation_AddReference(&tp_operation0->base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tp_operation0->base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                layer,
                &tp_operation0->base,
                (*count) ++);
        }
        else
        {
            vx_tensor tmpTensor;
            vx_uint32 size = kzgroup * weights_biases->weights_sizes[3] * TENSOR_DATA_SIZE(outputs);
            vx_uint32 kzoffset = 0, kzoffset2 = 0;

            vx_tensor_create_params_t tensor_create_params;

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 1;
            tensor_create_params.sizes = &size;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(outputs);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(outputs);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(outputs);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(outputs);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(outputs);
            }

            vxmONERROR(vxnneOperation_Initialize(&tp_operation1->base,
                                                 layer,
                                                 VXNNE_OPERATION_TARGET_TP,
                                                 VXNNE_OPERATOR_FULLYCONNECTED,
                                                 VX_NULL,
                                                 vxnneOperation_TP_Deinitialize,
                                                 batchCount,
                                                 0));

            tmpTensor = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_true_e);
            if (tmpTensor == VX_NULL)
            {
                vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                vxmONERROR(VX_ERROR_NO_MEMORY);
            }

            conv->tpType = TP_SINGLE_FC;
            conv->other_ref = (vx_reference)weights_biases;
            conv->data_buff = gcvNULL;

            for (i = 0; i < weights_biases->slice_num; i++)
            {
                conv->tp_value[i].e32[0] = 0;
                conv->tp_value[i].u32[0] = kzgroup;
                conv->tp_value[i].u32[1] = zoffset;
                conv->tp_value[i].u32[2] = kzoffset;
                conv->tp_value[i].u32[3] = kzoffset2;

                if (i % kzgroup == kzgroup - 1)
                {
                    kzoffset = kzoffset2 = 0;
                    zoffset += weights_biases->slice_array[i].z_count;
                }
                else
                {
                    kzoffset += weights_biases->slice_array[i].kz_count;
                    kzoffset2 += weights_biases->weights_sizes[3];
                }
            }

            vxnneLayer_SetOperation(
                layer,
                &tp_operation0->base,
                (*count) ++);
            tp_operation0->weights_biases = weights_biases;
            tp_operation0->input          = inputs;
            tp_operation0->output         = tmpTensor;

            vxnneOperation_AddReference(&tp_operation0->base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tp_operation0->base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            conv = &tp_operation1->base.parameter;

            conv->pad_x_left = pad;
            conv->pad_y_top = pad;
            conv->pad_x_right = pad;
            conv->pad_y_bottom = pad;
            conv->pool_size_x = 0;
            conv->pool_size_y = 0;
            conv->pool_stride = 1;
            conv->enable_relu = enable_relu;
            conv->conv_rounding_type = 0;
            conv->pad_mode = VX_PAD_CONSTANT;
            conv->pad_const = 0;

            conv->tpType = TP_SINGLE_FC;
            conv->other_ref = gcvNULL;
            conv->data_buff = gcvNULL;
            conv->tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
            conv->tp_value->e32[0] = 1;
            conv->tp_value->u32[0] = kzgroup;
            conv->tp_value->u32[1] = weights_biases->weights_sizes[3];

            vxnneLayer_SetOperation(
                layer,
                &tp_operation1->base,
                (*count) ++);
            tp_operation1->input  = tmpTensor;
            tp_operation1->output = outputs;

            vxnneOperation_AddReference(&tp_operation1->base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tp_operation1->base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            layer->num_temp_tensors = 1;
            layer->temp_tensors[0] = tmpTensor;
        }
    }
    else if (vxnneIsNNSupportFormat(context, inputs, weights_biases, outputs) &&
        (nn_operation != VX_NULL))
    {
        vx_op_param_s conv = {0};
        vx_tensor reshapeInput = VX_NULL, reshapeOutput = VX_NULL;
        vx_uint32 reshapeDimCount = 0;

        vxmONERROR(vxnneOperation_Initialize(&nn_operation->base,
                                             layer,
                                             VXNNE_OPERATION_TARGET_NN,
                                             VXNNE_OPERATOR_FULLYCONNECTED,
                                             VX_NULL,
                                             VX_NULL,
                                             batchCount,
                                             NNE_COMMAND_SIZE * weights_biases->slice_num));

        conv.pad_x_left = conv.pad_x_right = conv.pad_y_top = conv.pad_y_bottom = pad;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.pool_type = VIV_NN_POOLING_NON;
        conv.pool_size_x = conv.pool_size_y = 0;
        conv.conv_rounding_type = conv_rounding_type;
        conv.enable_relu = enable_relu;

        vxnneLayer_SetOperation(
            layer,
            &nn_operation->base,
            (*count) ++);

        if ((inputs->dimCount == 1) ||
            (inputs->dimCount == 2) ||
            (((inputs->dimCount == 3) || (inputs->dimCount == 4)) && ((inputs->dims[0] == 1) || (inputs->dims[1] != 1))))
        {
            vx_int32 reshapeSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
            reshapeSize[0] = 1;
            reshapeSize[1] = 1;

            if ((inputs->dimCount == 1) || (inputs->dimCount == 2))
            {
                for (i = 2; i < inputs->dimCount + 2; i++)
                    reshapeSize[i] = inputs->dims[i-2];

                reshapeDimCount = inputs->dimCount + 2;
            }
            else if (((inputs->dimCount == 3) || (inputs->dimCount == 4)) && ((inputs->dims[0] == 1) || (inputs->dims[1] != 1)))
            {
                reshapeSize[2] = inputs->dims[0] * inputs->dims[1] * inputs->dims[2];
                reshapeDimCount = inputs->dimCount;
            }

            reshapeInput = vxoTensor_ReshapeTensor(inputs, reshapeSize, reshapeDimCount);

            vxmASSERT(reshapeInput != VX_NULL);

            nn_operation->inputs                   = reshapeInput;
            vxnneOperation_AddReference(&nn_operation->base, (vx_reference)reshapeInput, VXNNE_OPERATION_REFENRENCE_INPUT);
        }
        else
        {
            nn_operation->inputs                   = inputs;
            vxnneOperation_AddReference(&nn_operation->base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        }

        if (outputs->dimCount == 1 || outputs->dimCount == 2)
        {
            vx_int32 reshapeSize[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
            reshapeSize[0] = 1;
            reshapeSize[1] = 1;

            for (i = 2; i < outputs->dimCount + 2; i++)
                reshapeSize[i] = outputs->dims[i-2];

            reshapeOutput = vxoTensor_ReshapeTensor(outputs, reshapeSize, outputs->dimCount + 2);

            vxmASSERT(reshapeOutput != VX_NULL);

            nn_operation->outputs                   = reshapeOutput;
            vxnneOperation_AddReference(&nn_operation->base, (vx_reference)reshapeOutput, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            nn_operation->outputs                  = outputs;
            vxnneOperation_AddReference(&nn_operation->base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        nn_operation->weights_biases           = weights_biases;
        nn_operation->pad_x_left               = pad;
        nn_operation->pad_x_right              = pad;
        nn_operation->pad_y_top                = pad;
        nn_operation->pad_y_bottom             = pad;
        nn_operation->down_scale_size_rounding = conv_rounding_type;
        nn_operation->enable_relu              = enable_relu;

        memcpy(&nn_operation->base.parameter, &conv, sizeof(vx_op_param_s));
    }
    else if (enable_shader && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxmONERROR(vxoFCOperationSH_Initialize(sh_operation,
                                               layer,
                                               inputs,
                                               1,
                                               weights_biases->wb_base->origWeight,
                                               weights_biases->wb_base->origBias,
                                               0,
                                               0,
                                               enable_relu,
                                               outputs,
                                               &op_index));
    }
    else
    {
         /* TODO: SW path. */
    }

    node->layer = layer;

OnError:
    return status;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedReluLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_context                  context = vxGetContext((vx_reference)node);
    vx_tensor                   inputs = (vx_tensor)parameters[0];
    vx_weights_biases_parameter weights_biases = (vx_weights_biases_parameter)parameters[1]; // need modify
    vx_scalar                   pad_s = (vx_scalar)parameters[2];
    vx_scalar                   down_scale_size_rounding_s = (vx_scalar)parameters[6];
    vx_scalar                   enable_relu_s = (vx_scalar)parameters[7];
    vx_tensor                   outputs = (vx_tensor)parameters[8];

    vx_uint32                   pad;
    vx_enum                     conv_rounding_type;
    vx_bool                     enable_relu;
    vx_int32                    count = 0;

    vx_status                   status = VX_SUCCESS;
    vxnne_fully_connected_relu_layer  fullyConnectReluLayer = gcvNULL;

    if (!vxnneIsNNSupportFormat(context, inputs, weights_biases, outputs) &&
        !vxnneIsTPSupportFormat(context, inputs, weights_biases, outputs))
    {
        vxError("hw not support this format. function %s line %d", __FUNCTION__, __LINE__);
        status = VX_ERROR_NOT_SUPPORTED;
        goto exit;
    }


    if (TENSOR_DIM_NUM(inputs) == 2)
    {
        if ((inputs->dims[0] != weights_biases->weights_sizes[2]) ||
            (outputs->dims[0] != weights_biases->weights_sizes[3]))
        {
            vxError("parameter is invalid at function %s, line %d\n", __FUNCTION__, __LINE__);
            status = VX_ERROR_INVALID_PARAMETERS;
            goto exit;
        }
    }
    else if (TENSOR_DIM_NUM(inputs) == 4)
    {
        if (((inputs->dims[0] * inputs->dims[1] * inputs->dims[2]) != weights_biases->weights_sizes[2]) ||
            ((outputs->dimCount == 4) && (outputs->dims[2] !=weights_biases->weights_sizes[3])) ||
            ((outputs->dimCount == 2) && (outputs->dims[0] !=weights_biases->weights_sizes[3])))
        {
            vxError("parameter is invalid at function %s, line %d\n", __FUNCTION__, __LINE__);
            status = VX_ERROR_INVALID_PARAMETERS;
            goto exit;
        }
    }
    pad                  = pad_s->value->u32;
    conv_rounding_type   = down_scale_size_rounding_s->value->e;
    enable_relu          = enable_relu_s->value->b;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_fully_connected_relu_layer_s), (gctPOINTER*)&fullyConnectReluLayer);
    if (!fullyConnectReluLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(fullyConnectReluLayer, sizeof(vxnne_fully_connected_relu_layer_s));

    vxnneLayer_Initialize(&fullyConnectReluLayer->base,
                            "FullyConnectedReluLayer",
                            node,
                            vxmOPERATION_COUNT(fullyConnectReluLayer),
                            fullyConnectReluLayer->operations,
                            VX_NULL);

    status = vxoNNFullyConnectedLayerInitializer(
        node,
        &fullyConnectReluLayer->base,
        &fullyConnectReluLayer->fully_connected_TPoperation[0],
        &fullyConnectReluLayer->fully_connected_TPoperation[1],
        &fullyConnectReluLayer->fully_connected_relu_operation,
        &fullyConnectReluLayer->fully_connected_SHoperation,
        inputs,
        weights_biases,
        pad,
        conv_rounding_type,
        enable_relu,
        &count,
        outputs);

    if (status != VX_SUCCESS && fullyConnectReluLayer) gcoOS_Free(gcvNULL, (gctPOINTER)fullyConnectReluLayer);

exit:
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedReluLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }
    return VX_SUCCESS;
}

vx_status VX_CALLBACK vxoBaseKernel_NNSoftmaxLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_Softmax_params) - 1) return VX_ERROR_INVALID_PARAMETERS;

    ptr->type                 = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  outputs                    = (vx_tensor)parameters[1];
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_uint32  dims                       = TENSOR_DIM_NUM(inputs);
    vx_uint32  width                      = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32  height                     = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
    vx_bool    useShadeExe                = vx_false_e;
    vx_bool    enable_format              = vx_false_e;
    vx_bool    enable_tf_quantize         = vx_false_e;
    vx_bool    enable_float32             = vx_false_e;
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_float32 beta                       = 1.0;
    vxnne_softmax_layer  softmaxLayer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_softmax_layer_s), (gctPOINTER*)&softmaxLayer);
    if (!softmaxLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(softmaxLayer, sizeof(vxnne_softmax_layer_s));

    vxnneLayer_Initialize(&softmaxLayer->base,
                          "SoftmaxLayer",
                          node,
                          vxmOPERATION_COUNT(softmaxLayer),
                          softmaxLayer->operations,
                          VX_NULL);

    switch(dims)
    {
        case 1:
            batchCount   = 1;
            break;
        case 2:
            batchCount   = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
            break;
        case 3:
            batchCount   = 1;
            break;
        case 4:
            batchCount   = TENSOR_VIEW_SIZE_INDEX(inputs, 3);
            break;
        default:
            vxError("Input tensor error dimension[%u]\n", dims);
            status = VX_ERROR_INVALID_DIMENSION;
            goto exit;
    }

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enable_float32 = (vx_bool)(outputFormat == VX_TYPE_FLOAT32 && ((width % 4 == 0) || ((width * height < IMG_MAX_WIDTH) && ((width * height % 4 == 0) || dims < 3)) || dims == 1));
        enable_format = (((inputFormat == VX_TYPE_INT8 ||  inputFormat == VX_TYPE_FLOAT16) && (outputFormat == VX_TYPE_FLOAT16 || enable_float32)) || (inputFormat == VX_TYPE_INT16 && (outputFormat == VX_TYPE_INT16 || outputFormat == VX_TYPE_FLOAT16))
                         || (inputFormat == VX_TYPE_INT8 &&  outputFormat == VX_TYPE_INT8));
        enable_tf_quantize = ((inputFormat == VX_TYPE_UINT8) && (outputFormat == VX_TYPE_FLOAT16 || enable_float32 || outputFormat == VX_TYPE_UINT8));
    }
    else
    {
        enable_format = ((inputFormat == VX_TYPE_FLOAT32 ||  inputFormat == VX_TYPE_FLOAT16) && (outputFormat == VX_TYPE_FLOAT16 || outputFormat == VX_TYPE_FLOAT32));
        enable_tf_quantize = ((inputFormat == VX_TYPE_UINT8) && (outputFormat == VX_TYPE_FLOAT16 || outputFormat == VX_TYPE_FLOAT32 || outputFormat == VX_TYPE_UINT8));
    }
    /* Current the SH layer only process 3D tensor*/
    useShadeExe  = (enable_format || enable_tf_quantize);

   if(useShadeExe && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
   {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if (dims == 2) batchCount = 1;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetSoftmaxShaderExecutable(node->base.context, VXNNE_KERNEL_SOFTMAX, &node->kernelAttributes.borderMode, dims, inputs, beta, outputs);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
        }
        else
        {
            vx_scalar beta_s = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &beta);

            shaderExecutable = vxnneGetGPUSoftmaxShaderExecutable(node->base.context, VXNNE_KERNEL_SOFTMAX, &node->kernelAttributes.borderMode, beta_s, inputs, outputs);

            if (beta_s) vxReleaseScalar(&beta_s);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
        }

        status = vxnneShaderOperation_Initialize(&softmaxLayer->softmax_SHoperation,
            &softmaxLayer->base,
            VXNNE_OPERATOR_SOFTMAX,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS) {
            goto exit;
        }

        vxnneOperation_AddReference(&softmaxLayer->softmax_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&softmaxLayer->softmax_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        vxnneLayer_SetOperation(&softmaxLayer->base, &softmaxLayer->softmax_SHoperation.base, 0);
    }
    else
    {
        vxnneOperation_Initialize(&softmaxLayer->softmax_sw_operation.base,
            &softmaxLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_SOFTMAX,
            vxnneExecuteSWSoftmax,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(&softmaxLayer->base, &softmaxLayer->softmax_sw_operation.base, 0);
        softmaxLayer->softmax_sw_operation.inputs           = inputs;
        softmaxLayer->softmax_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&softmaxLayer->softmax_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&softmaxLayer->softmax_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &softmaxLayer->base;
    return status;

exit:
    if (softmaxLayer) {
        gcoOS_Free(gcvNULL, (gctPOINTER)softmaxLayer);
        softmaxLayer = VX_NULL;
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConcat2Layer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcat2Layer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcat2Layer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcat2Layer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  in0                     = (vx_tensor)parameters[0];
    vx_tensor  in1                     = (vx_tensor)parameters[1];
    vx_tensor  out                     = (vx_tensor)parameters[2];
    vx_uint32 batchCount               = TENSOR_SIZE_INDEX(in0, 3);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        vxnne_concat2_layer  concatLayer = VX_NULL;

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_concat2_layer_s), (gctPOINTER*)&concatLayer);
        if (!concatLayer)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }

        gcoOS_ZeroMemory(concatLayer, sizeof(vxnne_concat2_layer_s));

        vxnneLayer_Initialize(&concatLayer->base,
                              "ConcatLayer",
                              node,
                              vxmOPERATION_COUNT(concatLayer),
                              concatLayer->operations,
                              VX_NULL);

        vxnneOperation_Initialize(&concatLayer->concat2_operation.base,
                                  &concatLayer->base,
                                  VXNNE_OPERATION_TARGET_SW,
                                  VXNNE_OPERATOR_CONCAT2,
                                  vxnneExecuteSWConcat2,
                                  VX_NULL,
                                  batchCount,
                                  0);

        vxnneLayer_SetOperation(&concatLayer->base, &concatLayer->concat2_operation.base, 0);
        concatLayer->concat2_operation.inputs0           = in0;
        concatLayer->concat2_operation.inputs1           = in1;
        concatLayer->concat2_operation.outputs           = out;

        vxnneOperation_AddReference(&concatLayer->concat2_operation.base, (vx_reference)in0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&concatLayer->concat2_operation.base, (vx_reference)in1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&concatLayer->concat2_operation.base, (vx_reference)out, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        node->layer = &concatLayer->base;
    }

exit:
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcat2Layer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorCopy(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorCopy_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorCopy_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

#define INPUT_SIZE_ALIGN_4  (4)
VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorCopy_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  src                     = (vx_tensor)parameters[0];
    vx_tensor  dst                     = (vx_tensor)parameters[1];
    vx_uint32  batchCount              = TENSOR_VIEW_DIM_NUM(src) > 3 ? TENSOR_SIZE_INDEX(src, TENSOR_VIEW_DIM_NUM(src) - 1) : 1;
    vx_enum    inputFormat             = TENSOR_DATA_TYPE(src);
    vx_enum    outputFormat            = TENSOR_DATA_TYPE(dst);
    vx_bool    shExe_flag              = vx_false_e;
    vx_bool    enable_dataConv2F32     = vx_false_e;
    vx_uint32  reshpTensor_Sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {1};
    vx_uint32  reshpTensor_Dims           = 2;
    vxnne_tensor_copy  copyNode        = VX_NULL;
    vx_context context                 = vxGetContext((vx_reference)node);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        vx_uint32 src_elementCount = 0;
        vx_uint32 dst_elementCount = 0;

        status = vxoTensor_GetTensorElementCount(src, &src_elementCount);
        status = vxoTensor_GetTensorElementCount(dst, &dst_elementCount);

        if(context->evisNoInst.supportEVIS)
        {
            if (src_elementCount < IMG_MAX_WIDTH)
            {
                reshpTensor_Sizes[0]   = src_elementCount;
                reshpTensor_Sizes[1]   = 1;
                reshpTensor_Dims = 2;

                if (inputFormat != VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                    enable_dataConv2F32 = vx_true_e;
            }
            else
            {
                vx_uint32 gcd = outputFormat == VX_TYPE_FLOAT32 ? 4 : 1;
                vx_uint32 divisors = 1;
                vx_uint32 element_count = src_elementCount;

                vxoGetDataDivisors(element_count, &divisors, gcd);
                reshpTensor_Sizes[0] = divisors;
                element_count = element_count / divisors;
                vxoGetDataDivisors(element_count, &divisors, 1);
                reshpTensor_Sizes[1] = divisors;
                element_count = element_count / divisors;
                reshpTensor_Sizes[2] = element_count;
                reshpTensor_Dims = 3;

                if (inputFormat != VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32
                    && (reshpTensor_Sizes[0] % INPUT_SIZE_ALIGN_4 == 0))
                    enable_dataConv2F32 = vx_true_e;
            }

            shExe_flag = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat != VX_TYPE_FLOAT32)
                                 || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
                                 || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)
                                 || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_FLOAT16)
                                 || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT16)
                                 || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                 || (inputFormat == VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32)
                                 || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16)
                                 || enable_dataConv2F32)
                                 && src_elementCount == dst_elementCount);
        }
        else
        {
            vxoElementOptimization_GetTensorShape(src, reshpTensor_Sizes, &reshpTensor_Dims);

            shExe_flag = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                                 || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                                 || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                 || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT32)
                                 || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16))
                                 && src_elementCount == dst_elementCount);
        }

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_copy_s), (gctPOINTER*)&copyNode);
        if (!copyNode)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }

        gcoOS_ZeroMemory(copyNode, sizeof(vxnne_tensor_copy_s));

        vxnneLayer_Initialize(&copyNode->base,
                              "TensorCopy",
                              node,
                              vxmOPERATION_COUNT(copyNode),
                              copyNode->operations,
                              VX_NULL);

        if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
            (TENSOR_VIEW_SIZE_INDEX(dst, 0) * TENSOR_VIEW_SIZE_INDEX(dst, 1) * TENSOR_VIEW_SIZE_INDEX(dst, 2) > 1) &&
            vxnneIsTPSupportFormat(context, src, VX_NULL, dst))
        {
            vx_op_param_s conv = {0};

            status = vxnneOperation_Initialize(&copyNode->tensor_copy_tp_operation.base,
                &copyNode->base,
                VXNNE_OPERATION_TARGET_TP,
                VXNNE_OPERATOR_TENSOR_COPY,
                VX_NULL,
                 vxnneOperation_TP_Deinitialize,
                batchCount,
                0);

            if (status != VX_SUCCESS) goto exit;

            memset(&conv, 0, sizeof(vx_op_param_s));

            conv.enable_relu = vx_false_e;
            conv.pool_stride = 1;
            conv.tpType = TP_TENSOR_COPY;

            memcpy(&copyNode->tensor_copy_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

            vxnneOperation_AddReference(&copyNode->tensor_copy_tp_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&copyNode->tensor_copy_tp_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            copyNode->tensor_copy_tp_operation.input = src;
            copyNode->tensor_copy_tp_operation.output = dst;

            vxnneLayer_SetOperation(
                &copyNode->base,
                &copyNode->tensor_copy_tp_operation.base,
                0);
        }
        else
        {
            if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) )
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;
                vx_tensor input      = NULL;
                vx_tensor output     = NULL;

                input     = vxoTensor_ReshapeTensor(src, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);
                output     = vxoTensor_ReshapeTensor(dst, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    if (input && output)
                        shaderExecutable = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, output);
                }
                else
                {
                    if (input && output)
                        shaderExecutable = vxnneGPUTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, output);
                }

                if (input) vxoTensor_ReleaseTensor(&input);
                if (output) vxoTensor_ReleaseTensor(&output);

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&copyNode->tensor_copy_sh_operation,
                    &copyNode->base,
                    VXNNE_OPERATOR_CONVERT_FORMAT,
                    1, /*batchCount is 1 after reshape tensor object*/
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&copyNode->tensor_copy_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&copyNode->tensor_copy_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                vxnneLayer_SetOperation(&copyNode->base, &copyNode->tensor_copy_sh_operation.base, 0);
            }
            else
            {
                vxnneOperation_Initialize(&copyNode->tensor_copy_operation.base,
                    &copyNode->base,
                    VXNNE_OPERATION_TARGET_SW,
                    VXNNE_OPERATOR_TENSOR_COPY,
                    vxnneExecuteSWTensorCopy,
                    VX_NULL,
                    batchCount,
                    0);
                vxnneLayer_SetOperation(&copyNode->base, &copyNode->tensor_copy_operation.base, 0);

                copyNode->tensor_copy_operation.src           = src;
                copyNode->tensor_copy_operation.dst           = dst;

                vxnneOperation_AddReference(&copyNode->tensor_copy_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&copyNode->tensor_copy_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
        }

        node->layer = &copyNode->base;
    }

    return status;

exit:
    if(copyNode) gcoOS_Free(NULL, (gctPOINTER)copyNode);
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorCopy_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorReverse(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReverse_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReverse_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReverse_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status     = VX_SUCCESS;
    vx_tensor  input      = (vx_tensor)parameters[0];
    vx_tensor  output     = (vx_tensor)parameters[6];
    vx_uint32  numOfAxis  = ((vx_scalar)parameters[1])->value->u32;
    vx_uint32  batchCount = (TENSOR_SIZE_INDEX(input, TENSOR_VIEW_DIM_NUM(input)) == 0) ? 1 : TENSOR_SIZE_INDEX(input, TENSOR_VIEW_DIM_NUM(input));
    vx_context context = vxGetContext((vx_reference)node);
    vxnne_tensor_reverse  reverseNode = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        vx_uint32 i;
        vx_bool dataFormat_flag, axFlag;

        axFlag = vx_true_e;

        dataFormat_flag = (vx_bool)(!checkOutputTensorDoAlu(input, output));
        for (i = 0; i <numOfAxis; i++)
        {
           if(((vx_scalar)parameters[i + 2])->value->n32 == 3)
           {
               axFlag = vx_false_e;
               break;
           }
        }

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_reverse_s), (gctPOINTER*)&reverseNode);
        if (!reverseNode)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }

        gcoOS_ZeroMemory(reverseNode, sizeof(vxnne_tensor_reverse_s));

        vxnneLayer_Initialize(&reverseNode->base,
                              "TensorReverse",
                              node,
                              vxmOPERATION_COUNT(reverseNode),
                              reverseNode->operations,
                              VX_NULL);

        if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_REVERSE) &&
            vxnneIsTPSupportFormat(context, input, VX_NULL, output))
        {
            vx_op_param_s conv = {0};
            vx_int32 axis[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};

            for (i = 0; i <numOfAxis; i++)
            {
                axis[i] = ((vx_scalar)parameters[i + 2])->value->n32;
            }

            status = vxnneOperation_Initialize(&reverseNode->tensor_reverse_tp_operation.base,
                                               &reverseNode->base,
                                               VXNNE_OPERATION_TARGET_TP,
                                               VXNNE_OPERATOR_TENSOR_REVERSE,
                                               VX_NULL,
                                               vxnneOperation_TP_Deinitialize,
                                               batchCount,
                                               0);
            if (status != VX_SUCCESS) goto exit;
            vxnneLayer_SetOperation(&reverseNode->base, &reverseNode->tensor_reverse_tp_operation.base, 0);
            reverseNode->tensor_reverse_tp_operation.input  = input;
            reverseNode->tensor_reverse_tp_operation.output = output;

            vxnneOperation_AddReference(&reverseNode->tensor_reverse_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reverseNode->tensor_reverse_tp_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            conv.pad_x_left = 0;
            conv.pad_y_top = 0;
            conv.pool_size_x = 0;
            conv.pool_size_y = 0;
            conv.pool_stride = 1;
            conv.enable_relu = vx_false_e;
            conv.conv_rounding_type = 0;
            conv.pad_mode = VX_PAD_CONSTANT;
            conv.pad_const = 0;
            conv.tpType = TP_REVERSE;
            conv.data_buff = gcvNULL;
            conv.other_ref = (vx_reference)input;
            conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
            conv.tp_value->u32[0] = numOfAxis;
            conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(axis));
            vxMemCopy(conv.tp_value->p8[0], axis, sizeof(axis));

            vxMemCopy(&reverseNode->tensor_reverse_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));
        }
        else if(dataFormat_flag && vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)
            && numOfAxis < 4 && axFlag)
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_uint32 axis[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {8, 8, 8, 8, 8, 8};
            for (i = 0; i <numOfAxis; i++)
            {
                axis[i] = ((vx_scalar)parameters[i + 2])->value->n32;
            }

            shaderExecutable = vxnneGetReverseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_REVERSE, &node->kernelAttributes.borderMode,
                input, output, numOfAxis, axis);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&reverseNode->tensor_reverse_sh_operation,
                &reverseNode->base,
                VXNNE_OPERATOR_TENSOR_REVERSE,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&reverseNode->tensor_reverse_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reverseNode->tensor_reverse_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                &reverseNode->base,
                &reverseNode->tensor_reverse_sh_operation.base,
                0);
        }
        else
        {
            vxnneOperation_Initialize(&reverseNode->tensor_reverse_sw_operation.base,
                &reverseNode->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_TENSOR_REVERSE,
                vxnneExecuteSWTensorReverse,
                VX_NULL,
                batchCount,
                0);

            vxnneLayer_SetOperation(&reverseNode->base, &reverseNode->tensor_reverse_sw_operation.base, 0);
            reverseNode->tensor_reverse_sw_operation.input           = input;
            reverseNode->tensor_reverse_sw_operation.output          = output;
            reverseNode->tensor_reverse_sw_operation.numOfAxis       = numOfAxis;
            for (i = 0; i <numOfAxis; i++)
            {
                reverseNode->tensor_reverse_sw_operation.axis[i] = (vx_scalar)parameters[i + 2];
            }

            vxnneOperation_AddReference(&reverseNode->tensor_reverse_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reverseNode->tensor_reverse_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        node->layer = &reverseNode->base;
    }

    return status;

exit:
    if(reverseNode) gcoOS_Free(gcvNULL, reverseNode);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReverse_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoInternelKernel_NNTensorReduceSum(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReduceSum_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReduceSum_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReduceSum_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  src                     = (vx_tensor)parameters[0];
    vx_tensor  dst                     = (vx_tensor)parameters[1];
    vx_scalar  reduceDim               = (vx_scalar)parameters[2];
    vx_scalar  keepDim                 = (vx_scalar)parameters[3];
    vx_uint32  batchCount              = TENSOR_SIZE_INDEX(src, 3);

    vx_uint32  axis                    = 1024;
    vx_bool    shExe_flag              = vx_false_e;
    vx_enum    inputFormat             = TENSOR_DATA_TYPE(src);
    vx_enum    outputFormat            = TENSOR_DATA_TYPE(dst);
    vx_uint32  tmpTensorIndex          = 0;
    vx_uint32  operationIdx            = 0;

    vx_context context                 = vxGetContext((vx_reference)node);

    vxnne_tensor_reduce_sum  reduceNode = VX_NULL;
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_reduce_sum_s), (gctPOINTER*)&reduceNode);
    if (!reduceNode)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(reduceNode, sizeof(vxnne_tensor_reduce_sum_s));

    vxnneLayer_Initialize(&reduceNode->base,
        "TensorReduceSum",
        node,
        vxmOPERATION_COUNT(reduceNode),
        reduceNode->operations,
        VX_NULL);

    if (reduceDim)
    {
        axis = reduceDim->value->u32;
        if (axis > 3)
        {
            status = VX_ERROR_INVALID_PARAMETERS;
            vxError("Invalid input dimention %d function %s line %d", axis, __FUNCTION__, __LINE__);
            goto exit;
        }
    }

    shExe_flag = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
        || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT16)
        || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT8)
        || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_UINT8)
        || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
        || (inputFormat == VX_TYPE_INT8  && outputFormat == VX_TYPE_INT8)
        || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_FLOAT16)
        || (inputFormat == VX_TYPE_INT8  && outputFormat == VX_TYPE_FLOAT16)
        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16))
        && (reduceDim != NULL)
        && (axis <= 3));

    if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vx_tensor_create_params_t tensor_create_params;
        vx_uint32 dims          = TENSOR_DIM_NUM(src);
        vx_uint32 width         = TENSOR_VIEW_SIZE_INDEX(src, 0);
        vx_uint32 height        = dims > 1 ? TENSOR_VIEW_SIZE_INDEX(src, 1) : 1;
        vx_uint32 depth         = dims > 2 ? TENSOR_VIEW_SIZE_INDEX(src, 2) : 1;
        vx_uint32 batch         = dims > 3 ? TENSOR_VIEW_SIZE_INDEX(src, 3) : 1;
        vx_uint32 sizes[4]      = {width, height, depth, batch};
        vx_uint32 new_sizes[4]  = {width, height, depth, batch};
        vx_uint32 perm_array[4] = {0, 1, 2, 3};
        vx_tensor transTensor   = NULL;
        vx_uint32 i             = 0;
        vx_bool   enable_trans  = vx_false_e;
        vx_bool   enable_axis   = vx_true_e;

        if (axis == 3)
        {
            enable_trans = vx_true_e;

            if (axis == 3)
            {
                perm_array[0] = 3;
                perm_array[1] = 0;
                perm_array[2] = 1;
                perm_array[3] = 2;
            }
        }
        else
        {
            transTensor = src;
        }

        if (enable_trans)
        {
            for (i = 0; i < 4; i ++)
            {
                new_sizes[i] = sizes[perm_array[i]];
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = dims;
            tensor_create_params.sizes = new_sizes;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(src);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(src);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(src);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(src);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(src);
            }

            transTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);

            reduceNode->base.temp_tensors[tmpTensorIndex++] = transTensor;

            if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP_TRANSPOSE) &&
                vxnneIsTPSupportFormat(context, src, VX_NULL, transTensor))
            {
                vx_op_param_s conv = {0};
                vx_uint32 dnum = 4;

                status = vxnneOperation_Initialize(&reduceNode->tensor_reduce_sum_trans_tp_operation.base,
                    &reduceNode->base,
                    VXNNE_OPERATION_TARGET_TP,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    VX_NULL,
                    vxnneOperation_TP_Deinitialize,
                    1,
                    0);
                if (status != VX_SUCCESS) goto exit;

                conv.pad_x_left = 0;
                conv.pad_y_top = 0;
                conv.pool_size_x = 0;
                conv.pool_size_y = 0;
                conv.pool_stride = 1;
                conv.enable_relu = vx_false_e;
                conv.conv_rounding_type = 0;
                conv.pad_mode = VX_PAD_CONSTANT;
                conv.pad_const = 0;
                conv.tpType = TP_TRANSPOSE;
                conv.other_ref = (vx_reference)src;
                conv.data_buff = gcvNULL;
                conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
                conv.tp_value->u32[0] = dnum;
                conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(vx_uint32) * dnum);
                vxMemCopy(conv.tp_value->p8[0], perm_array, sizeof(vx_uint32) * dnum);

                vxMemCopy(&reduceNode->tensor_reduce_sum_trans_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

                vxnneLayer_SetOperation(
                    &reduceNode->base,
                    &reduceNode->tensor_reduce_sum_trans_tp_operation.base,
                    operationIdx++);

                reduceNode->tensor_reduce_sum_trans_tp_operation.input  = src;
                reduceNode->tensor_reduce_sum_trans_tp_operation.output = transTensor;

                vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_trans_tp_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_trans_tp_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
            else
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;

                shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src, perm_array, dims, transTensor);

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&reduceNode->tensor_reduce_sum_trans_sh_operation,
                    &reduceNode->base,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_trans_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_trans_sh_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &reduceNode->base,
                    &reduceNode->tensor_reduce_sum_trans_sh_operation.base,
                    operationIdx++);
            }
        }

        if (enable_axis)
        {
            vxnne_shader_executable shaderExecutable = NULL;
            vx_float32              axis_coef        = 1.0f;
            vx_uint32               batch            = new_sizes[3];

            shaderExecutable = vxnneGetTensorMeanAxisShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MEAN_AXIS0, &node->kernelAttributes.borderMode, axis_coef, transTensor, dst, axis);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&reduceNode->tensor_reduce_sum_sh_operation,
                &reduceNode->base,
                VXNNE_OPERATOR_TENSOR_MEAN,
                batch,
                shaderExecutable);

            if (status != VX_SUCCESS)
            {
                goto exit;
            }

            vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_sh_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &reduceNode->base,
                &reduceNode->tensor_reduce_sum_sh_operation.base,
                operationIdx++);
        }

        reduceNode->base.num_temp_tensors = tmpTensorIndex;
    }
    else
    {
        vxnneOperation_Initialize(&reduceNode->tensor_reduce_sum_operation.base,
            &reduceNode->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_REDUCE_SUM,
            vxnneExecuteSWTensorReduceSum,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &reduceNode->base,
            &reduceNode->tensor_reduce_sum_operation.base,
            0);

        reduceNode->tensor_reduce_sum_operation.src           = src;
        reduceNode->tensor_reduce_sum_operation.dst           = dst;
        reduceNode->tensor_reduce_sum_operation.reduceDim     = reduceDim;
        reduceNode->tensor_reduce_sum_operation.keepDim       = keepDim;

        vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        if (reduceDim)
            vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_operation.base, (vx_reference)reduceDim, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reduceNode->tensor_reduce_sum_operation.base, (vx_reference)keepDim, VXNNE_OPERATION_REFENRENCE_INPUT);
    }

    node->layer = &reduceNode->base;

    return status;

exit:
    if (reduceNode)
    {
        if (reduceNode->tensor_reduce_sum_trans_tp_operation.base.parameter.tp_value)
        {
            if (reduceNode->tensor_reduce_sum_trans_tp_operation.base.parameter.tp_value->p8[0])
            {
                gcoOS_Free(NULL, (gctPOINTER)reduceNode->tensor_reduce_sum_trans_tp_operation.base.parameter.tp_value->p8[0]);
            }
            gcoOS_Free(NULL, (gctPOINTER)reduceNode->tensor_reduce_sum_trans_tp_operation.base.parameter.tp_value);
        }
        gcoOS_Free(NULL, (gctPOINTER)reduceNode);
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorReduceSum_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoInternelKernel_NNTensorPad(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorPad_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorPad_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorPad_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  src                     = (vx_tensor)parameters[0];
    vx_tensor  dst                     = (vx_tensor)parameters[1];
    vx_scalar  padLeft                 = (vx_scalar)parameters[2];
    vx_scalar  padRight                = (vx_scalar)parameters[3];
    vx_scalar  padTop                  = (vx_scalar)parameters[4];
    vx_scalar  padBottom               = (vx_scalar)parameters[5];
    vx_scalar  padMode                 = (vx_scalar)parameters[6];
    vx_scalar  padConst                = (vx_scalar)parameters[7];
    vx_uint32  batchCount              = TENSOR_SIZE_INDEX(src, 3);

    vx_enum    inputFormat             = TENSOR_DATA_TYPE(src);
    vx_int8    inputFixPointPos        = TENSOR_POS(src);
    vx_int32   inputZeroPoint          = TENSOR_TF_ZEROPOINT(src);
    vx_float32 inputScale              = TENSOR_TF_SCALE(src);
    vx_uint32  inputElementSize        = TENSOR_DATA_SIZE(src);
    vx_enum    outputFormat            = TENSOR_DATA_TYPE(dst);
    vx_int8    outputFixPointPos       = TENSOR_POS(dst);
    vx_int32   outputZeroPoint         = TENSOR_TF_ZEROPOINT(dst);
    vx_float32 outputScale             = TENSOR_TF_SCALE(dst);
    vx_enum    padModeVal              = padMode->value->e;
    vx_bool    dataFormatFlag          = vx_false_e;
    vx_bool    tensorPadSHFlag         = vx_false_e;
    vx_context context                 = vxGetContext((vx_reference)node);

    vxnne_tensor_pad  padNode = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_pad_s), (gctPOINTER*)&padNode);
    if (!padNode)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(padNode, sizeof(vxnne_tensor_pad_s));

    vxnneLayer_Initialize(&padNode->base,
                          "TensorPadOperation",
                          node,
                          vxmOPERATION_COUNT(padNode),
                          padNode->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        dataFormatFlag = (vx_bool)((inputFormat == outputFormat) && (inputElementSize & 3) && (inputFixPointPos == outputFixPointPos)
            && (inputZeroPoint == outputZeroPoint) && (inputScale == outputScale));
    }
    else
    {
        dataFormatFlag = (vx_bool)(((inputFormat == outputFormat) && (inputFormat == VX_TYPE_FLOAT32 || inputFormat == VX_TYPE_FLOAT16)) ||
                                    ((inputFormat == VX_TYPE_UINT8) && (inputZeroPoint == outputZeroPoint) && (inputScale == outputScale)));
    }

    tensorPadSHFlag = (vx_bool)(dataFormatFlag && (padModeVal == VX_PAD_CONSTANT || padModeVal == VX_PAD_REPLICATE));

    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(context, src, VX_NULL, dst))
    {
        vx_op_param_s conv = {0};

        status = vxnneOperation_Initialize(&padNode->tensor_pad_tp_operation.base,
            &padNode->base,
            VXNNE_OPERATION_TARGET_TP,
            VXNNE_OPERATOR_TENSOR_PAD,
            VX_NULL,
            VX_NULL,
            batchCount,
            0);

        if (status != VX_SUCCESS) goto exit;

        memset(&conv, 0, sizeof(vx_op_param_s));

        conv.pad_x_left = padLeft->value->n32;
        conv.pad_y_top = padTop->value->n32;
        conv.pad_x_right = padRight->value->n32;
        conv.pad_y_bottom = padBottom->value->n32;
        conv.pad_mode = padModeVal;
        conv.pad_const = (vx_int32)padConst->value->n32;
        conv.pool_size_x = conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.pool_stride = 1;
        conv.tpType = TP_TENSOR_PAD;

        memcpy(&padNode->tensor_pad_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneOperation_AddReference(&padNode->tensor_pad_tp_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_tp_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        padNode->tensor_pad_tp_operation.input = src;
        padNode->tensor_pad_tp_operation.output = dst;

        vxnneLayer_SetOperation(
            &padNode->base,
            &padNode->tensor_pad_tp_operation.base,
            0);
    }
    else if(tensorPadSHFlag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetTensorPadShaderExecutable(node->base.context,
                                                             VXNNE_KERNEL_TENSOR_PAD,
                                                             &node->kernelAttributes.borderMode,
                                                             src,
                                                             padLeft,
                                                             padRight,
                                                             padTop,
                                                             padBottom,
                                                             padMode,
                                                             padConst,
                                                             dst);
        }
        else
        {
            shaderExecutable = vxnneGetGPUTensorPadShaderExecutable(node->base.context,
                                                                 VXNNE_KERNEL_TENSOR_PAD,
                                                                 &node->kernelAttributes.borderMode,
                                                                 src,
                                                                 padLeft,
                                                                 padRight,
                                                                 padTop,
                                                                 padBottom,
                                                                 padMode,
                                                                 padConst,
                                                                 dst);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&padNode->tensor_pad_sh_operation,
                                                 &padNode->base,
                                                 VXNNE_OPERATOR_TENSOR_PAD,
                                                 batchCount,
                                                 shaderExecutable);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
                &padNode->base,
                &padNode->tensor_pad_sh_operation.base,
                0);

        vxnneOperation_AddReference(&padNode->tensor_pad_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        vxnneOperation_Initialize(&padNode->tensor_pad_operation.base,
                                  &padNode->base,
                                  VXNNE_OPERATION_TARGET_SW,
                                  VXNNE_OPERATOR_TENSOR_PAD,
                                  vxnneExecuteSWTensorPad,
                                  VX_NULL,
                                  batchCount,
                                  0);

        vxnneLayer_SetOperation(
            &padNode->base,
            &padNode->tensor_pad_operation.base,
            0);

        padNode->tensor_pad_operation.src           = src;
        padNode->tensor_pad_operation.dst           = dst;
        padNode->tensor_pad_operation.padLeft       = padLeft;
        padNode->tensor_pad_operation.padRight      = padRight;
        padNode->tensor_pad_operation.padTop        = padTop;
        padNode->tensor_pad_operation.padBottom     = padBottom;
        padNode->tensor_pad_operation.padMode       = padMode;
        padNode->tensor_pad_operation.padConst      = padConst;

        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padLeft, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padRight, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padTop, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padBottom, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padMode, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padConst, VXNNE_OPERATION_REFENRENCE_INPUT);
    }

    node->layer = &padNode->base;
    return status;

exit:
    if (padNode != NULL) gcoOS_Free(NULL, padNode);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorPad2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status = VX_SUCCESS;

    vx_tensor  src = (vx_tensor)parameters[0];
    vx_tensor  dst = (vx_tensor)parameters[1];
    vx_tensor  pad_dims = (vx_tensor)parameters[2];
    vx_scalar  padMode = (vx_scalar)parameters[3];
    vx_scalar  padConst = (vx_scalar)parameters[4];
    vx_uint32  batchCount = TENSOR_SIZE_INDEX(src, 3);

    vxnne_tensor_pad  padNode = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_pad_s), (gctPOINTER*)&padNode);
    if (!padNode)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(padNode, sizeof(vxnne_tensor_pad_s));

    vxnneLayer_Initialize(&padNode->base,
        "TensorPadOperation2",
        node,
        vxmOPERATION_COUNT(padNode),
        padNode->operations,
        VX_NULL);
    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(node->base.context, src, VX_NULL, dst))
    {
        vx_int32 index = 0;
        vx_op_param_s conv = { 0 };
        vx_int32_ptr pad_base = VX_NULL;
        vxoTensor_GetTensorViewMemory(pad_dims, (gctPOINTER *)&pad_base, VX_NULL);

        status = vxnneOperation_Initialize(&padNode->tensor_pad_tp_operation.base,
            &padNode->base,
            VXNNE_OPERATION_TARGET_TP,
            VXNNE_OPERATOR_TENSOR_PAD,
            VX_NULL,
            VX_NULL,
            batchCount,
            0);

        if (status != VX_SUCCESS) goto exit;
        /*pad width and height*/
        memset(&conv, 0, sizeof(vx_op_param_s));

        conv.pad_x_left = pad_base[0];
        conv.pad_y_top = pad_base[2];
        conv.pad_x_right = pad_base[1];
        conv.pad_y_bottom = pad_base[3];
        conv.pad_mode = padMode->value->e;
        conv.pad_const = padConst != VX_NULL ? (vx_int32)padConst->value->n32 : 0;
        conv.pool_size_x = conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.pool_stride = 1;
        conv.tpType = TP_TENSOR_PAD;

        memcpy(&padNode->tensor_pad_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneOperation_AddReference(&padNode->tensor_pad_tp_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&padNode->tensor_pad_tp_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        padNode->tensor_pad_tp_operation.input = src;
        padNode->tensor_pad_tp_operation.output = dst;

        vxnneLayer_SetOperation(
            &padNode->base,
            &padNode->tensor_pad_tp_operation.base,
            index++);

        /* pad depth and batch*/

    }
    else
    {
        vx_int32 inWidth = 0, inHeight = 0, inDepth = 0, inBatch = 0;
        vx_int32 outWidth = 0, outHeight = 0, outDepth = 0, outBatch = 0;
        vx_bool shader_flag = vx_false_e;
        vx_bool pad_flag = vx_false_e;
        vx_bool whc_flag = vx_false_e;
        vx_int32_ptr pad_base = VX_NULL;
        vx_enum pad_mode = padMode->value->e;

        outWidth  = TENSOR_VIEW_SIZE_INDEX(dst, 0);
        outHeight = TENSOR_VIEW_SIZE_INDEX(dst, 1);
        outDepth = TENSOR_VIEW_SIZE_INDEX(dst, 2);
        outBatch = TENSOR_VIEW_SIZE_INDEX(dst, 3);
        inWidth  = TENSOR_VIEW_SIZE_INDEX(src, 0);
        inHeight = TENSOR_VIEW_SIZE_INDEX(src, 1);
        inDepth = TENSOR_VIEW_SIZE_INDEX(src, 2);
        inBatch = TENSOR_VIEW_SIZE_INDEX(src, 3);
        vxoTensor_GetTensorViewMemory(pad_dims, (gctPOINTER*)&pad_base, VX_NULL);

        if(outDepth == inDepth && outBatch == inBatch)
        {
            pad_flag = vx_true_e;
            shader_flag = vx_true_e;
        }
        else if(outWidth == inWidth && outHeight == inHeight && pad_mode == VX_PAD_CONSTANT
            && ((outDepth != inDepth && outBatch == inBatch) || (outDepth == inDepth && outBatch != inBatch)))
        {
            shader_flag = vx_true_e;

            if(outBatch > 1)
            {
                if(outDepth != inDepth)
                {
                    if(outWidth * outHeight < 65536
                        || outDepth * outBatch < 65536)
                        shader_flag = vx_true_e;
                    else
                        shader_flag = vx_false_e;
                }
                else
                {
                    if(outWidth * outHeight < 65536
                        || outHeight * outDepth < 65536
                        || outDepth * outBatch < 65536)
                        shader_flag = vx_true_e;
                    else
                        shader_flag = vx_false_e;
                }
            }
        }
        else if(pad_mode == VX_PAD_CONSTANT && outBatch < 2)
        {
            shader_flag = vx_true_e;
            whc_flag = vx_true_e;
        }

        if (shader_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable;

            if(pad_flag)
            {
                vx_scalar padLeft = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &pad_base[0]);
                vx_scalar padRight = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &pad_base[1]);
                vx_scalar padTop = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &pad_base[2]);
                vx_scalar padBottom = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &pad_base[3]);

                shaderExecutable = vxnneGetTensorPadShaderExecutable(node->base.context,
                    VXNNE_KERNEL_TENSOR_PAD,
                    &node->kernelAttributes.borderMode,
                    src,
                    padLeft,
                    padRight,
                    padTop,
                    padBottom,
                    padMode,
                    padConst,
                    dst);

                vxReleaseScalar(&padLeft);
                vxReleaseScalar(&padRight);
                vxReleaseScalar(&padTop);
                vxReleaseScalar(&padBottom);
            }
            else if(outWidth * outHeight < 65536 && outDepth != inDepth && !whc_flag)
            {
                vx_uint32  reshpTensor_Sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {1};
                vx_uint32  reshpTensor_Dims           = 3;
                vx_uint32 leftPad  = 0, topPad = 0, rightPad = 0, bottomPad = 0;
                vx_tensor input      = NULL;
                vx_tensor output     = NULL;
                vx_scalar padLeft, padRight, padTop, padBottom;

                reshpTensor_Sizes[0] = outWidth * outHeight;
                reshpTensor_Sizes[1] = outDepth;
                reshpTensor_Sizes[2] = outBatch == 0 ? 1: outBatch;
                output     = vxoTensor_ReshapeTensor(dst, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

                reshpTensor_Sizes[0] = inWidth * inHeight;
                reshpTensor_Sizes[1] = inDepth;
                reshpTensor_Sizes[2] = inBatch == 0 ? 1: inBatch;
                input     = vxoTensor_ReshapeTensor(src, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

                topPad = pad_base[4];
                bottomPad = pad_base[5];

                padLeft = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &leftPad);
                padRight = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &rightPad);
                padTop = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &topPad);
                padBottom = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &bottomPad);

                shaderExecutable = vxnneGetTensorPadShaderExecutable(node->base.context,
                    VXNNE_KERNEL_TENSOR_PAD,
                    &node->kernelAttributes.borderMode,
                    input,
                    padLeft,
                    padRight,
                    padTop,
                    padBottom,
                    padMode,
                    padConst,
                    output);

                vxReleaseScalar(&padLeft);
                vxReleaseScalar(&padRight);
                vxReleaseScalar(&padTop);
                vxReleaseScalar(&padBottom);

                if (input) vxoTensor_ReleaseTensor(&input);
                if (output) vxoTensor_ReleaseTensor(&output);
            }
            else if(outWidth * outHeight * inDepth < 65536 && inBatch != outBatch && !whc_flag)
            {
                vx_uint32  reshpTensor_Sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {1};
                vx_uint32  reshpTensor_Dims           = 3;
                vx_uint32 leftPad  = 0, topPad = 0, rightPad = 0, bottomPad = 0;
                vx_tensor input      = NULL;
                vx_tensor output     = NULL;
                vx_scalar padLeft, padRight, padTop, padBottom;

                reshpTensor_Sizes[0] = outWidth * outHeight * outDepth;
                reshpTensor_Sizes[1] = outBatch == 0 ? 1: outBatch;
                reshpTensor_Sizes[2] = 1;
                output     = vxoTensor_ReshapeTensor(dst, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

                reshpTensor_Sizes[0] = inWidth * inHeight * inDepth;
                reshpTensor_Sizes[1] = inBatch == 0 ? 1: inBatch;
                reshpTensor_Sizes[2] = 1;
                input     = vxoTensor_ReshapeTensor(src, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

                topPad = pad_base[6];
                bottomPad = pad_base[7];

                padLeft = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &leftPad);
                padRight = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &rightPad);
                padTop = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &topPad);
                padBottom = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &bottomPad);

                shaderExecutable = vxnneGetTensorPadShaderExecutable(node->base.context,
                    VXNNE_KERNEL_TENSOR_PAD,
                    &node->kernelAttributes.borderMode,
                    input,
                    padLeft,
                    padRight,
                    padTop,
                    padBottom,
                    padMode,
                    padConst,
                    output);

                vxReleaseScalar(&padLeft);
                vxReleaseScalar(&padRight);
                vxReleaseScalar(&padTop);
                vxReleaseScalar(&padBottom);

                if (input) vxoTensor_ReleaseTensor(&input);
                if (output) vxoTensor_ReleaseTensor(&output);
            }
            else
            {
                shaderExecutable = vxnneGetTensorPad2ShaderExecutable(node->base.context,
                    VXNNE_KERNEL_TENSOR_PAD,
                    &node->kernelAttributes.borderMode,
                    src,
                    padConst,
                    dst,
                    pad_base);
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&padNode->tensor_pad_sh_operation,
                &padNode->base,
                VXNNE_OPERATOR_TENSOR_PAD,
                batchCount,
                shaderExecutable);
            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&padNode->tensor_pad_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_sh_operation.base, (vx_reference)padConst, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &padNode->base,
                &padNode->tensor_pad_sh_operation.base,
                0);
        }
        else
        {
            vxnneOperation_Initialize(&padNode->tensor_pad_operation.base,
                &padNode->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_TENSOR_PAD,
                vxnneExecuteSWTensorPad2,
                VX_NULL,
                batchCount,
                0);

            vxnneLayer_SetOperation(
                &padNode->base,
                &padNode->tensor_pad_operation.base,
                0);

            padNode->tensor_pad_operation.src       = src;
            padNode->tensor_pad_operation.dst       = dst;
            padNode->tensor_pad_operation.pad_dims  = pad_dims;
            padNode->tensor_pad_operation.padMode   = padMode;
            padNode->tensor_pad_operation.padConst  = padConst;

            vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)pad_dims, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padMode, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&padNode->tensor_pad_operation.base, (vx_reference)padConst, VXNNE_OPERATION_REFENRENCE_INPUT);
        }
    }

    node->layer = &padNode->base;
    return status;

exit:
    if (padNode != NULL) gcoOS_Free(NULL, padNode);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorPad_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}


/***************************************************************************************************************************
 * NormalizationLayer and NormalizationLayer2
 *
 * See the difference between NormalizationLayer and NormalizationLayer2 as below.
 * 1. NormalizationLayer supports both SAME_MAP and ACROSS_MAP but NormalizationLayer2 supports CROSS_MAP only.
 * 2. For ACROSS_MAP, the common formula is as follows.
 *      b[xyz] = a[xyz] / power(bias + alpha * sum(square(a[xyi]) / div), beta)
 *      i is from max(0, z - kernel / 2) to min(inImageZSize - 1, z + norm_size / 2).
 *    NormalizationLayer does the formula with bias = 1.0f and div = norm_size.
 *    NormalizationLayer2 does the formula with div = 1.
 * 3. For SAME_MAP, NormalizationLayer does as follows.
 *
 ***************************************************************************************************************************/
vx_status vxnneExecuteSWNormalization(struct _vxnne_operation_s *operation)
{
    vxnne_normalization_operation lrn_operation = (vxnne_normalization_operation)operation;

    vx_tensor input  = lrn_operation->inputs;
    vx_tensor output = lrn_operation->outputs;

    gctPOINTER input_base;
    gctPOINTER output_base;

    vx_uint32 width   = TENSOR_SIZE_INDEX(input, 0);
    vx_uint32 height  = TENSOR_SIZE_INDEX(input, 1);
    vx_uint32 channel = TENSOR_SIZE_INDEX(input, 2);
    vx_uint32 batch   = TENSOR_DIM_NUM(input) > 3 ? TENSOR_SIZE_INDEX(input, 3) : 1;

    vx_enum    norm_type = lrn_operation->norm_type;
    vx_uint32  norm_size = lrn_operation->norm_size;
    vx_uint32  div       = lrn_operation->div;
    vx_float32 alpha     = lrn_operation->alpha;
    vx_float32 beta      = lrn_operation->beta;
    vx_float32 bias      = lrn_operation->bias;
    vx_uint32  nsz2      = norm_size / 2;
    vx_type_e  input_format  = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e  output_format = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_int8    input_fp_pos   = TENSOR_POS(input);
    vx_int8    output_fp_pos  = TENSOR_POS(output);
    vx_enum    output_rounding_mode = TENSOR_ROUNDING_MODE(output);
    vx_uint32  input_stride, output_stride;
    vx_float32 sum = 0, val = 0;
    vx_uint32  w, h, c, n, b, i, j;
    vx_uint32  start_w, end_w, start_h, end_h, start_c, end_c;
    vx_uint32  input_data_size = vxDataType_GetSize(input_format);
    vx_uint32  output_data_size = vxDataType_GetSize(output_format);
    if ((input_data_size == 0) || (output_data_size == 0))
    {
        return VX_FAILURE;
    }
    else
    {
        input_stride = TENSOR_STRIDE_INDEX(input, 2) / input_data_size;
        output_stride = TENSOR_STRIDE_INDEX(output, 2) / output_data_size;
    }

    vxoTensor_GetTensorViewMemory(input, &input_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, &output_base, VX_NULL);

    if (norm_type == VX_NN_NORMALIZATION_SAME_MAP)
    {
        for (b = 0; b < batch; b++)
        {
            for (c = 0; c < channel; c++)
            {
                for (h = 0; h < height; h++)
                {
                    start_h = gcmMAX((vx_int32)(h - nsz2), 0);
                    end_h   = gcmMIN(h + nsz2, height - 1);
                    for (w = 0; w < width; w++)
                    {
                        sum = 0;

                        for (j = start_h; j <= end_h; j++)
                        {
                            start_w = gcmMAX((vx_int32)(w - nsz2), 0);
                            end_w   = gcmMIN(w + nsz2, width - 1);

                            for (i = start_w; i <= end_w; i++)
                            {
                                if (input_format == VX_TYPE_UINT8)
                                {
                                    val = vxnneGetDataQuant((vx_type_e)TENSOR_DATA_TYPE(input),
                                                            (c + b * channel) * input_stride + width * j + i,
                                                            (vx_uint8_ptr)input_base,
                                                            TENSOR_TF_ZEROPOINT(input),
                                                            TENSOR_TF_SCALE(input));
                                }
                                else
                                {
                                    val = vxnneGetData(input_format,
                                                       (c + b * channel) * input_stride + width * j + i,
                                                       (vx_uint8_ptr)input_base, input_fp_pos);
                                }

                                sum += val * val;
                            }
                        }

                        if (input_format == VX_TYPE_UINT8)
                        {
                            val = vxnneGetDataQuant((vx_type_e)TENSOR_DATA_TYPE(input),
                                                    (c + b * channel) * input_stride + width * h + w,
                                                    (vx_uint8_ptr)input_base,
                                                    TENSOR_TF_ZEROPOINT(input),
                                                    TENSOR_TF_SCALE(input));
                        }
                        else
                        {
                            val = vxnneGetData(input_format,
                                               (c + b * channel) * input_stride + width * h + w,
                                               (vx_uint8_ptr)input_base,
                                               input_fp_pos);
                        }

                        val = val / powf((bias + (alpha / div) * sum), beta);

                        if (output_format == VX_TYPE_UINT8)
                        {
                            vxnneSaveDataQuant((vx_type_e)TENSOR_DATA_TYPE(output),
                                               (c + b * channel) * output_stride + width * h + w,
                                               (vx_float64)(val),
                                               output_base,
                                               TENSOR_TF_ZEROPOINT(output),
                                               TENSOR_TF_SCALE(output),
                                               output_rounding_mode);
                        }
                        else
                        {
                            vxnneSaveData(output_format, (c + b * channel) * output_stride + width * h + w,
                                          val,
                                          (vx_uint8_ptr)output_base,
                                          output_fp_pos,
                                          output_rounding_mode);
                        }
                    }
                }
            }
        }
    }
    else
    {
        for (b = 0; b < batch; b++)
        {
            for (c = 0; c < channel; c++)
            {
                start_c = gcmMAX((vx_int32)(c - nsz2), 0);
                end_c   = gcmMIN(c + nsz2, channel - 1);

                for (h = 0; h < height; h++)
                {
                    for (w = 0; w < width; w++)
                    {
                        sum = 0;

                        for(n = start_c; n <= end_c; n++)
                        {
                            if (input_format == VX_TYPE_UINT8)
                            {
                                val = vxnneGetDataQuant((vx_type_e)TENSOR_DATA_TYPE(input),
                                                        (n + b * channel) * input_stride + width * h + w,
                                                        (vx_uint8_ptr)input_base,
                                                        TENSOR_TF_ZEROPOINT(input),
                                                        TENSOR_TF_SCALE(input));
                            }
                            else
                            {
                                val = vxnneGetData(input_format,
                                                   (n + b * channel) * input_stride + width * h + w,
                                                   (vx_uint8_ptr)input_base,
                                                   input_fp_pos);
                            }

                            sum += val * val;
                        }
                        if (input_format == VX_TYPE_UINT8)
                        {
                            val = vxnneGetDataQuant((vx_type_e)TENSOR_DATA_TYPE(input),
                                                    (c + b * channel) * input_stride + width * h + w,
                                                    (vx_uint8_ptr)input_base,
                                                    TENSOR_TF_ZEROPOINT(input),
                                                    TENSOR_TF_SCALE(input));
                        }
                        else
                        {
                            val = vxnneGetData(input_format,
                                               (c + b * channel) * input_stride + width * h + w,
                                               (vx_uint8_ptr)input_base,
                                               input_fp_pos);
                        }

                        val = val / powf((bias + (alpha / div) * sum), beta);

                        if (output_format == VX_TYPE_UINT8)
                        {
                            vxnneSaveDataQuant((vx_type_e)TENSOR_DATA_TYPE(output),
                                               (c + b * channel) * output_stride + width * h + w,
                                               (vx_float64)(val),
                                               output_base,
                                               TENSOR_TF_ZEROPOINT(output),
                                               TENSOR_TF_SCALE(output),
                                               output_rounding_mode);
                        }
                        else
                        {
                            vxnneSaveData(output_format,
                                          (c + b * channel) * output_stride + width * h + w,
                                          val,
                                          (vx_uint8_ptr)output_base,
                                          output_fp_pos,
                                          output_rounding_mode);
                        }
                    }
                }
            }
        }
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_tensor _AllocateTPLUTorListBuffer(vx_context context, vx_node node, vx_uint32 size, vx_enum type)
{
    vx_tensor buffer = VX_NULL;
    vx_tensor_create_params_t tensor_create_params;

    size = size != 0 ? size : TP_LUT_BUFF_SIZE;
    type = type != VX_TYPE_INVALID ? type :
           gcoHAL_IsFeatureAvailable1(gcvNULL, gcvFEATURE_TP_REAL_INT16) &&
           !gcoHAL_IsFeatureAvailable1(gcvNULL, gcvFEATURE_TP_SIMPLE_INT16) ? VX_TYPE_UINT32 : VX_TYPE_UINT16;

    vxZeroMemory(&tensor_create_params, gcmSIZEOF(vx_tensor_create_params_t));
    tensor_create_params.num_of_dims = 1;
    tensor_create_params.sizes = &size;
    tensor_create_params.data_format = type;
    tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;
    tensor_create_params.quant_data.dfp.fixed_point_pos = 0;

    buffer = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

    if (buffer == VX_NULL || vxoTensor_AllocateMemory(buffer) != VX_SUCCESS)
    {
        vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
        return VX_NULL;
    }

    return buffer;
}

VX_PRIVATE_API vx_tensor vxnneAllocateTPLUTBuffer(vx_context context, vx_node node)
{
    return _AllocateTPLUTorListBuffer(context, node, 0, VX_TYPE_INVALID);
}

VX_PRIVATE_API vx_tensor vxnneAllocateTPROIListBuffer(vx_context context, vx_node node, vx_uint32 size, vx_enum type)
{
    return _AllocateTPLUTorListBuffer(context, node, size, type);
}

VX_PRIVATE_API void vxnneInitROITensorFromBuffer(vx_tensor tensor, void* buffer, vx_uint32 element_size)
{
    vx_uint8_ptr data_ptr;

    vxoTensor_GetTensorViewMemory(tensor, (gctPOINTER *)&data_ptr, VX_NULL);

    vxMemCopy(data_ptr, buffer, element_size);
}

VX_PRIVATE_API vx_status vxoLRNOperationTP_Initialize(
    vxnne_tp_operation operation,
    vxnne_layer layer,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_enum norm_type,
    vx_uint32 norm_size,
    vx_uint32 div,
    vx_float32 alpha,
    vx_float32 beta,
    vx_float32 bias,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    vx_node node = layer->node;
    vx_context context = vxGetContext((vx_reference)node);
    vx_op_param op_param = VX_NULL;

    if (!op_index)
    {
        vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
    }

    vxmONERROR(vxnneOperation_Initialize(&operation->base,
                                         layer,
                                         VXNNE_OPERATION_TARGET_TP,
                                         VXNNE_OPERATOR_NORMALIZATION,
                                         VX_NULL,
                                         vxnneOperation_TP_Deinitialize,
                                         batch_count,
                                         0));

    op_param = &operation->base.parameter;

    op_param->data_buff = vxnneAllocateTPLUTBuffer(context, node);
    if (op_param->data_buff == VX_NULL) vxmONERROR(VX_ERROR_NO_MEMORY);

    op_param->pad_x_left   = 0;
    op_param->pad_y_top    = 0;
    op_param->pad_x_right  = 0;
    op_param->pad_y_bottom = 0;
    op_param->pool_size_x  = 0;
    op_param->pool_size_y  = 0;
    op_param->pool_stride  = 1;
    op_param->enable_relu  = vx_false_e;
    op_param->pad_mode     = VX_PAD_CONSTANT;
    /* For TP PAD_CONST, the HW just uses the data without minus ZERO_POINT. */
    op_param->pad_const    = 0;
    op_param->tpType       = TP_LRN;
    op_param->other_ref    = VX_NULL;
    op_param->conv_rounding_type = 0;

    op_param->tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
    if (!op_param->tp_value)
    {
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }
    op_param->tp_value->e32[0] = norm_type;
    op_param->tp_value->u32[0] = norm_size;
    op_param->tp_value->u32[1] = div;
    op_param->tp_value->f32[0] = alpha;
    op_param->tp_value->f32[1] = beta;
    op_param->tp_value->f32[2] = bias;

    vxmONERROR(vxnneLayer_SetOperation(layer,
                                       &operation->base,
                                       (*op_index)++));

    operation->input  = input;
    operation->output = output;

    vxnneOperation_AddReference(&operation->base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&operation->base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

    return status;

OnError:
    if (op_param && op_param->data_buff)
    {
        vxoTensor_ReleaseTensor(&op_param->data_buff);
        op_param->data_buff = VX_NULL;
    }

    if (op_param && op_param->tp_value)
    {
        vxFree(op_param->tp_value);
        op_param->tp_value = VX_NULL;
    }

    return status;
}

VX_PRIVATE_API vx_status vxoLRNOperationSH_Initialize(
    vxnne_shader_operation operation,
    vxnne_layer layer,
    vx_tensor inputs,
    vx_tensor outputs,
    vx_uint32 batchCount,
    vx_enum norm_type,
    vx_uint32 norm_size,
    vx_uint32 div,
    vx_float32 alpha,
    vx_float32 beta,
    vx_float32 bias,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    vx_node                 node         = layer->node;
    vx_context              context      = vxGetContext((vx_reference)node);
    vxnne_shader_executable shaderExecutable = VX_NULL;
    vx_scalar               type_s       = VX_NULL;
    vx_scalar               norm_size_s  = VX_NULL;
    vx_scalar               alpha_s      = VX_NULL;
    vx_scalar               beta_s       = VX_NULL;
    vx_scalar               bias_s       = VX_NULL;
    vx_enum    inputFormat               = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat              = TENSOR_DATA_TYPE(outputs);
    vx_bool    sammap_flag               = vx_false_e;
    vx_bool    acrossmap_flag            = vx_false_e;
    vx_bool    dataformat_flag[6]        = {vx_false_e};
    vx_bool    norm_config[3]            = {vx_false_e};
    vx_bool    generic_flag              = vx_false_e;
    vx_bool    isuint8_flag              = vx_false_e;
    vx_bool    norm_shader_flag          = vx_false_e;

    if (!op_index)
    {
        vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
    }

    sammap_flag        = (vx_bool)(norm_type == VX_NN_NORMALIZATION_SAME_MAP);
    acrossmap_flag     = (vx_bool)(norm_type == VX_NN_NORMALIZATION_ACROSS_MAPS);
    norm_config[0]     = (vx_bool)(norm_size == 3 && beta == 0.75);
    norm_config[1]     = (vx_bool)(norm_size == 5 && beta == 0.75);
    norm_config[2]     = (vx_bool)(norm_size == 11 && beta == 0.5);
    dataformat_flag[0] = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 || inputFormat == VX_TYPE_INT8) && (outputFormat == VX_TYPE_FLOAT16 || outputFormat == VX_TYPE_INT8));
    dataformat_flag[1] = (vx_bool)(inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16);
    dataformat_flag[2] = (vx_bool)(inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8);
    dataformat_flag[3] = (vx_bool)(inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16);
    dataformat_flag[4] = (vx_bool)(inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32);
    dataformat_flag[5] = (vx_bool)(inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16);
    isuint8_flag       = (vx_bool)(acrossmap_flag && (dataformat_flag[2] || dataformat_flag[5]));
    generic_flag       = (vx_bool)((acrossmap_flag && dataformat_flag[0]) || (sammap_flag && dataformat_flag[3])
                                    ||(acrossmap_flag && dataformat_flag[5]));
    norm_shader_flag   = (vx_bool)((sammap_flag && norm_config[0] && dataformat_flag[0])
                                || (acrossmap_flag && norm_config[0] && dataformat_flag[0])
                                || (acrossmap_flag && norm_config[1] && dataformat_flag[0])
                                || (acrossmap_flag && norm_config[2] && (dataformat_flag[0] || dataformat_flag[1]))
                                || generic_flag || isuint8_flag || dataformat_flag[4]);

    if (div == 1)
    {
        if (acrossmap_flag)
        {
            if(context->evisNoInst.supportEVIS)
            {
                alpha = alpha * (vx_float32)norm_size;
            }
            else
            {
                norm_size = norm_size/2;
            }
        }
        else
        {
            alpha = alpha * (vx_float32)(norm_size * norm_size);
        }
    }

    type_s      = vxCreateScalar(context, VX_TYPE_ENUM, &norm_type);
    norm_size_s = vxCreateScalar(context, VX_TYPE_UINT32, &norm_size);
    alpha_s     = vxCreateScalar(context, VX_TYPE_FLOAT32, &alpha);
    beta_s      = vxCreateScalar(context, VX_TYPE_FLOAT32, &beta);

    if(context->evisNoInst.supportEVIS)
    {
        if (isuint8_flag)
            shaderExecutable = vxnneGetNormalizationUint8ShaderExecutable(node->base.context, VXNNE_KERNEL_NORMALIZATION, &node->kernelAttributes.borderMode, inputs, type_s, norm_size_s, alpha_s, beta_s, bias, outputs);
        else
            shaderExecutable = vxnneGetNormalizationShaderExecutable(node->base.context, VXNNE_KERNEL_NORMALIZATION, &node->kernelAttributes.borderMode, inputs, type_s, norm_size_s, alpha_s, beta_s, bias, outputs);
    }
    else
    {
        vx_scalar bias_s = vxCreateScalar(context, VX_TYPE_FLOAT32, &bias);

        shaderExecutable = vxnneGetGPUNormalizationShaderExecutable(context, VXNNE_KERNEL_NORMALIZATION, &node->kernelAttributes.borderMode, inputs, type_s, norm_size_s, alpha_s, beta_s, bias_s, outputs);

        if(bias_s) vxReleaseScalar(&bias_s);
    }

    vxReleaseScalar(&type_s);
    vxReleaseScalar(&norm_size_s);
    vxReleaseScalar(&alpha_s);
    vxReleaseScalar(&beta_s);
    vxReleaseScalar(&bias_s);

    if (!shaderExecutable)
    {
        vxmONERROR(VX_FAILURE);
    }

    vxmONERROR(vxnneShaderOperation_Initialize(operation,
                                               layer,
                                               VXNNE_OPERATOR_NORMALIZATION,
                                               batchCount,
                                               shaderExecutable));

    vxmONERROR(vxnneLayer_SetOperation(layer,
                                       &operation->base,
                                       (*op_index)++));

    vxnneOperation_AddReference(&operation->base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&operation->base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

OnError:
    return status;
}

VX_PRIVATE_API vx_status vxoLRNOperationSW_Initialize(
    vxnne_normalization_operation operation,
    vxnne_layer layer,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_enum norm_type,
    vx_uint32 norm_size,
    vx_uint32 div,
    vx_float32 alpha,
    vx_float32 beta,
    vx_float32 bias,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    if (!op_index)
    {
        vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
    }

    vxmONERROR(vxnneOperation_Initialize(&operation->base,
                                         layer,
                                         VXNNE_OPERATION_TARGET_SW,
                                         VXNNE_OPERATOR_NORMALIZATION,
                                         vxnneExecuteSWNormalization,
                                         VX_NULL,
                                         batch_count,
                                         0));

    vxmONERROR(vxnneLayer_SetOperation(layer,
                                       &operation->base,
                                       (*op_index)++));

    operation->inputs    = input;
    operation->norm_type = norm_type;
    operation->norm_size = norm_size;
    operation->div       = div;
    operation->alpha     = alpha;
    operation->beta      = beta;
    operation->bias      = bias;
    operation->outputs   = output;

    vxnneOperation_AddReference(&operation->base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&operation->base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

OnError:
    return status;
}

VX_PRIVATE_API vx_status _InitializeLRNOperation(
    vxnne_normalization_layer layer,
    vxnne_operation_target_e target,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_enum norm_type,
    vx_uint32 norm_size,
    vx_uint32 div,
    vx_float32 alpha,
    vx_float32 beta,
    vx_float32 bias,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    if (!op_index)
    {
        return VX_ERROR_INVALID_PARAMETERS;
    }

    switch (target)
    {
    case VXNNE_OPERATION_TARGET_TP:
        vxmONERROR(vxoLRNOperationTP_Initialize(&layer->lrn_tp_operation,
                                                &layer->base,
                                                input,
                                                output,
                                                batch_count,
                                                norm_type,
                                                norm_size,
                                                div,
                                                alpha,
                                                beta,
                                                bias,
                                                op_index));
        break;

    case VXNNE_OPERATION_TARGET_SH:
        vxmONERROR(vxoLRNOperationSH_Initialize(&layer->lrn_sh_operation,
                                                &layer->base,
                                                input,
                                                output,
                                                batch_count,
                                                norm_type,
                                                norm_size,
                                                div,
                                                alpha,
                                                beta,
                                                bias,
                                                op_index));
        break;

    case VXNNE_OPERATION_TARGET_SW:
        vxmONERROR(vxoLRNOperationSW_Initialize(&layer->lrn_sw_operation,
                                                &layer->base,
                                                input,
                                                output,
                                                batch_count,
                                                norm_type,
                                                norm_size,
                                                div,
                                                alpha,
                                                beta,
                                                bias,
                                                op_index));
        break;

    default:
        status = VX_ERROR_NOT_SUPPORTED;
        break;
    }

OnError:
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNNormalization(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalization_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalization_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_Normalization_params) - 1)
       return VX_ERROR_INVALID_PARAMETERS;

    ptr->type = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalization_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_context context = vxGetContext((vx_reference)node);

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  type_s                     = (vx_scalar)parameters[1];
    vx_scalar  norm_size_s                = (vx_scalar)parameters[2];
    vx_scalar  alpha_s                    = (vx_scalar)parameters[3];
    vx_scalar  beta_s                     = (vx_scalar)parameters[4];
    vx_tensor  outputs                    = (vx_tensor)parameters[5];

    vx_enum    norm_type                  = type_s->value->e;
    vx_uint32  norm_size                  = norm_size_s->value->u32;
    vx_uint32  div                        = norm_type == VX_NN_NORMALIZATION_SAME_MAP ? norm_size * norm_size : norm_size;
    vx_float32 alpha                      = alpha_s->value->f32;
    vx_float32 beta                       = beta_s->value->f32;
    vx_enum    input_format               = TENSOR_DATA_TYPE(inputs);
    vx_enum    output_format              = TENSOR_DATA_TYPE(outputs);
    vx_bool    sammap_flag                = vx_false_e;
    vx_bool    acrossmap_flag             = vx_false_e;
    vx_bool    dataformat_flag[6]         = {vx_false_e};
    vx_bool    norm_config[3]             = {vx_false_e};
    vx_bool    generic_flag               = vx_false_e;
    vx_bool    isuint8_flag               = vx_false_e;
    vx_bool    norm_shader_flag           = vx_false_e;
    vx_uint32  batch_count                = TENSOR_DIM_NUM(inputs) > 3 ? TENSOR_SIZE_INDEX(inputs, 3) : 1;
    vxnne_normalization_layer lrn_layer   = VX_NULL;
    vxnne_operation_target_e target       = VXNNE_OPERATION_TARGET_NONE;
    vx_uint32  size_x, size_y;
    vx_uint32  op_index                   = 0;

    size_x = TENSOR_SIZE_INDEX(inputs, 0);
    size_y = TENSOR_SIZE_INDEX(inputs, 1);

    /* Destroy the existing layer. */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    /* Create the layer. */
    lrn_layer = (vxnne_normalization_layer)vxAllocate(sizeof(vxnne_normalization_layer_s));
    if (!lrn_layer)
    {
        vxError("Allocate memory fail at function %s line %d.", __FUNCTION__, __LINE__);
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }

    vxmONERROR(vxnneLayer_Initialize(&lrn_layer->base,
                                     "NormalizationLayer",
                                     node,
                                     vxmOPERATION_COUNT(lrn_layer),
                                     lrn_layer->operations,
                                     VX_NULL));

    sammap_flag        = (vx_bool)(norm_type == VX_NN_NORMALIZATION_SAME_MAP);
    acrossmap_flag     = (vx_bool)(norm_type == VX_NN_NORMALIZATION_ACROSS_MAPS);
    norm_config[0]     = (vx_bool)(norm_size == 3 && beta == 0.75);
    norm_config[1]     = (vx_bool)(norm_size == 5 && beta == 0.75);
    norm_config[2]     = (vx_bool)(norm_size == 11 && beta == 0.5);
    dataformat_flag[0] = (vx_bool)((input_format == VX_TYPE_FLOAT16 || input_format == VX_TYPE_INT8) && (output_format == VX_TYPE_FLOAT16 || output_format == VX_TYPE_INT8));
    dataformat_flag[1] = (vx_bool)(input_format == VX_TYPE_INT16 && output_format == VX_TYPE_INT16);
    dataformat_flag[2] = (vx_bool)(input_format == VX_TYPE_UINT8 && output_format == VX_TYPE_UINT8);
    dataformat_flag[3] = (vx_bool)(input_format == VX_TYPE_FLOAT16 && output_format == VX_TYPE_FLOAT16);
    dataformat_flag[4] = (vx_bool)(input_format == VX_TYPE_FLOAT32 && output_format == VX_TYPE_FLOAT32);
    dataformat_flag[5] = (vx_bool)(input_format == VX_TYPE_UINT8 && output_format == VX_TYPE_FLOAT16);
    isuint8_flag       = (vx_bool)((acrossmap_flag && norm_config[0] && dataformat_flag[2])
        || (acrossmap_flag && norm_config[1] && dataformat_flag[2])
        || (acrossmap_flag && norm_config[2] && dataformat_flag[2]));
    generic_flag       = (vx_bool)((acrossmap_flag && dataformat_flag[0]) || (sammap_flag && dataformat_flag[3])
                                    || (dataformat_flag[1])
                                    || (acrossmap_flag && dataformat_flag[5]));
    norm_shader_flag   = (vx_bool)((sammap_flag && norm_config[0] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[0] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[1] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[2] && (dataformat_flag[0] || dataformat_flag[1]))
        || generic_flag || isuint8_flag || dataformat_flag[4]);

    /* Choose acceleration path. */
    if (vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_LRN) &&
        norm_size <= 5 &&
        (norm_type == VX_NN_NORMALIZATION_SAME_MAP || size_x * size_y < 65536))
    {
        target = VXNNE_OPERATION_TARGET_TP;
    }
    else if (norm_shader_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        target = VXNNE_OPERATION_TARGET_SH;
    }
    else
    {
        target = VXNNE_OPERATION_TARGET_SW;
    }

    vxmONERROR(_InitializeLRNOperation(lrn_layer,
                                       target,
                                       inputs,
                                       outputs,
                                       batch_count,
                                       norm_type,
                                       norm_size,
                                       div,
                                       alpha,
                                       beta,
                                       1.0f, /* bias */
                                       &op_index));

    node->layer = &lrn_layer->base;

    return status;

OnError:
    if (lrn_layer != NULL)
    {
        vxFree(lrn_layer);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalization_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNNormalizationLayer2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalizationLayer2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalizationLayer2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalizationLayer2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  type_s                     = (vx_scalar)parameters[1];
    vx_scalar  norm_size_s                = (vx_scalar)parameters[2];
    vx_scalar  alpha_s                    = (vx_scalar)parameters[3];
    vx_scalar  beta_s                     = (vx_scalar)parameters[4];
    vx_scalar  bias_s                     = (vx_scalar)parameters[5];
    vx_tensor  outputs                    = (vx_tensor)parameters[6];

    vx_enum    norm_type                  = type_s->value->e;
    vx_uint32  norm_size                  = norm_size_s->value->u32;
    vx_float32 alpha                      = alpha_s->value->f32;
    vx_float32 beta                       = beta_s->value->f32;
    vx_float32 bias                       = bias_s->value->f32;
    vx_enum    input_format               = TENSOR_DATA_TYPE(inputs);
    vx_enum    output_format              = TENSOR_DATA_TYPE(outputs);
    vx_bool    sammap_flag                = vx_false_e;
    vx_bool    acrossmap_flag             = vx_false_e;
    vx_bool    dataformat_flag[6]         = {vx_false_e};
    vx_bool    norm_config[3]             = {vx_false_e};
    vx_bool    generic_flag               = vx_false_e;
    vx_bool    isuint8_flag               = vx_false_e;
    vx_bool    norm_shader_flag           = vx_false_e;

    vx_uint32  size_x                     = TENSOR_SIZE_INDEX(inputs, 0);
    vx_uint32  size_y                     = TENSOR_SIZE_INDEX(inputs, 1);
    vx_uint32  batch_count                = TENSOR_DIM_NUM(inputs) > 3 ? TENSOR_SIZE_INDEX(inputs, 3) : 1;
    vx_context context                    = node->base.context;
    vxnne_normalization_layer lrn2_layer  = VX_NULL;
    vxnne_operation_target_e target       = VXNNE_OPERATION_TARGET_NONE;
    vx_uint32 op_index                    = 0;

    /* Destroy the existing layer. */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    /* Create the layer. */
    lrn2_layer = (vxnne_normalization_layer)vxAllocate(gcmSIZEOF(vxnne_normalization_layer_s));
    if (!lrn2_layer)
    {
        vxError("Allocate memory fail at function %s line %d.", __FUNCTION__, __LINE__);
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }

    vxmONERROR(vxnneLayer_Initialize(&lrn2_layer->base,
                                     "NormalizationLayer2",
                                     node,
                                     vxmOPERATION_COUNT(lrn2_layer),
                                     lrn2_layer->operations,
                                     VX_NULL));

    sammap_flag        = (vx_bool)(norm_type == VX_NN_NORMALIZATION_SAME_MAP);
    acrossmap_flag     = (vx_bool)(norm_type == VX_NN_NORMALIZATION_ACROSS_MAPS);
    norm_config[0]     = (vx_bool)(norm_size == 3 && beta == 0.75);
    norm_config[1]     = (vx_bool)(norm_size == 5 && beta == 0.75);
    norm_config[2]     = (vx_bool)(norm_size == 11 && beta == 0.5);
    dataformat_flag[0] = (vx_bool)((input_format == VX_TYPE_FLOAT16 || input_format == VX_TYPE_INT8) && (output_format == VX_TYPE_FLOAT16 || output_format == VX_TYPE_INT8));
    dataformat_flag[1] = (vx_bool)(input_format == VX_TYPE_INT16 && output_format == VX_TYPE_INT16);
    dataformat_flag[2] = (vx_bool)(input_format == VX_TYPE_UINT8 && output_format == VX_TYPE_UINT8);
    dataformat_flag[3] = (vx_bool)(input_format == VX_TYPE_FLOAT16 && output_format == VX_TYPE_FLOAT16);
    dataformat_flag[4] = (vx_bool)(input_format == VX_TYPE_FLOAT32 && output_format == VX_TYPE_FLOAT32);
    dataformat_flag[5] = (vx_bool)(input_format == VX_TYPE_UINT8 && output_format == VX_TYPE_FLOAT16);
    isuint8_flag       = (vx_bool)((acrossmap_flag && norm_config[0] && dataformat_flag[2])
        || (acrossmap_flag && norm_config[1] && dataformat_flag[2])
        || (acrossmap_flag && norm_config[2] && dataformat_flag[2]));
    generic_flag       = (vx_bool)((acrossmap_flag && dataformat_flag[0]) || (sammap_flag && dataformat_flag[3])
                                    || (dataformat_flag[1])
                                    || (acrossmap_flag && dataformat_flag[5]));
    norm_shader_flag   = (vx_bool)((sammap_flag && norm_config[0] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[0] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[1] && dataformat_flag[0])
        || (acrossmap_flag && norm_config[2] && (dataformat_flag[0] || dataformat_flag[1]))
        || generic_flag || isuint8_flag || dataformat_flag[4]);

    /* Choose acceleration path. */
    if (vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_LRN) &&
        norm_size <= 5 &&
        (norm_type == VX_NN_NORMALIZATION_SAME_MAP || size_x * size_y < 65536))
    {
        target = VXNNE_OPERATION_TARGET_TP;
    }
    else if (norm_shader_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        target = VXNNE_OPERATION_TARGET_SH;
    }
    else
    {
        target = VXNNE_OPERATION_TARGET_SW;
    }

    vxmONERROR(_InitializeLRNOperation(lrn2_layer,
                                       target,
                                       inputs,
                                       outputs,
                                       batch_count,
                                       norm_type,
                                       norm_size,
                                       1, /* div */
                                       alpha,
                                       beta,
                                       bias,
                                       &op_index));

    node->layer = &lrn2_layer->base;

    return status;

OnError:
    if (lrn2_layer)
    {
        if (lrn2_layer->lrn_tp_operation.base.parameter.tp_value)
        {
            vxFree(lrn2_layer->lrn_tp_operation.base.parameter.tp_value);
        }
        vxFree(lrn2_layer);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNormalizationLayer2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNPoolingLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_PoolingLayer_params) - 1) return VX_ERROR_INVALID_PARAMETERS;

    ptr->type                 = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

vx_status vxnnePoolingOperation_Deinitialize(vxnne_operation_s *operation)
{
    vxnne_pooling_operation pooling_operation = (vxnne_pooling_operation)operation;

    if (pooling_operation->weights_biases != VX_NULL)
    {
        vxReleaseWeightsBiasesParameter(&pooling_operation->weights_biases);
    }

    vxnneOperation_Deinitialize(operation);
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnnePoolingInitializer(
    vx_node node,
    char* name,
    vx_tensor inputs,
    vx_scalar pool_type_s,
    vx_scalar pool_size_x_s,
    vx_scalar pool_size_y_s,
    vx_uint32 pool_pad_x_left,
    vx_uint32 pool_pad_x_right,
    vx_uint32 pool_pad_y_top,
    vx_uint32 pool_pad_y_bottom,
    vx_scalar rounding_s,
    vx_enum padMode,
    vx_scalar padConst,
    vx_tensor outputs
    )
{
    vx_status  status                        = VX_SUCCESS;

    vx_enum poolTypeValue                    = pool_type_s->value->e;
    vx_uint32 poolSizeXValue                 = pool_size_x_s->value->u32;
    vx_uint32 poolSizeYValue                 = pool_size_y_s->value->u32;
    vx_enum roundingValue                    = rounding_s->value->e;
    vx_enum inputdata_format                 = TENSOR_DATA_TYPE(inputs);
    vx_enum outputdata_format                = TENSOR_DATA_TYPE(outputs);
    vx_bool dataFormat_AvgPool_flag[6]       = {vx_false_e};
    vx_bool avgPool_flag                     = vx_false_e;
    vx_bool enable_tf_quantize               = vx_false_e;
    vx_bool enable_int16_sh                  = vx_false_e;
    vx_context context                       = vxGetContext((vx_reference)node);

    vx_uint32 inputsWidth, inputsHeight, outputsWidth, outputsHeight;
    vx_int32  inputsDepth, outputsDepth;
    vxnne_pooling_layer  poolingLayer = gcvNULL;
    vx_uint32  stride = 0;
    vx_uint32 totalSize = 0;
    vx_uint32 maxAllocateSize = 256 * 1024 * 1024; /* set max allocate size because fpga out of memory when using nn do avg pooling, max value is 256M */
    vx_uint32 batchCount;


    inputsWidth   = TENSOR_SIZE_INDEX(inputs, 0);
    inputsHeight  = TENSOR_SIZE_INDEX(inputs, 1);
    inputsDepth   = TENSOR_SIZE_INDEX(inputs, 2);
    batchCount    = TENSOR_SIZE_INDEX(inputs, 3);
    outputsWidth  = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    outputsHeight = TENSOR_VIEW_SIZE_INDEX(outputs, 1);
    outputsDepth  = TENSOR_VIEW_SIZE_INDEX(outputs, 2);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_pooling_layer_s), (gctPOINTER*)&poolingLayer);
    if (!poolingLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(poolingLayer, sizeof(vxnne_pooling_layer_s));

    vxnneLayer_Initialize(&poolingLayer->base,
                          name,
                          node,
                          vxmOPERATION_COUNT(poolingLayer),
                          poolingLayer->operations,
                          vxnneLayer_Deinitialize);


    if (outputsWidth == 1)
    {
        stride = 1;
    }
    else
    {
        stride = vxoNNExternsionConvlutionRound((vx_float32)(inputsWidth + pool_pad_x_left + pool_pad_x_right - poolSizeXValue) / (outputsWidth - 1), roundingValue);
    }

    totalSize = poolSizeXValue * poolSizeYValue * inputsDepth * outputsDepth * (vx_uint32)vxDataType_GetSize((vx_type_e)TENSOR_DATA_TYPE(inputs)) + outputsDepth * sizeof(vx_float32);

    if(context->evisNoInst.supportEVIS)
    {
        dataFormat_AvgPool_flag[0] = (vx_bool)(inputdata_format == VX_TYPE_INT8 && outputdata_format == VX_TYPE_FLOAT16);
        dataFormat_AvgPool_flag[1] = (vx_bool)(inputdata_format == VX_TYPE_FLOAT16 && outputdata_format == VX_TYPE_INT8);
        dataFormat_AvgPool_flag[2] = (vx_bool)(inputdata_format == VX_TYPE_INT8 && outputdata_format == VX_TYPE_INT8);
        dataFormat_AvgPool_flag[3] = (vx_bool)(inputdata_format == VX_TYPE_FLOAT16 && outputdata_format == VX_TYPE_FLOAT16);
        dataFormat_AvgPool_flag[4] = (vx_bool)(inputdata_format == VX_TYPE_INT16);
        dataFormat_AvgPool_flag[5] = (vx_bool)(inputdata_format == VX_TYPE_UINT8 );
    }
    else
    {
        dataFormat_AvgPool_flag[0] = vx_false_e;
        dataFormat_AvgPool_flag[1] = vx_false_e;
        dataFormat_AvgPool_flag[2] = vx_false_e;
        dataFormat_AvgPool_flag[3] = (vx_bool)((inputdata_format == VX_TYPE_FLOAT16 && outputdata_format == VX_TYPE_FLOAT16) ||
                                               (inputdata_format == VX_TYPE_FLOAT32 && outputdata_format == VX_TYPE_FLOAT32));
        dataFormat_AvgPool_flag[4] = vx_false_e;
        dataFormat_AvgPool_flag[5] = (vx_bool)(inputdata_format == VX_TYPE_UINT8 );
    }

   enable_tf_quantize           =  (vx_bool)(dataFormat_AvgPool_flag[5] );

   enable_int16_sh              = (vx_bool)(dataFormat_AvgPool_flag[4]);

   avgPool_flag   = (vx_bool)(((dataFormat_AvgPool_flag[0])
                            ||  (dataFormat_AvgPool_flag[1])
                            ||  (dataFormat_AvgPool_flag[2])
                            ||  (dataFormat_AvgPool_flag[3])
                            ||  (enable_tf_quantize == vx_true_e)
                            ||  (enable_int16_sh == vx_true_e))
                            &&  (poolTypeValue == VX_NN_POOLING_AVG || poolTypeValue == VX_NN_POOLING_AVG_ANDROID));

    /* if the needed total size is larger than maxAllocateSize, do pooling with CPU version. maybe need implement avg pooling with shader */
    if (vxnneIsNNSupportFormat(context, inputs, VX_NULL, outputs) &&
        (poolTypeValue == VX_NN_POOLING_AVG) &&
        (stride == 1) &&
        (totalSize <= maxAllocateSize) &&
        (avgPool_flag == vx_false_e))
    {
        status = vxnneOperation_Initialize(&poolingLayer->pooling_nne_operation.base,
                                           &poolingLayer->base,
                                           VXNNE_OPERATION_TARGET_NN,
                                           VXNNE_OPERATOR_POOLING,
                                           VX_NULL,
                                           vxnnePoolingOperation_Deinitialize,
                                           batchCount,
                                           NNE_COMMAND_SIZE);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &poolingLayer->base,
            &poolingLayer->pooling_nne_operation.base,
            0);

        poolingLayer->pooling_nne_operation.inputs            = inputs;
        poolingLayer->pooling_nne_operation.pool_type         = poolTypeValue;
        poolingLayer->pooling_nne_operation.pool_size_x       = poolSizeXValue;
        poolingLayer->pooling_nne_operation.pool_size_y       = poolSizeYValue;
        poolingLayer->pooling_nne_operation.pool_pad_x_left   = pool_pad_x_left;
        poolingLayer->pooling_nne_operation.pool_pad_x_right  = pool_pad_x_right;
        poolingLayer->pooling_nne_operation.pool_pad_y_top    = pool_pad_y_top;
        poolingLayer->pooling_nne_operation.pool_pad_y_bottom = pool_pad_y_bottom;
        poolingLayer->pooling_nne_operation.rounding          = roundingValue;
        poolingLayer->pooling_nne_operation.outputs           = outputs;

        vxnneOperation_AddReference(&poolingLayer->pooling_nne_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&poolingLayer->pooling_nne_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        /* prepare data for nne */
        {
            vx_int8 *weightData = VX_NULL;
            vx_float32 *biasData = VX_NULL;
            vx_uint32 i, j;
            vx_int32 w, z;
            vx_int8 *weightsValuePtr;
            vx_int8 *zerosValuePtr;
            vx_uint32 weightItemCount;
            vx_int8 *pWeightData;
            vx_float32 *pBiasData;
            vx_tensor weights = VX_NULL;
            vx_tensor biases = VX_NULL;
            vx_context context = VX_NULL;
            vx_uint32 numWeightDims = 4, numBiasDims = 1;
            vx_uint32 weightSize[4] = {poolSizeXValue, poolSizeYValue, inputsDepth, outputsDepth};
            vx_uint32 weightStrideSize[4];
            vx_uint32 biasSize[4] = {outputsDepth};
            vx_uint32 biasStrideSize[1];
            vx_tensor_addressing weightUserAddr = NULL;
            vx_tensor_addressing biasUserAddr = NULL;
            vx_weights_biases_parameter weights_biases = VX_NULL;
            vx_type_e inputDataFormat = (vx_type_e)TENSOR_DATA_TYPE(inputs);
            vx_type_e weightDataFormat = inputDataFormat;
            vx_type_e biasDataFormat = (weightDataFormat == VX_TYPE_INT8 || weightDataFormat == VX_TYPE_UINT8) ? VX_TYPE_INT32 : VX_TYPE_FLOAT32;
            vx_int32 weightItemSize = vxnneGetTypeSize(weightDataFormat);
            vx_int32 biasItemSize = vxnneGetTypeSize(biasDataFormat);
            vx_int8 weightFixPointPos, biasFixPointPos;
            vx_enum weightRoundingMode;

            context = vxGetContext((vx_reference)node);
            if (context == VX_NULL)
            {
                vxError("vxGetContext fail at function %s line %d", __FUNCTION__, __LINE__);
                goto exit;
            }

            /* generate special weight and bias data for average pooling. create weightsBiasesParameters from this specail weight and bias */
            weightsValuePtr = (vx_int8*)vxAllocateAndZeroMemory(poolSizeXValue * poolSizeYValue * weightItemSize);
            zerosValuePtr = (vx_int8*)vxAllocate(poolSizeXValue * poolSizeYValue * weightItemSize);

            weightData = (vx_int8*)vxAllocate(poolSizeXValue * poolSizeYValue * inputsDepth * outputsDepth * weightItemSize);
            biasData = (vx_float32*)vxAllocate(outputsDepth * biasItemSize);

            if (weightsValuePtr == NULL || zerosValuePtr == NULL || weightData == NULL || biasData == NULL)
            {
                status = VX_ERROR_NO_MEMORY;
                vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
                if (weightsValuePtr != VX_NULL) vxFree(weightsValuePtr);
                if (zerosValuePtr != VX_NULL) vxFree(zerosValuePtr);
                if (weightData != VX_NULL) vxFree(weightData);
                if (biasData != VX_NULL) vxFree(biasData);
                goto exit;
            }

            weightItemCount = poolSizeXValue * poolSizeYValue;

            if (weightDataFormat == VX_TYPE_UINT8 && TENSOR_DATA_TYPE(outputs) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(inputs) == VX_QUANT_AFFINE_SCALE)
            {
                vx_tensor_create_params_t  tensor_create_params;
                vx_weights_biases_parameter_optimizations_t opt = {0};
                vx_float32 scale = 1.0f / (vx_float32)(255 * weightItemCount);

                opt.inputZeroPoint = TENSOR_TF_ZEROPOINT(inputs);
                opt.zrl = -1;
                opt.outputFormat = TENSOR_DATA_TYPE(outputs);

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = numWeightDims;
                tensor_create_params.sizes = weightSize;
                tensor_create_params.data_format = weightDataFormat;
                tensor_create_params.quant_format = VX_QUANT_AFFINE_SCALE;
                tensor_create_params.quant_data.affine.scale = scale;
                tensor_create_params.quant_data.affine.zeroPoint = 0;

                weights = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);
                if (weights == NULL)
                {
                    status = VX_ERROR_NO_MEMORY;
                    vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
                    if (weightsValuePtr != VX_NULL) vxFree(weightsValuePtr);
                    if (zerosValuePtr != VX_NULL) vxFree(zerosValuePtr);
                    if (weightData != VX_NULL) vxFree(weightData);
                    if (biasData != VX_NULL) vxFree(biasData);
                    goto exit;
                }

                weightRoundingMode = TENSOR_ROUNDING_MODE(weights);

                for (j = 0; j < weightItemCount; j++)
                {
                    vxnneSaveDataQuant(weightDataFormat, j, 1.0f/weightItemCount, weightsValuePtr, 0, scale, weightRoundingMode);
                    vxnneSaveDataQuant(weightDataFormat, j, 0.0f, zerosValuePtr, 0, scale, weightRoundingMode);
                }

                pWeightData = weightData;
                pBiasData = biasData;

                for (w = 0; w < outputsDepth; w++)
                {
                    for (z = 0; z < inputsDepth; z++)
                    {
                        if (w == z)
                        {
                            memcpy(pWeightData, weightsValuePtr, weightItemCount * weightItemSize);
                        }
                        else
                        {
                            memcpy(pWeightData, zerosValuePtr, weightItemCount * weightItemSize);
                        }
                        pWeightData += weightItemCount * weightItemSize;
                    }
                    *pBiasData++ = 0.0f;
                }

                weightStrideSize[0] = weightItemSize;
                for (i = 1; i < numWeightDims; i++)
                {
                    weightStrideSize[i] =  weightStrideSize[i-1] * weightSize[i-1];
                }

                weightUserAddr = vxCreateTensorAddressing(context, weightSize, weightStrideSize, (vx_uint8)numWeightDims);

                vxoCopyTensorPatch(weights, VX_NULL, weightUserAddr, weightData, VX_WRITE_ONLY,0);

                biasStrideSize[0] = biasItemSize;

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = numBiasDims;
                tensor_create_params.sizes = biasSize;
                tensor_create_params.data_format = biasDataFormat;
                tensor_create_params.quant_format = VX_QUANT_AFFINE_SCALE;
                tensor_create_params.quant_data.affine.scale = scale * TENSOR_TF_SCALE(inputs) / TENSOR_TF_SCALE(outputs);;
                tensor_create_params.quant_data.affine.zeroPoint = 0;

                biases = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

                biasUserAddr = vxCreateTensorAddressing(context, biasSize, biasStrideSize, (vx_uint8)numBiasDims);

                vxoCopyTensorPatch(biases, VX_NULL, biasUserAddr, biasData, VX_WRITE_ONLY,0);

                weights_biases = _createWeightsBiasesParameterFromTensors(context,
                                                                       VX_NN_CONVOLUTION_LAYER,
                                                                       (vx_uint32*)(TENSOR_SIZES(inputs)),
                                                                       inputs->dimCount,
                                                                       inputs->dimCount,
                                                                       pool_pad_x_left,
                                                                       pool_pad_x_right,
                                                                       pool_pad_y_top,
                                                                       pool_pad_y_bottom,
                                                                       0,
                                                                       0,
                                                                       0,
                                                                       0,
                                                                       roundingValue,
                                                                       (vx_uint32*)(TENSOR_SIZES(outputs)),
                                                                       VX_NULL,
                                                                       &opt,
                                                                       TENSOR_DATA_TYPE(weights),
                                                                       0,
                                                                       VX_TENSOR_RANK_WHCN,
                                                                       weights,
                                                                       biases,
                                                                       VX_NULL,
                                                                       vx_false_e,
                                                                       vx_false_e);
            }
            else
            {
                vx_tensor_create_params_t tensor_create_params;

                weightFixPointPos = (vx_int8)(8 - gcoMATH_Ceiling(gcoMATH_Log(1.0f/weightItemCount) + 1));
                biasFixPointPos = weightFixPointPos + TENSOR_POS(inputs);

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = numWeightDims;
                tensor_create_params.sizes = weightSize;
                tensor_create_params.data_format = weightDataFormat;
                tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;
                tensor_create_params.quant_data.dfp.fixed_point_pos = weightFixPointPos;

                weights = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

                weightRoundingMode = TENSOR_ROUNDING_MODE(weights);

                for (j = 0; j < weightItemCount; j++)
                {
                    vxnneSaveData(weightDataFormat, j, 1.0f/weightItemCount, weightsValuePtr, weightFixPointPos, weightRoundingMode);
                    vxnneSaveData(weightDataFormat, j, 0.0f, zerosValuePtr, weightFixPointPos, weightRoundingMode);
                }

                pWeightData = weightData;
                pBiasData = biasData;

                for (w = 0; w < outputsDepth; w++)
                {
                    for (z = 0; z < inputsDepth; z++)
                    {
                        if (w == z)
                        {
                            memcpy(pWeightData, weightsValuePtr, weightItemCount * weightItemSize);
                        }
                        else
                        {
                            memcpy(pWeightData, zerosValuePtr, weightItemCount * weightItemSize);
                        }
                        pWeightData += weightItemCount * weightItemSize;
                    }
                    *pBiasData++ = 0.0f;
                }

                weightStrideSize[0] = weightItemSize;
                for (i = 1; i < numWeightDims; i++)
                {
                    weightStrideSize[i] =  weightStrideSize[i-1] * weightSize[i-1];
                }

                weightUserAddr = vxCreateTensorAddressing(context, weightSize, weightStrideSize, (vx_uint8)numWeightDims);

                vxoCopyTensorPatch(weights, VX_NULL, weightUserAddr, weightData, VX_WRITE_ONLY,0);

                biasStrideSize[0] = biasItemSize;

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = numBiasDims;
                tensor_create_params.sizes = biasSize;
                tensor_create_params.data_format = biasDataFormat;
                tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;
                tensor_create_params.quant_data.dfp.fixed_point_pos = biasFixPointPos;

                biases = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);
                biasUserAddr = vxCreateTensorAddressing(context, biasSize, biasStrideSize, (vx_uint8)numBiasDims);

                vxoCopyTensorPatch(biases, VX_NULL, biasUserAddr, biasData, VX_WRITE_ONLY,0);

                weights_biases = _createWeightsBiasesParameterFromTensors(context,
                                                                       VX_NN_CONVOLUTION_LAYER,
                                                                       (vx_uint32*)(TENSOR_SIZES(inputs)),
                                                                       inputs->dimCount,
                                                                       inputs->dimCount,
                                                                       pool_pad_x_left,
                                                                       pool_pad_x_right,
                                                                       pool_pad_y_top,
                                                                       pool_pad_y_bottom,
                                                                       0,
                                                                       0,
                                                                       0,
                                                                       0,
                                                                       roundingValue,
                                                                       (vx_uint32*)(TENSOR_SIZES(outputs)),
                                                                       VX_NULL,
                                                                       VX_NULL,
                                                                       TENSOR_DATA_TYPE(weights),
                                                                       0,
                                                                       VX_TENSOR_RANK_WHCN,
                                                                       weights,
                                                                       biases,
                                                                       VX_NULL,
                                                                       vx_false_e,
                                                                       vx_false_e);
            }

            if (weightsValuePtr != VX_NULL)
                vxFree(weightsValuePtr);

            if (zerosValuePtr != VX_NULL)
                vxFree(zerosValuePtr);

            if (weightData != VX_NULL)
                vxFree(weightData);

            if (biasData != VX_NULL)
                vxFree(biasData);

            {
                vx_op_param_s conv = {0};

                conv.pad_x_left = pool_pad_x_left;
                conv.pad_x_right = pool_pad_x_right;
                conv.pad_y_top = pool_pad_y_top;
                conv.pad_y_bottom = pool_pad_y_bottom;
                conv.pad_mode = VX_PAD_CONSTANT;
                conv.pad_const = 0,
                conv.pool_type = VIV_NN_POOLING_NON;
                conv.conv_rounding_type = roundingValue;
                conv.enable_relu = vx_false_e;
                conv.pool_size_x = conv.pool_size_y = 0;
                /* fill in cmd buffer */
                memcpy(&poolingLayer->pooling_nne_operation.base.parameter, &conv, sizeof(vx_op_param_s));
            }

            poolingLayer->pooling_nne_operation.weights_biases = weights_biases;

            if (weights != VX_NULL)
            {
                vxoTensor_ReleaseTensor(&weights);
            }

            if (biases != VX_NULL)
            {
                vxoTensor_ReleaseTensor(&biases);
            }

            if (weightUserAddr != VX_NULL)
            {
                vxReleaseTensorAddressing(&weightUserAddr);
            }

            if (biasUserAddr != VX_NULL)
            {
                vxReleaseTensorAddressing(&biasUserAddr);
            }
        }
    }
    else
    {
        vx_bool isTpSupportFormat = vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs);
        vx_bool isStride1Support = ((stride == 1) && gcoHAL_IsFeatureAvailable(gcvNULL, gcvFEATURE_TP_MAX_POOLING_STRIDE1) && poolSizeXValue <= 3) ? vx_true_e : vx_false_e;
        vx_bool isStride2Support = ((stride == 2) && (stride == poolSizeXValue || stride == poolSizeXValue-1)) ? vx_true_e : vx_false_e;
        vx_bool isPoolSizeSupport = (poolSizeXValue == 1 && !pool_pad_x_left && !pool_pad_y_top && TENSOR_VIEW_SIZE_INDEX(inputs, 0) % stride == 0 && TENSOR_VIEW_SIZE_INDEX(inputs, 1) % stride == 0) ? vx_true_e : vx_false_e;

        /* stride!=2 is not supported yet */
        if ((poolTypeValue == VX_NN_POOLING_MAX) &&
            isTpSupportFormat &&
            vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_MAX_POOLING) &&
            (isStride1Support || isStride2Support || isPoolSizeSupport) &&
            (poolSizeXValue <= 64))
        {
            vx_op_param_s conv = {0};

            status = vxnneOperation_Initialize(&poolingLayer->pooling_tp_operation.base,
                                               &poolingLayer->base,
                                               VXNNE_OPERATION_TARGET_TP,
                                               VXNNE_OPERATOR_POOLING,
                                               VX_NULL,
                                               VX_NULL,
                                               batchCount,
                                               0);
            if (status != VX_SUCCESS) goto exit;

            conv.pad_x_left   = pool_pad_x_left;
            conv.pad_x_right  = pool_pad_x_right;
            conv.pad_y_top    = pool_pad_y_top;
            conv.pad_y_bottom = pool_pad_y_bottom;
            conv.pool_size_x  = poolSizeXValue;
            conv.pool_size_y  = poolSizeYValue;
            conv.pool_stride  = stride;
            conv.enable_relu  = vx_false_e;
            conv.conv_rounding_type = roundingValue;
            conv.pad_mode = VX_PAD_REPLICATE;
            conv.pad_const = 0;
            conv.tpType = TP_MAX_POOLING;
            conv.other_ref = gcvNULL;
            conv.data_buff = gcvNULL;

            memcpy(&poolingLayer->pooling_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

            vxnneLayer_SetOperation(
                &poolingLayer->base,
                &poolingLayer->pooling_tp_operation.base,
                0);

            poolingLayer->pooling_tp_operation.input  = inputs;
            poolingLayer->pooling_tp_operation.output = outputs;

            vxnneOperation_AddReference(&poolingLayer->pooling_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&poolingLayer->pooling_tp_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vx_bool kernel_MaxPool_flag[5]     = {vx_false_e};
            vx_bool dataformat_MaxPool_flag[6] = {vx_false_e};
            vx_bool maxPool_flag               = vx_false_e;
            vx_bool enable_L2Pool_SH           = vx_false_e;
            vx_bool generic_flag               = vx_false_e;
            vx_bool enable_outputALU           = checkOutputTensorDoAlu(inputs, outputs);
            vx_bool enable_downSampleSH        = (vx_bool)(stride == 2 && poolSizeXValue == 1 && poolSizeYValue == 1 && (!enable_outputALU));
            vx_bool enable_tf_avgPool          = (vx_bool)(pool_pad_x_left || pool_pad_x_right ||pool_pad_y_top || pool_pad_y_bottom);

            kernel_MaxPool_flag[0]             = (vx_bool)(stride == 2 && poolSizeXValue == 3 && pool_pad_x_left == 1);
            kernel_MaxPool_flag[1]             = (vx_bool)(stride == 2 && poolSizeXValue == 2 && pool_pad_x_left == 0);
            kernel_MaxPool_flag[2]             = (vx_bool)(stride == 2 && poolSizeXValue == 3 && pool_pad_x_left == 0);
            kernel_MaxPool_flag[3]             = (vx_bool)(stride == 1 && poolSizeXValue == 3 && pool_pad_x_left == 1);
            kernel_MaxPool_flag[4]             = (vx_bool)(stride == 1 && poolSizeXValue == 2 && poolSizeYValue == 2 && pool_pad_x_left == 0 && pool_pad_y_top == 0);

            if(context->evisNoInst.supportEVIS)
            {
                dataformat_MaxPool_flag[0]         = (vx_bool)((inputdata_format == VX_TYPE_FLOAT16 || inputdata_format == VX_TYPE_INT8)
                                                            && (outputdata_format == VX_TYPE_FLOAT16 || outputdata_format == VX_TYPE_INT8));
                dataformat_MaxPool_flag[1]         = (vx_bool)(inputdata_format == VX_TYPE_INT16 && outputdata_format == VX_TYPE_INT16);
                dataformat_MaxPool_flag[2]         = (vx_bool)(inputdata_format == VX_TYPE_UINT8 && outputdata_format == VX_TYPE_UINT8);
                dataformat_MaxPool_flag[3]         = (vx_bool)(inputdata_format == VX_TYPE_FLOAT16 && outputdata_format == VX_TYPE_FLOAT16);
                dataformat_MaxPool_flag[4]         = (vx_bool)(inputdata_format == VX_TYPE_INT8 && outputdata_format == VX_TYPE_INT8);
                dataformat_MaxPool_flag[5]         = (vx_bool)(inputdata_format == VX_TYPE_INT16 && outputdata_format == VX_TYPE_INT16);
                enable_L2Pool_SH                   = (vx_bool)((inputdata_format == VX_TYPE_FLOAT16 || inputdata_format == VX_TYPE_UINT8) && (outputdata_format == VX_TYPE_FLOAT16 || outputdata_format == VX_TYPE_UINT8) && (poolTypeValue == VX_NN_POOLING_L2));

            }
            else
            {
                dataformat_MaxPool_flag[0]         = vx_false_e;
                dataformat_MaxPool_flag[1]         = vx_false_e;
                dataformat_MaxPool_flag[2]         = (vx_bool)(inputdata_format == VX_TYPE_UINT8 && outputdata_format == VX_TYPE_UINT8);
                dataformat_MaxPool_flag[3]         = (vx_bool)((inputdata_format == VX_TYPE_FLOAT16 && outputdata_format == VX_TYPE_FLOAT16) ||
                                                                (inputdata_format == VX_TYPE_FLOAT32 && outputdata_format == VX_TYPE_FLOAT32));
                dataformat_MaxPool_flag[4]         = vx_false_e;
                dataformat_MaxPool_flag[5]         = vx_false_e;
                enable_L2Pool_SH                   = (vx_bool)((dataformat_MaxPool_flag[2] || dataformat_MaxPool_flag[3]) && (poolTypeValue == VX_NN_POOLING_L2));
            }

            generic_flag                       = (vx_bool)(dataformat_MaxPool_flag[2] || dataformat_MaxPool_flag[3] || dataformat_MaxPool_flag[4] || dataformat_MaxPool_flag[5]);
            maxPool_flag                       = (vx_bool)((((kernel_MaxPool_flag[0] || kernel_MaxPool_flag[1] || kernel_MaxPool_flag[2] || kernel_MaxPool_flag[3]) && dataformat_MaxPool_flag[0])
                                               || (kernel_MaxPool_flag[3] && dataformat_MaxPool_flag[1])
                                               || ((kernel_MaxPool_flag[3] || kernel_MaxPool_flag[0]) && dataformat_MaxPool_flag[2])
                                               || (kernel_MaxPool_flag[4] && dataformat_MaxPool_flag[4])
                                               || generic_flag)
                                               && (poolTypeValue == VX_NN_POOLING_MAX));

            enable_downSampleSH = enable_downSampleSH && (poolTypeValue == VX_NN_POOLING_MAX || poolTypeValue == VX_NN_POOLING_AVG) && (pool_pad_x_left == 0 && pool_pad_y_top == 0);

            enable_tf_avgPool  = enable_tf_avgPool && avgPool_flag && (poolTypeValue == VX_NN_POOLING_AVG_ANDROID);

            /* tf pad avgpool, now only support fp16 to fp16 and u8 to u8*/
            enable_tf_avgPool  = enable_tf_avgPool && (dataFormat_AvgPool_flag[1] || dataFormat_AvgPool_flag[3] || dataFormat_AvgPool_flag[5]);

            if ((avgPool_flag || maxPool_flag || enable_L2Pool_SH || enable_downSampleSH) && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            {
                vxnne_shader_executable shaderExecutable = NULL;
                vx_scalar stride_s = NULL;
                stride_s = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &stride);
                if (!stride_s)
                {
                    status = VX_FAILURE;
                    goto exit;
                }

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    if (enable_tf_avgPool)
                    {
                        vx_tensor mask          = NULL;
                        vx_uint32 sizes[4]      = {1, 1, 1, 1};
                        vx_uint32 mask_size     = 0;
                        vx_uint8  *maskData     = VX_NULL;
                        vx_tensor_create_params_t tensor_create_params;

                        sizes[0]            = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
                        sizes[1]            = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
                        sizes[2]            = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
                        sizes[3]            = TENSOR_VIEW_SIZE_INDEX(inputs, 3);

                        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                        tensor_create_params.num_of_dims = TENSOR_DIM_NUM(inputs);
                        tensor_create_params.sizes = sizes;
                        tensor_create_params.data_format = VX_TYPE_UINT8;
                        tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
                        if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                        {
                            tensor_create_params.quant_data.dfp.fixed_point_pos = 0;
                        }
                        else
                        {
                            tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
                            tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
                        }
                        mask                = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
                        if (vxoTensor_AllocateMemory(mask) != VX_SUCCESS)
                        {
                            vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }
                        vxoTensor_GetTensorViewMemory(mask, (vx_ptr_ptr)&maskData, VX_NULL);
                        vxoTensor_GetTensorElementCount(mask, &mask_size);
                        memset(maskData, 1, mask_size);

                        shaderExecutable = vxnneGetTFAvgPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_TF_AVGPOOLING, &node->kernelAttributes.borderMode, inputs, mask, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, VX_NN_ACTIVATION_NONE, outputs);


                        poolingLayer->base.num_temp_tensors = 1;
                        poolingLayer->base.temp_tensors[0]  = mask;
                    }
                    else if (enable_downSampleSH)
                    {
                        vx_float32   scale_factor[2]    = {inputsWidth / (vx_float32)outputsWidth, inputsHeight / (vx_float32)outputsHeight};

                        if (scale_factor[0] == 2.0f && scale_factor[1] == 2.0f)
                            shaderExecutable = vxnneGetResizeNearestNeighborShaderExecutable(node->base.context, VXNNE_KERNEL_RESIZE_NEAREST_NEIGHBOR, &node->kernelAttributes.borderMode, inputs, VX_INTERPOLATION_NEAREST_NEIGHBOR, outputs);
                        else
                            shaderExecutable = vxnneGetMaxPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_MAXPOOLING, &node->kernelAttributes.borderMode, inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, VX_NN_ACTIVATION_NONE, outputs);

                    }
                    else if (maxPool_flag)
                        shaderExecutable = vxnneGetMaxPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_MAXPOOLING, &node->kernelAttributes.borderMode,
                        inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, VX_NN_ACTIVATION_NONE, outputs);
                    else if (enable_L2Pool_SH)
                        shaderExecutable = vxnneGetL2PoolingShaderExecutable(node->base.context, VXNNE_KERNEL_L2POOLING, &node->kernelAttributes.borderMode,
                        inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, VX_NN_ACTIVATION_NONE, outputs);
                    else if(avgPool_flag && enable_tf_quantize)
                        shaderExecutable = vxnneGetAvgPooling_UInt8ShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING_UINT8, &node->kernelAttributes.borderMode,
                        inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, VX_NN_ACTIVATION_NONE, outputs);
                    else if(avgPool_flag && enable_int16_sh)
                        shaderExecutable = vxnneGetAvgPooling_Int16ShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING_INT16, &node->kernelAttributes.borderMode,
                        inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, VX_NN_ACTIVATION_NONE, outputs);
                    else
                        shaderExecutable = vxnneGetAvgPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING, &node->kernelAttributes.borderMode,
                        inputs, pool_type_s, stride_s, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, rounding_s, outputs);
                }
                else
                {
                    vx_uint32 stride_y_value = vxoNNExternsionConvlutionRound((vx_float32)(inputsHeight + pool_pad_y_top + pool_pad_y_bottom - poolSizeYValue) / (outputsHeight - 1), roundingValue);
                    vx_scalar stride_y = NULL;

                    stride_y = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &stride_y_value);
                    if(avgPool_flag)
                    {
                        shaderExecutable = vxnneGetGPUAvgPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING, &node->kernelAttributes.borderMode,
                                        inputs, pool_type_s, stride_s, stride_y, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, pool_pad_x_right,
                                        pool_pad_y_bottom, rounding_s, outputs);
                    }
                    else if (enable_L2Pool_SH)
                    {
                        shaderExecutable = vxnneGetGPUL2PoolingShaderExecutable(node->base.context, VXNNE_KERNEL_L2POOLING, &node->kernelAttributes.borderMode,
                                        inputs, pool_type_s, stride_s, stride_y, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, pool_pad_x_right,
                                        pool_pad_y_bottom, rounding_s, outputs);
                    }
                    else if (maxPool_flag)
                    {
                        shaderExecutable = vxnneGetGPUMaxPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_MAXPOOLING, &node->kernelAttributes.borderMode,
                                       inputs, pool_type_s, stride_s, stride_y, pool_size_x_s, pool_size_y_s, pool_pad_x_left, pool_pad_y_top, pool_pad_x_right,
                                       pool_pad_y_bottom,rounding_s, outputs);
                    }

                    if (stride_y) (vxReleaseScalar(&stride_y));
                }

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    vxReleaseScalar(&stride_s);
                    goto exit;
                }

                status = vxnneShaderOperation_Initialize(&poolingLayer->pooling_sh_operation,
                                                &poolingLayer->base,
                                                VXNNE_OPERATOR_POOLING,
                                                batchCount,
                                                shaderExecutable);

                if (status != VX_SUCCESS)
                {
                    vxReleaseScalar(&stride_s);
                    goto exit;
                }

                vxnneOperation_AddReference(&poolingLayer->pooling_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&poolingLayer->pooling_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &poolingLayer->base,
                    &poolingLayer->pooling_sh_operation.base,
                    0);

                if (stride_s) (vxReleaseScalar(&stride_s));
            }
            else
            {
                status = vxnneOperation_Initialize(&poolingLayer->pooling_sw_operation.base,
                                                   &poolingLayer->base,
                                                   VXNNE_OPERATION_TARGET_SW,
                                                   VXNNE_OPERATOR_POOLING,
                                                   vxnneExecuteSWPooling,
                                                   VX_NULL,
                                                   batchCount,
                                                   0);

                vxnneLayer_SetOperation(
                    &poolingLayer->base,
                    &poolingLayer->pooling_sw_operation.base,
                    0);

                poolingLayer->pooling_sw_operation.inputs            = inputs;
                poolingLayer->pooling_sw_operation.pool_type         = pool_type_s->value->e;
                poolingLayer->pooling_sw_operation.pool_size_x       = pool_size_x_s->value->u32;
                poolingLayer->pooling_sw_operation.pool_size_y       = pool_size_y_s->value->u32;
                poolingLayer->pooling_sw_operation.pool_pad_x_left   = pool_pad_x_left;
                poolingLayer->pooling_sw_operation.pool_pad_x_right  = pool_pad_x_right;
                poolingLayer->pooling_sw_operation.pool_pad_y_top    = pool_pad_y_top;
                poolingLayer->pooling_sw_operation.pool_pad_y_bottom = pool_pad_y_bottom;
                poolingLayer->pooling_sw_operation.rounding          = rounding_s->value->e;
                poolingLayer->pooling_sw_operation.outputs           = outputs;

                vxnneOperation_AddReference(&poolingLayer->pooling_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&poolingLayer->pooling_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
        }
    }

    node->layer = &poolingLayer->base;
    return status;

exit:
    if (poolingLayer) gcoOS_Free(gcvNULL, (gctPOINTER)poolingLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  poolTypeScalar             = (vx_scalar)parameters[1];
    vx_scalar  poolSizeXScalar            = (vx_scalar)parameters[2];
    vx_scalar  poolSizeYScalar            = (vx_scalar)parameters[3];
    vx_scalar  poolPadXScalar             = (vx_scalar)parameters[4];
    vx_scalar  poolPadYScalar             = (vx_scalar)parameters[5];
    vx_scalar  roundingScalar             = (vx_scalar)parameters[6];
    vx_tensor  outputs                    = (vx_tensor)parameters[7];

    vx_uint32 pad_x_left;
    vx_uint32 pad_x_right;
    vx_uint32 pad_y_top;
    vx_uint32 pad_y_bottom;

    vx_status  status                     = VX_SUCCESS;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    pad_x_left = pad_x_right = poolPadXScalar->value->u32;
    pad_y_top = pad_y_bottom = poolPadYScalar->value->u32;

    return status = vxnnePoolingInitializer(node,
                                            "PoolingLayer",
                                            inputs,
                                            poolTypeScalar,
                                            poolSizeXScalar,
                                            poolSizeYScalar,
                                            pad_x_left,
                                            pad_x_right,
                                            pad_y_top,
                                            pad_y_bottom,
                                            roundingScalar,
                                            VX_PAD_CONSTANT,
                                            VX_NULL,
                                            outputs);

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNPoolingLayer2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    if (index != vxmLENGTH_OF(nn_PoolingLayer_params2) - 1) return VX_ERROR_INVALID_PARAMETERS;

    ptr->type                 = VX_TYPE_TENSOR;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  poolTypeScalar             = (vx_scalar)parameters[1];
    vx_scalar  poolSizeXScalar            = (vx_scalar)parameters[2];
    vx_scalar  poolSizeYScalar            = (vx_scalar)parameters[3];
    vx_scalar  poolPadXLeftScalar         = (vx_scalar)parameters[4];
    vx_scalar  poolPadXRightScalar        = (vx_scalar)parameters[5];
    vx_scalar  poolPadYTopScalar          = (vx_scalar)parameters[6];
    vx_scalar  poolPadYBottomScalar       = (vx_scalar)parameters[7];
    vx_scalar  roundingScalar             = (vx_scalar)parameters[8];
    vx_tensor  outputs                    = (vx_tensor)parameters[9];

    vx_uint32 poolPadXLeft                = poolPadXLeftScalar->value->u32;
    vx_uint32 poolPadXRight               = poolPadXRightScalar->value->u32;
    vx_uint32 poolPadYTop                 = poolPadYTopScalar->value->u32;
    vx_uint32 poolPadYBottom              = poolPadYBottomScalar->value->u32;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    return status = vxnnePoolingInitializer(node,
                                            "PoolingLayer2",
                                            inputs,
                                            poolTypeScalar,
                                            poolSizeXScalar,
                                            poolSizeYScalar,
                                            poolPadXLeft,
                                            poolPadXRight,
                                            poolPadYTop,
                                            poolPadYBottom,
                                            roundingScalar,
                                            VX_PAD_CONSTANT,
                                            VX_NULL,
                                            outputs);
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPoolingLayer2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNFullyConnectedLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  weights                    = (vx_tensor)parameters[1];
    vx_tensor  biases                     = (vx_tensor)parameters[2];
    vx_tensor  outputs                    = (vx_tensor)parameters[node->numParameters - 1];

    vx_bool   enable_shader               = vx_false_e;
    vx_bool   supportDataFormat0          = vx_false_e;
    vx_bool   supportDataFormat1          = vx_false_e;
    vx_bool   supportDataFormat2          = vx_false_e;
    vx_bool   supportDataFormat3          = vx_false_e;
    vx_enum   input_dataformat            = TENSOR_DATA_TYPE(inputs);
    vx_enum   weight_dataformat           = TENSOR_DATA_TYPE(weights);
    vx_enum   bias_dataformat             = biases ? TENSOR_DATA_TYPE(biases) : VX_TYPE_INVALID;
    vx_enum   output_dataformat           = TENSOR_DATA_TYPE(outputs);
    vx_uint32 dims                        = TENSOR_VIEW_DIM_NUM(inputs);
    vx_uint32 width                       = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32 height                      = (dims > 1) ? TENSOR_VIEW_SIZE_INDEX(inputs, 1) : 1;
    vx_uint32 depth                       = (dims > 2) ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
    vx_uint32 inputDims                   = dims > 2 ? width * height * depth : width;
    vx_uint32 batch                       = 1;
    vxnne_fully_connected_relu_layer  fullyConnectedLayer = gcvNULL;
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    if (TENSOR_DIM_NUM(inputs) == 2)
    {
        batch = TENSOR_SIZE_INDEX(inputs, 1);
    }
    else if (TENSOR_DIM_NUM(inputs) == 4)
    {
        batch = TENSOR_SIZE_INDEX(inputs, 3);
    }

    if(node->base.context->evisNoInst.supportEVIS)
    {
        supportDataFormat0 = (vx_bool)(input_dataformat == VX_TYPE_FLOAT16 && weight_dataformat == VX_TYPE_FLOAT16 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_FLOAT32) && output_dataformat == VX_TYPE_FLOAT16);
        supportDataFormat1 = (vx_bool)(input_dataformat == VX_TYPE_INT8 && weight_dataformat == VX_TYPE_INT8 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_INT8);
        supportDataFormat2 = (vx_bool)(input_dataformat == VX_TYPE_INT16 && weight_dataformat == VX_TYPE_INT16 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_INT16);
        supportDataFormat3 = (vx_bool)(input_dataformat == VX_TYPE_UINT8 && weight_dataformat == VX_TYPE_UINT8 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat != VX_TYPE_FLOAT32);
        enable_shader      = (supportDataFormat0 || supportDataFormat1 || supportDataFormat2 || supportDataFormat3) && (inputDims < IMG_MAX_WIDTH);
    }
    else
    {
        supportDataFormat0 = (vx_bool)((input_dataformat == VX_TYPE_FLOAT16 && weight_dataformat == VX_TYPE_FLOAT16 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_FLOAT32) && output_dataformat == VX_TYPE_FLOAT16) ||
                                        (input_dataformat == VX_TYPE_FLOAT32 && weight_dataformat == VX_TYPE_FLOAT32 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_FLOAT32) && output_dataformat == VX_TYPE_FLOAT32));
        supportDataFormat3 = (vx_bool)(input_dataformat == VX_TYPE_UINT8 && weight_dataformat == VX_TYPE_UINT8 && (bias_dataformat == VX_TYPE_INVALID || bias_dataformat == VX_TYPE_INT32) && output_dataformat == VX_TYPE_UINT8);
        enable_shader      = (supportDataFormat0 || supportDataFormat3);
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_fully_connected_relu_layer_s), (gctPOINTER*)&fullyConnectedLayer);
    if (!fullyConnectedLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(fullyConnectedLayer, sizeof(vxnne_fully_connected_relu_layer_s));

    vxnneLayer_Initialize(&fullyConnectedLayer->base,
                          "FullyConnectedLayer",
                          node,
                          vxmOPERATION_COUNT(fullyConnectedLayer),
                          fullyConnectedLayer->operations,
                          vxnneLayer_Deinitialize);

    if (enable_shader && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_bool enable_cast_format = vx_false_e;
        vx_tensor input_rs = NULL;
        vx_tensor weights_rs = NULL;

        if ((inputDims % 16 == 0) && input_dataformat == VX_TYPE_UINT8 && weight_dataformat == VX_TYPE_UINT8 && biases
            && node->base.context->evisNoInst.supportEVIS == vx_false_e)
        {
            enable_cast_format = vx_true_e;

            input_rs = vxoTensor_ReformatTensor(inputs, VX_TYPE_UINT32);
            weights_rs = vxoTensor_ReformatTensor(weights, VX_TYPE_UINT32);

            fullyConnectedLayer->base.temp_tensors[0] = input_rs;
            fullyConnectedLayer->base.temp_tensors[1] = weights_rs;

            fullyConnectedLayer->base.num_temp_tensors = 2;
        }
        else
        {
            input_rs = inputs;
            weights_rs = weights;
        }

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetFullyConnectedShaderExecutable(node->base.context, VXNNE_KERNEL_FULLYCONNECTED,
                &node->kernelAttributes.borderMode, input_rs, weights, biases, VX_NN_ACTIVATION_NONE, outputs);
        }
        else
        {
            shaderExecutable = vxnneGetGPUFullyConnectedShaderExecutable(node->base.context, VXNNE_KERNEL_FULLYCONNECTED,
                &node->kernelAttributes.borderMode, enable_cast_format, input_rs, weights_rs, biases, VX_NN_ACTIVATION_NONE, outputs);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&fullyConnectedLayer->fully_connected_SHoperation,
            &fullyConnectedLayer->base,
            VXNNE_OPERATOR_FULLYCONNECTED,
            1,
            shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &fullyConnectedLayer->base,
            &fullyConnectedLayer->fully_connected_SHoperation.base,
            0);

        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_SHoperation.base, (vx_reference)input_rs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_SHoperation.base, (vx_reference)weights_rs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_SHoperation.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        status = vxnneOperation_Initialize(&fullyConnectedLayer->fully_connected_operation.base,
                                           &fullyConnectedLayer->base,
                                           VXNNE_OPERATION_TARGET_SW,
                                           VXNNE_OPERATOR_FULLYCONNECTED,
                                           vxnneExecuteSWFullyConnected,
                                           VX_NULL,
                                           1,
                                           0);

        vxnneLayer_SetOperation(
            &fullyConnectedLayer->base,
            &fullyConnectedLayer->fully_connected_operation.base,
            0);

        fullyConnectedLayer->fully_connected_operation.inputs           = inputs;
        fullyConnectedLayer->fully_connected_operation.weights          = weights;
        fullyConnectedLayer->fully_connected_operation.biases           = biases;
        fullyConnectedLayer->fully_connected_operation.outputs          = outputs;

        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_operation.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&fullyConnectedLayer->fully_connected_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &fullyConnectedLayer->base;
    return status;

exit:
    if (fullyConnectedLayer) gcoOS_Free(gcvNULL, (gctPOINTER)fullyConnectedLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNFullyConnectedLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNActivationLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNActivationLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNActivationLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNActivationLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;
    vx_context context                    = vxGetContext((vx_reference)node);

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  func_s                     = (vx_scalar)parameters[1];
    vx_scalar  a_s                        = (vx_scalar)parameters[2];
    vx_scalar  b_s                        = (vx_scalar)parameters[3];
    vx_tensor  outputs                    = (vx_tensor)parameters[4];
    vx_enum   inputFormat                 = TENSOR_DATA_TYPE(inputs);
    vx_enum   outputFormat                = TENSOR_DATA_TYPE(outputs);
    vx_uint32 batchCount                  = (TENSOR_DIM_NUM(inputs) > 3) ? TENSOR_SIZE_INDEX(inputs, 3) : 1;
    vx_enum   func_v                      = func_s->value->e;
    vx_bool   support_dataType[3]         = {vx_false_e, vx_false_e, vx_false_e};
    vx_bool   enable_tf_quantize          = vx_false_e;
    vx_bool   shExe_flag                  = vx_false_e;
    vx_bool   enable_tensorABS_SHExe      = vx_false_e;
    vx_bool   enable_tensorTR_SHExe       = vx_false_e;
    vx_uint32 input_width                 = TENSOR_SIZE_INDEX(inputs, 0);
    vx_uint32 input_height                = TENSOR_SIZE_INDEX(inputs, 1);
    vx_uint32 img2DSize                   = input_width * input_height;
    vxnne_activation_layer  activationLayer = gcvNULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_activation_layer_s), (gctPOINTER*)&activationLayer);
    if (!activationLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(activationLayer, sizeof(vxnne_activation_layer_s));

    vxnneLayer_Initialize(&activationLayer->base,
                          "ActivationLayer",
                          node,
                          vxmOPERATION_COUNT(activationLayer),
                          activationLayer->operations,
                          vxnneLayer_Deinitialize);

    if(context->evisNoInst.supportEVIS)
    {
        support_dataType[0] = (vx_bool)(((inputFormat == VX_TYPE_INT8 || inputFormat == VX_TYPE_FLOAT16) && (outputFormat == VX_TYPE_INT8 || outputFormat == VX_TYPE_FLOAT16)) || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16));
        support_dataType[1] = (vx_bool)((inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8) || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16));
        support_dataType[2] = (vx_bool)((inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8));

        enable_tensorABS_SHExe = (vx_bool)(inputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32 && func_v == VX_NN_ACTIVATION_ABS);
        enable_tensorTR_SHExe  = (vx_bool)(inputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32 &&
                                           (func_v == VX_NN_ACTIVATION_SQRT  ||
                                            func_v == VX_NN_ACTIVATION_RSQRT ||
                                            /*func_v == VX_NN_ACTIVATION_EXP ||*/
                                            /*func_v == VX_NN_ACTIVATION_LOG ||*/
                                            /*func_v == VX_NN_ACTIVATION_SIN ||*/
                                            func_v == VX_NN_ACTIVATION_SOFTRELU ||
                                            func_v == VX_NN_ACTIVATION_LOGISTIC ||
                                            func_v == VX_NN_ACTIVATION_SQUARE ||
                                            func_v == VX_NN_ACTIVATION_HYPERBOLIC_TAN));
    }
    else
    {
        support_dataType[0] = (vx_bool)((inputFormat == VX_TYPE_FLOAT32 || inputFormat == VX_TYPE_FLOAT16) && (outputFormat == VX_TYPE_FLOAT32 || outputFormat == VX_TYPE_FLOAT16));
        support_dataType[1] = (vx_bool)((inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32) || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16));
        support_dataType[2] = (vx_bool)((inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8));
    }

    enable_tf_quantize = (vx_bool)((func_v == VX_NN_ACTIVATION_RELU && support_dataType[2]) ||
                                    (func_v == VX_NN_ACTIVATION_RELU1 && support_dataType[2]) ||
                                    (func_v == VX_NN_ACTIVATION_BRELU && support_dataType[2]) ||
                                    (func_v == VX_NN_ACTIVATION_RELU6 && support_dataType[2]));

    shExe_flag = (vx_bool)((func_v == VX_NN_ACTIVATION_RELU && support_dataType[0]) ||
                           (func_v == VX_NN_ACTIVATION_RELU1 && support_dataType[1]) ||
                           (func_v == VX_NN_ACTIVATION_RELU6 && support_dataType[1]) ||
                           (func_v == VX_NN_ACTIVATION_LOGISTIC && (support_dataType[1] || support_dataType[2])) ||
                           (func_v == VX_NN_ACTIVATION_HYPERBOLIC_TAN && (support_dataType[1] || support_dataType[2])) ||
                           enable_tensorABS_SHExe ||
                           enable_tensorTR_SHExe  ||
                           enable_tf_quantize);

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_ACTIVATION) &&
        vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        TENSOR_VIEW_SIZE_INDEX(inputs, 3) == TENSOR_VIEW_SIZE_INDEX(outputs, 3) &&
        (TENSOR_VIEW_SIZE_INDEX(outputs, 0) * TENSOR_VIEW_SIZE_INDEX(outputs, 1) * TENSOR_VIEW_SIZE_INDEX(outputs, 2) > 1) &&
        (func_s->value->e == VX_NN_ACTIVATION_LOGISTIC ||
         func_s->value->e == VX_NN_ACTIVATION_RELU  ||
         func_s->value->e == VX_NN_ACTIVATION_RELU1 ||
         func_s->value->e == VX_NN_ACTIVATION_RELU6 ||
         func_s->value->e == VX_NN_ACTIVATION_LEAKYRELU ||
         func_s->value->e == VX_NN_ACTIVATION_HYPERBOLIC_TAN))
    {
        vx_op_param_s conv = {0};

        status = vxnneOperation_Initialize(&activationLayer->activation_tp_operation.base,
                                           &activationLayer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_ACTIVATION,
                                           VX_NULL,
                                           vxnneOperation_TP_Deinitialize,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        conv.data_buff = vxnneAllocateTPLUTBuffer(context, node);
        if (conv.data_buff == VX_NULL)
        {
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.conv_rounding_type = 0;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_ACTIVATION;
        conv.other_ref = gcvNULL;
        conv.tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        conv.tp_value->e32[0] = func_s->value->e;
        conv.tp_value->f32[0] = a_s->value->f32;
        conv.tp_value->f32[1] = b_s->value->f32;

        vxMemCopy(&activationLayer->activation_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_tp_operation.base,
            0);

        activationLayer->activation_tp_operation.input  = inputs;
        activationLayer->activation_tp_operation.output = outputs;

        vxnneOperation_AddReference(&activationLayer->activation_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_tp_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_uint32  reshpTensor_Sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {1};
        vx_uint32  reshpTensor_Dims           = 2;
        vx_tensor input      = NULL;
        vx_tensor output     = NULL;

        vx_float32 minVal = (vx_float32)a_s->value->n32;
        vx_float32 maxVal = (vx_float32)b_s->value->n32;

        vxoElementOptimization_GetTensorShape(inputs, reshpTensor_Sizes, &reshpTensor_Dims);

        input     = vxoTensor_ReshapeTensor(inputs, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);
        output     = vxoTensor_ReshapeTensor(outputs, (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

        if (func_v == VX_NN_ACTIVATION_RELU1)
        {
            minVal = -1;
            maxVal = 1;
        }
        else if (func_v == VX_NN_ACTIVATION_RELU6)
        {
            minVal = 0;
            maxVal = 6;
        }
        else if (func_v == VX_NN_ACTIVATION_RELU)
        {
            minVal = 0;
            maxVal = 32767;
        }
        else if (func_v == VX_NN_ACTIVATION_BRELU)
        {
            maxVal = minVal;
            minVal = 0;
        }

        batchCount = reshpTensor_Dims > 3 ? reshpTensor_Sizes[3] : 1;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            if (enable_tensorABS_SHExe)
                shaderExecutable = vxnneGetTensorAbsShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ABS, &node->kernelAttributes.borderMode, input, output);
            else if (enable_tensorTR_SHExe)
                shaderExecutable = vxnneGetTensorTRShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSCENDENTAL, &node->kernelAttributes.borderMode, input, minVal, maxVal, func_v, output);
            else if (enable_tf_quantize)
                shaderExecutable = vxnneGetActivation_UInt8ShaderExecutable(node->base.context, VXNNE_KERNEL_ACTIVATION_UINT8, &node->kernelAttributes.borderMode, func_v, input, minVal, maxVal, output);
            else
                shaderExecutable = vxnneGetActivationShaderExecutable(node->base.context, VXNNE_KERNEL_ACTIVATION, &node->kernelAttributes.borderMode, func_v, input, minVal, maxVal, output);
        }
        else
        {
            shaderExecutable = vxnneGetGPUActivationShaderExecutable(node->base.context, VXNNE_KERNEL_ACTIVATION, &node->kernelAttributes.borderMode, func_v, input, minVal, maxVal, output);
        }

        if (input) vxoTensor_ReleaseTensor(&input);
        if (output) vxoTensor_ReleaseTensor(&output);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&activationLayer->activation_SHoperation,
            &activationLayer->base,
            VXNNE_OPERATOR_ACTIVATION,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_SHoperation.base,
            0);
    }
    else if(img2DSize < IMG_MAX_WIDTH && func_v == VX_NN_ACTIVATION_SOFTRELU && support_dataType[0] && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        shaderExecutable = vxnneGetActivationSoftReluShaderExecutable(node->base.context, VXNNE_KERNEL_ACTIVATION_SOFTRELU, &node->kernelAttributes.borderMode, inputs, outputs);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&activationLayer->activation_SHoperation,
            &activationLayer->base,
            VXNNE_OPERATOR_ACTIVATION,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_SHoperation.base,
            0);

        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        status = vxnneOperation_Initialize(&activationLayer->activation_operation.base,
                                           &activationLayer->base,
                                           VXNNE_OPERATION_TARGET_SW,
                                           VXNNE_OPERATOR_ACTIVATION,
                                           vxnneExecuteSWActivation,
                                           VX_NULL,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_operation.base,
            0);

        activationLayer->activation_operation.inputs           = inputs;
        activationLayer->activation_operation.func             = func_s;
        activationLayer->activation_operation.a                = a_s;
        activationLayer->activation_operation.b                = b_s;
        activationLayer->activation_operation.outputs          = outputs;

        vxnneOperation_AddReference(&activationLayer->activation_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &activationLayer->base;
    return status;

exit:
    if (activationLayer) gcoOS_Free(gcvNULL, (gctPOINTER)activationLayer);
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNActivationLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                       Leaky Relu
 ***************************************************************************************************************************/

vx_status vxnneExecuteSWLeakyRelu(struct _vxnne_operation_s *operation)
{
    vxnne_activation_sw_operation activationOperation   = (vxnne_activation_sw_operation)operation;

    vx_tensor   inputs            = (vx_tensor)activationOperation->inputs;
    vx_scalar   negative_slopes   = (vx_scalar)activationOperation->a;
    vx_tensor   outputs           = (vx_tensor)activationOperation->outputs;
    vx_float32  negative_slope_v  = negative_slopes->value->f32;
    vx_uint32 elementCount = 0;
    vx_uint32 i;
    vx_float32 result = 0.0f;
    gctPOINTER inputBase;
    gctPOINTER outputBase;
    vx_type_e inputFormat  = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e outputFormat = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_status status = VX_SUCCESS;

    elementCount = (vx_uint32)vxoMemory_ComputeElementCount(&inputs->tensorBuffer->memory, 0);
    vxoTensor_GetTensorViewMemory(inputs, &inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputBase, VX_NULL);

    for (i = 0; i < elementCount; i++)
    {
        vx_float32 data = 0;

        data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), i, (vx_uint8_ptr)inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));

        result = (data > 0.0f) ? data : negative_slope_v * data;

        vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), i, result, (vx_uint8_ptr)outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
    }
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNLeakyReluLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLeakyReluLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLeakyReluLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLeakyReluLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status                     = VX_SUCCESS;
    vx_context context                   = vxGetContext((vx_reference)node);

    vx_tensor inputs                     = (vx_tensor)parameters[0];
    vx_scalar negative_slopes            = (vx_scalar)parameters[1];
    vx_tensor outputs                    = (vx_tensor)parameters[2];
    vx_enum   srcFormat                  = TENSOR_DATA_TYPE(inputs);
    vx_enum   dstFormat                  = TENSOR_DATA_TYPE(outputs);
    vx_uint32 batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_bool   shExe_flag                 = vx_false_e;

    vxnne_activation_layer  activationLayer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_activation_layer_s), (gctPOINTER*)&activationLayer);
    if (!activationLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(activationLayer, sizeof(vxnne_activation_layer_s));

    vxnneLayer_Initialize(&activationLayer->base,
                          "LeakyReluLayer",
                          node,
                          vxmOPERATION_COUNT(activationLayer),
                          activationLayer->operations,
                          vxnneLayer_Deinitialize);

    shExe_flag  = (vx_bool)((srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8)
                          || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_INT8)
                          || (srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_INT16)
                          || (srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_FLOAT16 && dstFormat != VX_TYPE_FLOAT32));

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_ACTIVATION) &&
        vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        (TENSOR_VIEW_SIZE_INDEX(outputs, 0) * TENSOR_VIEW_SIZE_INDEX(outputs, 1) * TENSOR_VIEW_SIZE_INDEX(outputs, 2) > 1))
    {
        vx_op_param_s conv = {0};

        status = vxnneOperation_Initialize(&activationLayer->activation_tp_operation.base,
                                           &activationLayer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_ACTIVATION,
                                           VX_NULL,
                                           vxnneOperation_TP_Deinitialize,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        conv.data_buff = vxnneAllocateTPLUTBuffer(context, node);
        if (conv.data_buff == VX_NULL)
        {
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.conv_rounding_type = 0;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_ACTIVATION;
        conv.other_ref = gcvNULL;
        conv.tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        conv.tp_value->e32[0] = VX_NN_ACTIVATION_LEAKYRELU;
        conv.tp_value->f32[0] = negative_slopes->value->f32;

        vxMemCopy(&activationLayer->activation_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_tp_operation.base,
            0);

        activationLayer->activation_tp_operation.input  = inputs;
        activationLayer->activation_tp_operation.output = outputs;

        vxnneOperation_AddReference(&activationLayer->activation_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_tp_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        shaderExecutable = vxnneGetLeakyReluShaderExecutable(node->base.context, VXNNE_KERNEL_NN_LEAKY, &node->kernelAttributes.borderMode, inputs, negative_slopes, outputs);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&activationLayer->activation_SHoperation,
                                        &activationLayer->base,
                                        VXNNE_OPERATOR_ACTIVATION,
                                        batchCount,
                                        shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)negative_slopes, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_SHoperation.base,
            0);
    }
    else
    {
        status = vxnneOperation_Initialize(&activationLayer->activation_operation.base,
                                           &activationLayer->base,
                                           VXNNE_OPERATION_TARGET_SW,
                                           VXNNE_OPERATOR_ACTIVATION,
                                           vxnneExecuteSWLeakyRelu,
                                           VX_NULL,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &activationLayer->base,
            &activationLayer->activation_operation.base,
            0);

        activationLayer->activation_operation.inputs           = inputs;
        activationLayer->activation_operation.func             = VX_NULL;
        activationLayer->activation_operation.a                = negative_slopes;
        activationLayer->activation_operation.b                = VX_NULL;
        activationLayer->activation_operation.outputs          = outputs;

        vxnneOperation_AddReference(&activationLayer->activation_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&activationLayer->activation_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &activationLayer->base;
    return status;

exit:
    if (activationLayer) gcoOS_Free(gcvNULL, (gctPOINTER)activationLayer);
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLeakyReluLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                       PRelu
 ***************************************************************************************************************************/

vx_status vxnneExecuteSWPRelu(struct _vxnne_operation_s *operation)
{
    vxnne_prelu_sw_operation preluOperation   = (vxnne_prelu_sw_operation)operation;

    vx_tensor   inputs            = (vx_tensor)preluOperation->inputs;
    vx_tensor   alpha             = (vx_tensor)preluOperation->alpha;
    vx_tensor   outputs           = (vx_tensor)preluOperation->outputs;
    vx_int32    inputZP           = TENSOR_TF_ZEROPOINT(inputs);
    vx_int8     inputFP           = TENSOR_POS(inputs);
    vx_float32  inputScale        = TENSOR_TF_SCALE(inputs);
    vx_int32    alphaZP           = TENSOR_TF_ZEROPOINT(alpha);
    vx_int8     alphaFP           = TENSOR_POS(alpha);
    vx_float32  alphaScale        = TENSOR_TF_SCALE(alpha);
    vx_int32    outputZP          = TENSOR_TF_ZEROPOINT(outputs);
    vx_float32  outputScale       = TENSOR_TF_SCALE(outputs);
    vx_int8     outputFP          = TENSOR_POS(outputs);

    vx_uint32   width             = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32   height            = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
    vx_uint32   channel           = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
    vx_uint32   spatial           = width * height;

    vx_uint32   i, c;
    vx_float32  result            = 0.0f;
    gctPOINTER  inputBase;
    gctPOINTER  alphaBase;
    gctPOINTER  outputBase;
    vx_type_e   inputFormat       = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_enum     inputQuantFormat  = TENSOR_QUANT_TYPE(inputs);
    vx_type_e   alphaFormat       = (vx_type_e)TENSOR_DATA_TYPE(alpha);
    vx_enum     alphaQuantFormat  = TENSOR_QUANT_TYPE(alpha);
    vx_type_e   outputFormat      = (vx_type_e)TENSOR_DATA_TYPE(outputs);
    vx_enum     outputQuantFormat = TENSOR_QUANT_TYPE(outputs);
    vx_enum     outputRound       = TENSOR_ROUNDING_MODE(outputs);

    vx_status   status            = VX_SUCCESS;

    vxoTensor_GetTensorViewMemory(inputs, &inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(alpha, &alphaBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputBase, VX_NULL);

    for (c = 0; c < channel; c++)
    {
        vx_float32 aV = vxnneGetDataExt(alphaFormat, alphaQuantFormat, c, (vx_uint8_ptr)alphaBase, alphaFP, alphaZP, alphaScale);
        for (i = 0; i < spatial; i++)
        {
            vx_uint32 index = c * spatial + i;
            vx_float32 in = vxnneGetDataExt(inputFormat, inputQuantFormat, index, (vx_uint8_ptr)inputBase, inputFP, inputZP, inputScale);

            result = (in < 0) ? in * aV : in;

            status = vxnneSaveDataExt(outputFormat, outputQuantFormat, index, result, (vx_uint8_ptr)outputBase, outputFP, outputZP, outputScale, outputRound);
        }
    }

    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNPReluLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPReluLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPReluLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPReluLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status                     = VX_SUCCESS;

    vx_tensor inputs                     = (vx_tensor)parameters[0];
    vx_tensor alpha                      = (vx_tensor)parameters[1];
    vx_tensor outputs                    = (vx_tensor)parameters[2];
    vx_uint32 batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_enum   srcFormat                  = TENSOR_DATA_TYPE(inputs);
    vx_enum   alphaFormat                = TENSOR_DATA_TYPE(alpha);
    vx_enum   dstFormat                  = TENSOR_DATA_TYPE(inputs);
    vx_bool   shExe_flag                 = vx_false_e;

    vxnne_prelu_layer  pReluLayer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_prelu_layer_s), (gctPOINTER*)&pReluLayer);
    if (!pReluLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(pReluLayer, sizeof(vxnne_prelu_layer_s));

    vxnneLayer_Initialize(&pReluLayer->base,
                          "PReluLayer",
                          node,
                          vxmOPERATION_COUNT(pReluLayer),
                          pReluLayer->operations,
                          vxnneLayer_Deinitialize);

    shExe_flag  = (vx_bool)(((srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8)
                          || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_INT8)
                          || (srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_INT16)
                          || (srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_FLOAT16)
                          || (srcFormat == VX_TYPE_FLOAT16 && dstFormat != VX_TYPE_FLOAT32))
                         && alphaFormat == VX_TYPE_FLOAT16);

    if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        shaderExecutable = vxnneGetPReluShaderExecutable(node->base.context, VXNNE_KERNEL_PRELU, &node->kernelAttributes.borderMode, inputs, alpha, outputs);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&pReluLayer->prelu_sh_operation,
                                        &pReluLayer->base,
                                        VXNNE_OPERATOR_PRELU,
                                        batchCount,
                                        shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneOperation_AddReference(&pReluLayer->prelu_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&pReluLayer->prelu_sh_operation.base, (vx_reference)alpha, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&pReluLayer->prelu_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &pReluLayer->base,
            &pReluLayer->prelu_sh_operation.base,
            0);

        if (batchCount > 1)
        {
            vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        }
    }
    else
    {
        status = vxnneOperation_Initialize(&pReluLayer->prelu_operation.base,
                                           &pReluLayer->base,
                                           VXNNE_OPERATION_TARGET_SW,
                                           VXNNE_OPERATOR_PRELU,
                                           vxnneExecuteSWPRelu,
                                           VX_NULL,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &pReluLayer->base,
            &pReluLayer->prelu_operation.base,
            0);

        pReluLayer->prelu_operation.inputs           = inputs;
        pReluLayer->prelu_operation.alpha            = alpha;
        pReluLayer->prelu_operation.outputs          = outputs;

        vxnneOperation_AddReference(&pReluLayer->prelu_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&pReluLayer->prelu_operation.base, (vx_reference)alpha, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&pReluLayer->prelu_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &pReluLayer->base;
    return status;

exit:
    if (pReluLayer) gcoOS_Free(gcvNULL, (gctPOINTER)pReluLayer);
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNPReluLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                       Batch Normalization
 ***************************************************************************************************************************/

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNBatchNormalizationLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNBatchNormalizationLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNBatchNormalizationLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

vx_status vxnneExecuteSWBatchNormPreProcess(vx_tensor means, vx_tensor variances, vx_tensor gamma, vx_tensor beta, vx_float32 eps, vx_float32 inputScale, vx_float32 outputScale, vx_int32 output_ZP, vx_tensor weights, vx_tensor biases)
{
    vx_status       status          = VX_SUCCESS;
    vx_uint8_ptr    weightsLogic    = NULL;
    vx_uint8_ptr    biasesLogic     = NULL;
    vx_uint8_ptr    meanLogic       = NULL;
    vx_uint8_ptr    varianceLogic   = NULL;
    vx_uint8_ptr    gammaLogic      = NULL;
    vx_uint8_ptr    betaLogic       = NULL;
    vx_float32_ptr  weightsF32Ptr   = NULL;
    vx_float32_ptr  biasesF32Ptr    = NULL;
    vx_type_e       meanFormat      = (vx_type_e)(TENSOR_DATA_TYPE(means));
    vx_type_e       varianceFormat  = (vx_type_e)(TENSOR_DATA_TYPE(variances));
    vx_type_e       gammaFormat     = (vx_type_e)(TENSOR_DATA_TYPE(gamma));
    vx_type_e       betaFormat      = (vx_type_e)(TENSOR_DATA_TYPE(beta));
    vx_uint32       output_size     = TENSOR_VIEW_SIZE_INDEX(weights, 0) * TENSOR_VIEW_SIZE_INDEX(weights, 1) >> 1;
    vx_uint32       i               = 0;
    vx_float32      meanf           = 0;
    vx_float32      variancef       = 0;
    vx_float32      gammaf          = 0;
    vx_float32      betaf           = 0;
    vx_float32      weightf         = 0;
    vx_float32      biasf           = 0;

    vxoTensor_GetTensorViewMemory(weights, (gctPOINTER*)&weightsLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(biases, (gctPOINTER*)&biasesLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(means, (gctPOINTER*)&meanLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(variances, (gctPOINTER*)&varianceLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(gamma, (gctPOINTER*)&gammaLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(beta, (gctPOINTER*)&betaLogic, VX_NULL);

    weightsF32Ptr = (vx_float32_ptr)weightsLogic;
    biasesF32Ptr  = (vx_float32_ptr)biasesLogic;
    for (i = 0; i < output_size; i ++)
    {

        meanf     = vxnneGetDataExt(meanFormat, TENSOR_QUANT_TYPE(means), i, meanLogic, TENSOR_POS(means), TENSOR_TF_ZEROPOINT(means), TENSOR_TF_SCALE(means));
        variancef = vxnneGetDataExt(varianceFormat, TENSOR_QUANT_TYPE(variances), i, varianceLogic, TENSOR_POS(variances), TENSOR_TF_ZEROPOINT(variances), TENSOR_TF_SCALE(variances));
        gammaf    = vxnneGetDataExt(gammaFormat, TENSOR_QUANT_TYPE(gamma), i, gammaLogic, TENSOR_POS(gamma), TENSOR_TF_ZEROPOINT(gamma), TENSOR_TF_SCALE(gamma));
        betaf     = vxnneGetDataExt(betaFormat, TENSOR_QUANT_TYPE(beta), i, betaLogic, TENSOR_POS(beta), TENSOR_TF_ZEROPOINT(beta), TENSOR_TF_SCALE(beta));

        weightf      = gammaf / sqrtf(variancef + eps);
        biasf        = betaf - meanf * weightf;

        weightf      = weightf * inputScale * outputScale;
        biasf        = biasf * outputScale + output_ZP;

        weightsF32Ptr[i] = weightf;
        biasesF32Ptr[i]  = biasf;
    }

    return status;
}

vx_status vxnneExecuteSWBatchNormalization(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input,output;
    vx_tensor means,variances,gamma,beta;
    vx_scalar epss;
    vx_uint8_ptr inputLogic, outputLogic;
    vx_uint8_ptr meanLogic, varianceLogic, gammaLogic, betaLogic;

    vx_uint32  width, height, channel;
    vx_uint32  c, i, spatial;
    vx_float32 meanf, variancef, gammaf, betaf;
    vx_float32 inputf, outputf;
    vx_type_e  inFormat, outFormat, meanFormat, varianceFormat, gammaFormat, betaFormat;
    vx_float32 eps;
    vx_float32 normalize;
    vx_uint32  index;

    vxnne_batchnorm_sw_operation activationOperation = (vxnne_batchnorm_sw_operation)operation;
    means       = (vx_tensor)activationOperation->mean;
    variances   = (vx_tensor)activationOperation->variance;
    gamma       = (vx_tensor)activationOperation->gamma;
    beta        = (vx_tensor)activationOperation->beta;

    input       = (vx_tensor)activationOperation->input;
    output      = (vx_tensor)activationOperation->output;

    epss        = (vx_scalar)activationOperation->eps;
    eps         = epss->value->f32;

    width       = TENSOR_VIEW_SIZE_INDEX(input, 0);
    height      = TENSOR_VIEW_SIZE_INDEX(input, 1);
    channel     = TENSOR_VIEW_SIZE_INDEX(input, 2);
    spatial     = width * height;

    inFormat        = (vx_type_e)(TENSOR_DATA_TYPE(input));
    outFormat       = (vx_type_e)(TENSOR_DATA_TYPE(output));
    meanFormat      = (vx_type_e)(TENSOR_DATA_TYPE(means));
    varianceFormat  = (vx_type_e)(TENSOR_DATA_TYPE(variances));
    gammaFormat     = (vx_type_e)(TENSOR_DATA_TYPE(gamma));
    betaFormat      = (vx_type_e)(TENSOR_DATA_TYPE(beta));

    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&inputLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&outputLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(means, (gctPOINTER*)&meanLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(variances, (gctPOINTER*)&varianceLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(gamma, (gctPOINTER*)&gammaLogic, VX_NULL);
    vxoTensor_GetTensorViewMemory(beta, (gctPOINTER*)&betaLogic, VX_NULL);

    for(c = 0; c < channel; c ++)
    {
        meanf     = vxnneGetDataExt(meanFormat, TENSOR_QUANT_TYPE(means), c, meanLogic, TENSOR_POS(means), TENSOR_TF_ZEROPOINT(means), TENSOR_TF_SCALE(means));
        variancef = vxnneGetDataExt(varianceFormat, TENSOR_QUANT_TYPE(variances), c, varianceLogic, TENSOR_POS(variances), TENSOR_TF_ZEROPOINT(variances), TENSOR_TF_SCALE(variances));
        gammaf    = vxnneGetDataExt(gammaFormat, TENSOR_QUANT_TYPE(gamma), c, gammaLogic, TENSOR_POS(gamma), TENSOR_TF_ZEROPOINT(gamma), TENSOR_TF_SCALE(gamma));
        betaf     = vxnneGetDataExt(betaFormat, TENSOR_QUANT_TYPE(beta), c, betaLogic, TENSOR_POS(beta), TENSOR_TF_ZEROPOINT(beta), TENSOR_TF_SCALE(beta));

        for(i = 0; i < spatial; i ++)
        {
            index       = c * spatial + i;
            inputf  = vxnneGetDataExt(inFormat, TENSOR_QUANT_TYPE(input), index, inputLogic, TENSOR_POS(input), TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input));
            /* Compute Normalize */
            normalize   = (inputf - meanf)/sqrtf(variancef + eps);
            /* Scale and Shift */
            outputf     = gammaf * normalize + betaf;
            vxnneSaveDataExt(outFormat, TENSOR_QUANT_TYPE(output), index, outputf, outputLogic, TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
        }
    }

    return status;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoNNBatchNormalizationLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_scalar  epss                       = (vx_scalar)parameters[0];
    vx_tensor  means                      = (vx_tensor)parameters[1];
    vx_tensor  variances                  = (vx_tensor)parameters[2];
    vx_tensor  gamma                      = (vx_tensor)parameters[3];
    vx_tensor  beta                       = (vx_tensor)parameters[4];
    vx_tensor  input                      = (vx_tensor)parameters[5];
    vx_tensor  output                     = (vx_tensor)parameters[6];
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(input);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(output);
    vx_bool   shExe_flag                  = vx_false_e;
    vx_tensor weights                     = NULL;
    vx_tensor biases                      = NULL;
    vx_tensor_create_params_t tensor_create_params;
    vx_uint32 batchCount                  = TENSOR_SIZE_INDEX(input, 3);

    vxnne_batchnorm_layer  batchnormLayer = NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_batchnorm_layer_s), (gctPOINTER*)&batchnormLayer);
    if (!batchnormLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(batchnormLayer, sizeof(vxnne_batchnorm_layer_s));

    vxnneLayer_Initialize(&batchnormLayer->base,
                          "BatchNormalizationLayer",
                          node,
                          vxmOPERATION_COUNT(batchnormLayer),
                          batchnormLayer->operations,
                          vxnneLayer_Deinitialize);

    shExe_flag = (vx_bool)((inputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32));

    if (shExe_flag && vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_uint32  sizes[]          = {1, 1, 1, 1};
        vx_uint32  dims             = 2;
        vx_float32 inputScale       = 1.0f;
        vx_float32 outputScale      = 1.0f;
        vx_int32   output_ZP        = 0;
        vx_int8    srcFixPointPos   = TENSOR_POS(input);
        vx_int8    dstFixPointPos   = TENSOR_POS(output);
        vx_uint32  axis             = 2;

        if (TENSOR_DIM_NUM(input) < 3)
            axis = 0;

        sizes[0]            = TENSOR_VIEW_SIZE_INDEX(input, axis) * 2;
        sizes[1]            = 1;

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = VX_TYPE_INT16;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;
        tensor_create_params.quant_data.dfp.fixed_point_pos = 0;

        weights = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
        if (vxoTensor_AllocateMemory(weights) != VX_SUCCESS)
        {
            vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        biases = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
        if (vxoTensor_AllocateMemory(biases) != VX_SUCCESS)
        {
            vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        if ((inputFormat == VX_TYPE_INT8 || inputFormat == VX_TYPE_INT16) && TENSOR_QUANT_TYPE(input) == VX_QUANT_DYNAMIC_FIXED_POINT)
        {
            if (srcFixPointPos >= 0)
            {
                inputScale = 1.0f / (vx_float32) (1 << srcFixPointPos);
            }
            else if (srcFixPointPos < 0)
            {
                inputScale = (vx_float32) (1 << -srcFixPointPos);
            }
        }
        else if (inputFormat == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(input) == VX_QUANT_AFFINE_SCALE)
        {
            inputScale = TENSOR_TF_SCALE(input);
        }

        if ((outputFormat == VX_TYPE_INT8 || outputFormat == VX_TYPE_INT16) && TENSOR_QUANT_TYPE(output) == VX_QUANT_DYNAMIC_FIXED_POINT)
        {
            if (dstFixPointPos >= 0)
            {
                outputScale = (vx_float32) (1 << dstFixPointPos);
            }
            else if (dstFixPointPos < 0)
            {
                outputScale = 1.0f / (vx_float32) (1 << -dstFixPointPos);
            }
        }
        else if (outputFormat == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(output) == VX_QUANT_AFFINE_SCALE)
        {
            outputScale     = 1.0f / TENSOR_TF_SCALE(output);
            output_ZP       = TENSOR_TF_ZEROPOINT(output);
        }

        vxnneExecuteSWBatchNormPreProcess(means, variances, gamma, beta, epss->value->f32, inputScale, outputScale, output_ZP, weights, biases);

        shaderExecutable = vxnneGetBatchNormShaderExecutable(node->base.context, VXNNE_KERNEL_BATCHNORM, &node->kernelAttributes.borderMode, axis,
                                                              input, weights, biases, output);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&batchnormLayer->batchnorm_sh_operation,
                                        &batchnormLayer->base,
                                        VXNNE_OPERATOR_BATCHNORM,
                                        batchCount,
                                        shaderExecutable);

        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &batchnormLayer->base,
            &batchnormLayer->batchnorm_sh_operation.base,
            0);

        vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 2, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 3, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);

        vxnneOperation_AddReference(&batchnormLayer->batchnorm_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&batchnormLayer->batchnorm_sh_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&batchnormLayer->batchnorm_sh_operation.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&batchnormLayer->batchnorm_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        batchnormLayer->base.num_temp_tensors = 2;
        batchnormLayer->base.temp_tensors[0] = weights;
        batchnormLayer->base.temp_tensors[1] = biases;
    }
    else
    {
        status = vxnneOperation_Initialize(&batchnormLayer->batchnorm_operation.base,
                                           &batchnormLayer->base,
                                           VXNNE_OPERATION_TARGET_SW,
                                           VXNNE_OPERATOR_BATCHNORM,
                                           vxnneExecuteSWBatchNormalization,
                                           VX_NULL,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &batchnormLayer->base,
            &batchnormLayer->batchnorm_operation.base,
            0);

        batchnormLayer->batchnorm_operation.eps              = epss;
        batchnormLayer->batchnorm_operation.mean             = means;
        batchnormLayer->batchnorm_operation.variance         = variances;
        batchnormLayer->batchnorm_operation.gamma            = gamma;
        batchnormLayer->batchnorm_operation.beta             = beta;
        batchnormLayer->batchnorm_operation.input            = input;
        batchnormLayer->batchnorm_operation.output           = output;

        vxnneOperation_AddReference(&batchnormLayer->batchnorm_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&batchnormLayer->batchnorm_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &batchnormLayer->base;
    return status;

exit:
    if (batchnormLayer) gcoOS_Free(gcvNULL, (gctPOINTER)batchnormLayer);
    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNBatchNormalizationLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWEltwise(struct _vxnne_operation_s *operation)
{
    vxnne_eltwise_sw_operation eltwiseOperation   = (vxnne_eltwise_sw_operation)operation;

    vx_tensor input1 = eltwiseOperation->input1;
    vx_tensor input2 = eltwiseOperation->input2;
    vx_tensor output = eltwiseOperation->output;

    vx_enum kernel = eltwiseOperation->kernel;
    vx_int32 dim1 = input1->viewRegion.dimCount;
    vx_int32 dim2 = input2->viewRegion.dimCount;
    vx_enum overflow = eltwiseOperation->overflow->value->e;

    if (dim1 == dim2)
    {
        switch(kernel)
        {
        case VX_KERNEL_TENSOR_ADD:
            eltwise(input1, input2, 1.f, overflow, VX_ROUND_POLICY_TO_ZERO, VX_TENSOR_OP_ADD, output);
            break;
        case VX_KERNEL_TENSOR_SUBTRACT:
            eltwise(input1, input2, 1.f, overflow, VX_ROUND_POLICY_TO_ZERO, VX_TENSOR_OP_SUB, output);
            break;
        case VX_KERNEL_TENSOR_MULTIPLY:
            {
                vx_enum rounding = eltwiseOperation->rounding->value->e;
                vx_float32 scale = eltwiseOperation->scale->value->f32;
                eltwise(input1, input2, scale, overflow, rounding, VX_TENSOR_OP_MUL, output);
            }
            break;
        default:
            vxError("Not support kenrel: %d\n", kernel);
            break;
        }
    }
    else
        vxError("Difference dim\n");
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorEltwise(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorEltwise_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorEltwise_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorEltwise_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
   vx_status  status  = VX_SUCCESS;

    vx_tensor input1   = (vx_tensor)parameters[0];
    vx_tensor input2   = (vx_tensor)parameters[1];
    vx_scalar scale = NULL;
    vx_scalar overflow = (vx_scalar)parameters[2];
    vx_scalar rounding = NULL;
    vx_tensor output   = (vx_tensor)parameters[3];
    vx_enum kernel     = node->kernel->enumeration;
    vxnne_eltwise_layer eltwiseLayer = VX_NULL;
    vxnne_eltwise_sw_operation_s * operation = VX_NULL;

    if (kernel == VX_KERNEL_TENSOR_MULTIPLY)
    {
        scale = (vx_scalar)parameters[2];
        overflow = (vx_scalar)parameters[3];
        rounding = (vx_scalar)parameters[4];
        output   = (vx_tensor)parameters[5];
    }

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_eltwise_layer_s), (gctPOINTER*)&eltwiseLayer);
    if (!eltwiseLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(eltwiseLayer, sizeof(vxnne_eltwise_layer_s));

    vxnneLayer_Initialize(&eltwiseLayer->base,
                          "eltwiseLayer",
                          node,
                          vxmOPERATION_COUNT(eltwiseLayer),
                          eltwiseLayer->operations,
                          vxnneLayer_Deinitialize);

    operation = &eltwiseLayer->eltwise_operation;

    status = vxnneOperation_Initialize(&eltwiseLayer->eltwise_operation.base,
                                       &eltwiseLayer->base,
                                       VXNNE_OPERATION_TARGET_SW,
                                       VXNNE_OPERATOR_ACTIVATION,
                                       vxnneExecuteSWEltwise,
                                       VX_NULL,
                                       TENSOR_SIZE_INDEX(input1, 3),
                                       0);

    vxnneLayer_SetOperation(
        &eltwiseLayer->base,
        &eltwiseLayer->eltwise_operation.base,
        0);

    operation->kernel           = node->kernel->enumeration;
    operation->input1           = input1;
    operation->input2           = input2;
    operation->scale            = scale;
    operation->overflow         = overflow;
    operation->rounding         = rounding;
    operation->output           = output;

    vxnneOperation_AddReference(&eltwiseLayer->eltwise_operation.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&eltwiseLayer->eltwise_operation.base, (vx_reference)input2, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&eltwiseLayer->eltwise_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

    node->layer = &eltwiseLayer->base;

exit:
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorEltwise_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorAdd(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorAdd_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorAdd_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_uint32 getTensorOffset(vx_uint32 index, vx_tensor inputTensor, vx_uint32 * out_dims)
{
    vx_uint32 offset = 0;
    vx_uint32 i;

    for(i = 0; i < inputTensor->dimCount; i++)
    {
        offset += inputTensor->strides[i] * (index % out_dims[i]);

        index /= out_dims[i];
    }

    return offset;
}

VX_PRIVATE_API vx_uint32 getExpandTensorOffset(vx_uint32 index, vx_tensor inputTensor, vx_uint32 * out_dims)
{
    vx_uint32 offset = 0;
    vx_uint32 i;

    for(i = 0; i < inputTensor->dimCount; i++)
    {
        if(inputTensor->dims[i] == out_dims[i])
            offset += inputTensor->strides[i] * (index % out_dims[i]);

        index /= out_dims[i];
    }

    return offset;
}

vx_status vxnneExecuteSWTensorAdd(vxnne_operation operation)
{
    vxnne_tensor_add_operation eltwiseOperation   = (vxnne_tensor_add_operation)operation;

    vx_tensor input1 = eltwiseOperation->input0;
    vx_tensor input2 = eltwiseOperation->input1;
    vx_tensor output = eltwiseOperation->output;
    vx_uint32 dims   = TENSOR_DIM_NUM(output);

    {
        vx_uint8_ptr input1base, input2base, outputbase;
        vx_uint32 i, numCount = 1;

        vxoTensor_GetTensorViewMemory(input1, (gctPOINTER*)&input1base, VX_NULL);
        vxoTensor_GetTensorViewMemory(input2, (gctPOINTER*)&input2base, VX_NULL);
        vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&outputbase, VX_NULL);

        for (i = 0; i < dims; i++)
        {
            numCount *= output->dims[i];
        }

        for (i = 0; i < numCount; i++)
        {
            vx_uint32 in1offset, in2offset, outoffset;
            vx_int8_ptr in1, in2, out;
            vx_status status = VX_SUCCESS;
            vx_float32 in1Data_fl32, in2Data_fl32;

            in1offset = getExpandTensorOffset(i, input1, output->dims);
            in2offset = getExpandTensorOffset(i, input2, output->dims);
            outoffset = getTensorOffset(i, output, output->dims);

            in1 = (vx_int8_ptr)input1base + in1offset;
            in2 = (vx_int8_ptr)input2base + in2offset;
            out = (vx_int8_ptr)outputbase + outoffset;

            in1Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input1), TENSOR_QUANT_TYPE(input1), 0, (vx_uint8_ptr)in1,
                TENSOR_POS(input1), TENSOR_TF_ZEROPOINT(input1), TENSOR_TF_SCALE(input1));
            in2Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input2), TENSOR_QUANT_TYPE(input2), 0, (vx_uint8_ptr)in2,
                TENSOR_POS(input2), TENSOR_TF_ZEROPOINT(input2), TENSOR_TF_SCALE(input2));
            status = vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), 0, in1Data_fl32 + in2Data_fl32, (vx_uint8_ptr)out,
                TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
        }
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorAdd_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input0   = (vx_tensor)parameters[0];
    vx_tensor input1   = (vx_tensor)parameters[1];
    vx_scalar policy   = (vx_scalar)parameters[2];
    vx_tensor output   = (vx_tensor)parameters[3];
    vx_context context = vxGetContext((vx_reference)node);

    vx_type_e input0Format          = TENSOR_DATA_TYPE(input0);
    vx_type_e input1Format          = TENSOR_DATA_TYPE(input1);
    vx_type_e outputFormat          = TENSOR_DATA_TYPE(output);

    vx_bool   format_flag           = vx_false_e;
    vx_bool   shExe_flag            = vx_true_e;
    vx_bool   swExe_flag            = vx_false_e;
    vx_bool   enable_2d_tensor      = vx_false_e;
    vx_enum   policyEnum            = policy->value->e;
    vx_uint32 depth                 = (TENSOR_DIM_NUM(output) > 2) ? TENSOR_VIEW_SIZE_INDEX(output, 2) : 1;
    vx_uint32 batchCount0           = (TENSOR_SIZE_INDEX(input0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input0, 3);
    vx_uint32 batchCount1           = (TENSOR_SIZE_INDEX(input1, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input1, 3);
    vx_uint32 batchCount            = (TENSOR_SIZE_INDEX(output, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(output, 3);

    vxnne_tensor_add_layer tensor_add_layer = VX_NULL;
    swExe_flag = (TENSOR_DIM_NUM(output) > 4);
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_add_layer_s), (gctPOINTER*)&tensor_add_layer);
    if (!tensor_add_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_add_layer, sizeof(vxnne_tensor_add_layer_s));

    vxnneLayer_Initialize(&tensor_add_layer->base,
                          "TensorAdd",
                          node,
                          vxmOPERATION_COUNT(tensor_add_layer),
                          tensor_add_layer->operations,
                          VX_NULL);

    if(context->evisNoInst.supportEVIS)
    {
        format_flag = (vx_bool)((input0Format != VX_TYPE_FLOAT32) && (input1Format != VX_TYPE_FLOAT32) && (outputFormat != VX_TYPE_FLOAT32));
        enable_2d_tensor = (vx_bool)(depth == 1 && ((input0Format == VX_TYPE_FLOAT16 && (input1Format == VX_TYPE_FLOAT16 || input1Format == VX_TYPE_FLOAT32) && outputFormat == VX_TYPE_FLOAT16) || format_flag) && policyEnum == VX_CONVERT_POLICY_SATURATE);
    }
    else
    {
        format_flag = vx_true_e;
    }

    shExe_flag = format_flag  || enable_2d_tensor;

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_ADD) &&
        vxnneIsTPSupportFormat(context, input0, VX_NULL, output) &&
        (input0Format == input1Format) &&
        TENSOR_POS(input0) == TENSOR_POS(input1) &&
        TENSOR_QUANT_TYPE(input0) == TENSOR_QUANT_TYPE(input1) &&
        TENSOR_TF_SCALE(input0) == TENSOR_TF_SCALE(input1) &&
        TENSOR_TF_ZEROPOINT(input0) == TENSOR_TF_ZEROPOINT(input1) &&
        !swExe_flag)
    {
        vx_op_param_s conv = {0};

        status = vxnneOperation_Initialize(&tensor_add_layer->tensorAddTP.base,
                                           &tensor_add_layer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_TENSOR_ADD,
                                           VX_NULL,
                                           VX_NULL,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        vxnneLayer_SetOperation(
            &tensor_add_layer->base,
            &tensor_add_layer->tensorAddTP.base,
            0);

        tensor_add_layer->tensorAddTP.input    = input0;
        tensor_add_layer->tensorAddTP.input_ex = input1;
        tensor_add_layer->tensorAddTP.output   = output;

        memset(&conv, 0, sizeof(vx_op_param_s));
        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.conv_rounding_type = 0;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_ADD;
        conv.other_ref = (vx_reference)input1;
        conv.data_buff = gcvNULL;

        memcpy(&tensor_add_layer->tensorAddTP.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneOperation_AddReference(&tensor_add_layer->tensorAddTP.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddTP.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddTP.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) &&  !swExe_flag)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            if (enable_2d_tensor)
            {
                shaderExecutable = vxnneGetTensor2DAddShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_2D_ADD, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_ADD, output);
            }
            else
            {
                if (input0Format != input1Format && input0Format == VX_TYPE_FLOAT16)
                    shaderExecutable = vxnneGetTensorAddShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ELTWISE, &node->kernelAttributes.borderMode, input1, input0, NULL, policy, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_ADD, output);
                else
                    shaderExecutable = vxnneGetTensorAddShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ELTWISE, &node->kernelAttributes.borderMode, input0, input1, NULL, policy, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_ADD, output);
            }

        }
        else
        {
                shaderExecutable = vxnneGetGPUTensorElewiseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ELTWISE, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_ADD, output);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&tensor_add_layer->tensorAddSH,
                                        &tensor_add_layer->base,
                                        VXNNE_OPERATOR_TENSOR_ADD,
                                        batchCount,
                                        shaderExecutable);
        if (status != VX_SUCCESS)
            goto exit;

        if (batchCount != 1 && batchCount0 != batchCount1)
        {
            vx_tensor src0 = (vx_tensor)shaderExecutable->param[0];
            vx_uint32 batch0 = (TENSOR_SIZE_INDEX(src0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(src0, 3);

            if (batch0 == 1)
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 0, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
            else
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        }

        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSH.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSH.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSH.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &tensor_add_layer->base,
            &tensor_add_layer->tensorAddSH.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&tensor_add_layer->tensorAddSW.base,
                                &tensor_add_layer->base,
                                VXNNE_OPERATION_TARGET_SW,
                                VXNNE_OPERATOR_TENSOR_ADD,
                                /*vxnneExecuteSWEltwise,*/
                                vxnneExecuteSWTensorAdd,
                                VX_NULL,
                                batchCount,
                                0);

        vxnneLayer_SetOperation(
            &tensor_add_layer->base,
            &tensor_add_layer->tensorAddSW.base,
            0);

        tensor_add_layer->tensorAddSW.input0    = input0;
        tensor_add_layer->tensorAddSW.input1    = input1;
        tensor_add_layer->tensorAddSW.policy    = policy;
        tensor_add_layer->tensorAddSW.output    = output;

        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSW.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSW.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_add_layer->tensorAddSW.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

/*    tensor_add_layer->base.num_temp_tensors = 0;
    if(input0 && input0_reshape_flag == vx_true_e)
        tensor_add_layer->base.temp_tensors[tensor_add_layer->base.num_temp_tensors++] = input0;
    if(input1 && input1_reshape_flag == vx_true_e)
        tensor_add_layer->base.temp_tensors[tensor_add_layer->base.num_temp_tensors++] = input1;
    if(output && output_reshape_flag == vx_true_e)
        tensor_add_layer->base.temp_tensors[tensor_add_layer->base.num_temp_tensors++] = output;*/

    node->layer = &tensor_add_layer->base;

    return status;

exit:
    if (tensor_add_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_add_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorAdd_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/*Tensor sub*/

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorSub(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSub_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSub_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorSub(vxnne_operation operation)
{
    vxnne_tensor_sub_operation eltwiseOperation   = (vxnne_tensor_sub_operation)operation;

    vx_tensor input1 = eltwiseOperation->input0;
    vx_tensor input2 = eltwiseOperation->input1;
    vx_tensor output = eltwiseOperation->output;
    vx_uint32 dims   = TENSOR_DIM_NUM(output);

    {
        vx_uint8_ptr input1base, input2base, outputbase;
        vx_uint32 i, numCount=1;

        vxoTensor_GetTensorViewMemory(input1, (gctPOINTER*)&input1base, VX_NULL);
        vxoTensor_GetTensorViewMemory(input2, (gctPOINTER*)&input2base, VX_NULL);
        vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&outputbase, VX_NULL);

        for(i = 0; i < dims; i++)
        {
            numCount *= output->dims[i];
        }

        for(i = 0; i < numCount; i++)
        {
            vx_uint32 in1offset, in2offset, outoffset;
            vx_int8_ptr in1, in2, out;

            in1offset = getExpandTensorOffset(i, input1, output->dims);
            in2offset = getExpandTensorOffset(i, input2, output->dims);
            outoffset = getTensorOffset(i, output, output->dims);

            in1 = (vx_int8_ptr)input1base + in1offset;
            in2 = (vx_int8_ptr)input2base + in2offset;
            out = (vx_int8_ptr)outputbase + outoffset;

            {
                vx_status status = VX_SUCCESS;
                vx_float32 in1Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input1), TENSOR_QUANT_TYPE(input1), 0, (vx_uint8_ptr)in1,
                    TENSOR_POS(input1), TENSOR_TF_ZEROPOINT(input1), TENSOR_TF_SCALE(input1));
                vx_float32 in2Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input2), TENSOR_QUANT_TYPE(input2), 0, (vx_uint8_ptr)in2,
                    TENSOR_POS(input2), TENSOR_TF_ZEROPOINT(input2), TENSOR_TF_SCALE(input2));
                status = vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), 0, in1Data_fl32 - in2Data_fl32, (vx_uint8_ptr)out,
                    TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
            }
        }
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSub_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input0   = (vx_tensor)parameters[0];
    vx_tensor input1   = (vx_tensor)parameters[1];
    vx_scalar policy   = (vx_scalar)parameters[2];
    vx_tensor output   = (vx_tensor)parameters[3];

    vx_type_e input0Format = TENSOR_DATA_TYPE(input0);
    vx_type_e input1Format = TENSOR_DATA_TYPE(input1);
    vx_type_e outputFormat = TENSOR_DATA_TYPE(output);

    vx_bool   format_flag           = vx_false_e;
    vx_bool   shExe_flag            = vx_true_e;
    vx_bool   swExe_flag            = vx_false_e;
    vx_bool   enable_2d_tensor      = vx_false_e;
    vx_enum   policyEnum            = policy->value->e;
    vx_uint32 depth                 = (TENSOR_DIM_NUM(output) > 2) ? TENSOR_VIEW_SIZE_INDEX(output, 2) : 1;
    vx_uint32 batchCount0           = (TENSOR_SIZE_INDEX(input0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input0, 3);
    vx_uint32 batchCount1 = (TENSOR_SIZE_INDEX(input1, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input1, 3);
    vx_uint32 batchCount = (TENSOR_SIZE_INDEX(output, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(output, 3);

    vxnne_tensor_sub_layer tensor_sub_layer = VX_NULL;

    swExe_flag = (TENSOR_DIM_NUM(output) > 4);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_sub_layer_s), (gctPOINTER*)&tensor_sub_layer);
    if (!tensor_sub_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_sub_layer, sizeof(vxnne_tensor_sub_layer_s));

    vxnneLayer_Initialize(&tensor_sub_layer->base,
                          "TensorSub",
                          node,
                          vxmOPERATION_COUNT(tensor_sub_layer),
                          tensor_sub_layer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        format_flag = (vx_bool)((input0Format != VX_TYPE_FLOAT32) && (input1Format != VX_TYPE_FLOAT32) && (outputFormat != VX_TYPE_FLOAT32));
        enable_2d_tensor = (vx_bool)(depth == 1 && format_flag && policyEnum == VX_CONVERT_POLICY_SATURATE);
        shExe_flag = format_flag  || enable_2d_tensor;
    }
    else
        shExe_flag = vx_true_e;

    if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) && !swExe_flag)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        if(node->base.context->evisNoInst.supportEVIS)
        {
            if (enable_2d_tensor)
                shaderExecutable = vxnneGetTensor2DAddShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_2D_ADD, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_SUB, output);
            else
                shaderExecutable = vxnneGetTensorAddShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ELTWISE, &node->kernelAttributes.borderMode, input0, input1, NULL, policy, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_SUB, output);
        }
        else
        {
            shaderExecutable = vxnneGetGPUTensorElewiseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_ELTWISE, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_SUB, output);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&tensor_sub_layer->tensorSubSH,
            &tensor_sub_layer->base,
            VXNNE_OPERATOR_TENSOR_SUB,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS)
            goto exit;

        if (batchCount != 1 && batchCount0 != batchCount1)
        {
            vx_tensor src0 = (vx_tensor)shaderExecutable->param[0];
            vx_uint32 batch0 = (TENSOR_SIZE_INDEX(src0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(src0, 3);

            if (batch0 == 1)
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 0, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
            else
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        }

        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSH.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSH.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSH.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &tensor_sub_layer->base,
            &tensor_sub_layer->tensorSubSH.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&tensor_sub_layer->tensorSubSW.base,
                                &tensor_sub_layer->base,
                                VXNNE_OPERATION_TARGET_SW,
                                VXNNE_OPERATOR_TENSOR_SUB,
                                /*vxnneExecuteSWEltwise,*/
                                vxnneExecuteSWTensorSub,
                                VX_NULL,
                                batchCount,
                                0);
        vxnneLayer_SetOperation(
            &tensor_sub_layer->base,
            &tensor_sub_layer->tensorSubSW.base,
            0);

        tensor_sub_layer->tensorSubSW.input0    = input0;
        tensor_sub_layer->tensorSubSW.input1    = input1;
        tensor_sub_layer->tensorSubSW.policy    = policy;
        tensor_sub_layer->tensorSubSW.output    = output;

        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSW.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSW.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_sub_layer->tensorSubSW.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &tensor_sub_layer->base;
    return status;

exit:
    if (tensor_sub_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_sub_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSub_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}
//end tensor sub

/* TODO: the round function may need to be refine on linux platform */
VX_PRIVATE_API double roundDouble(double x)
{
    double tmpFloat = 0.0;

    tmpFloat = floor(x + 0.5f);

    return tmpFloat;
}

VX_PRIVATE_API vx_float32 roundFloat(vx_float32 x)
{
    vx_float32 tmpFloat = 0.0;

    tmpFloat = floorf(x + 0.4999999f);

    return tmpFloat;
}

static vx_int16 trunc_to_int16(int_fast32_t val)
{
    union { vx_int16 i; vx_uint16 u; } tmp;
    tmp.u = (vx_uint16)val;
    return tmp.i;
}

static vx_int8 trunc_to_int8(int_fast32_t val)
{
    union { vx_int8 i; vx_uint8 u; } tmp;
    tmp.u = (vx_uint8)val;
    return tmp.i;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorMul(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMul_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMul_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

//Tensor mul
vx_status vxnneExecuteSWTensorMul(vxnne_operation operation)
{
    vxnne_tensor_mul_operation eltwiseOperation   = (vxnne_tensor_mul_operation)operation;

    vx_tensor input1 = eltwiseOperation->input0;
    vx_tensor input2 = eltwiseOperation->input1;
    vx_tensor output = eltwiseOperation->output;
    vx_uint32 dims   = TENSOR_DIM_NUM(output);

    {
        vx_uint8_ptr input1base, input2base, outputbase;
        vx_uint32 i, numCount = 1;

        vxoTensor_GetTensorViewMemory(input1, (gctPOINTER*)&input1base, VX_NULL);
        vxoTensor_GetTensorViewMemory(input2, (gctPOINTER*)&input2base, VX_NULL);
        vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&outputbase, VX_NULL);

        for (i = 0; i < dims; i++)
        {
            numCount *= output->dims[i];
        }

        for (i = 0; i < numCount; i++)
        {
            vx_uint32 in1offset, in2offset, outoffset;
            vx_int8_ptr in1, in2, out;
            vx_status status = VX_SUCCESS;
            vx_float32 in1Data_fl32, in2Data_fl32;

            in1offset = getExpandTensorOffset(i, input1, output->dims);
            in2offset = getExpandTensorOffset(i, input2, output->dims);
            outoffset = getTensorOffset(i, output, output->dims);

            in1 = (vx_int8_ptr)input1base + in1offset;
            in2 = (vx_int8_ptr)input2base + in2offset;
            out = (vx_int8_ptr)outputbase + outoffset;

            in1Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input1), TENSOR_QUANT_TYPE(input1), 0, (vx_uint8_ptr)in1,
                TENSOR_POS(input1), TENSOR_TF_ZEROPOINT(input1), TENSOR_TF_SCALE(input1));
            in2Data_fl32 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input2), TENSOR_QUANT_TYPE(input2), 0, (vx_uint8_ptr)in2,
                TENSOR_POS(input2), TENSOR_TF_ZEROPOINT(input2), TENSOR_TF_SCALE(input2));
            status = vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), 0, in1Data_fl32 * in2Data_fl32, (vx_uint8_ptr)out,
                TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
        }
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMul_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input0   = (vx_tensor)parameters[0];
    vx_tensor input1   = (vx_tensor)parameters[1];
    vx_scalar scale    = (vx_scalar)parameters[2];
    vx_scalar policy   = (vx_scalar)parameters[3];
    vx_scalar rounding = (vx_scalar)parameters[4];
    vx_tensor output   = (vx_tensor)parameters[5];

    vx_type_e input0Format = TENSOR_DATA_TYPE(input0);
    vx_type_e input1Format = TENSOR_DATA_TYPE(input1);
    vx_type_e outputFormat = TENSOR_DATA_TYPE(output);

    vx_bool shExe_flag   = vx_true_e;
    vx_bool swExe_flag   = vx_false_e;
    vx_uint32 batchCount0 = (TENSOR_SIZE_INDEX(input0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input0, 3);
    vx_uint32 batchCount1 = (TENSOR_SIZE_INDEX(input1, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input1, 3);
    vx_uint32 batchCount = (TENSOR_SIZE_INDEX(output, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(output, 3);

    vxnne_tensor_mul_layer tensor_mul_layer = VX_NULL;

    swExe_flag = (TENSOR_DIM_NUM(output) > 4);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_mul_layer_s), (gctPOINTER*)&tensor_mul_layer);
    if (!tensor_mul_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_mul_layer, sizeof(vxnne_tensor_mul_layer_s));

    vxnneLayer_Initialize(&tensor_mul_layer->base,
                          "TensorMul",
                          node,
                          vxmOPERATION_COUNT(tensor_mul_layer),
                          tensor_mul_layer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
        shExe_flag = (vx_bool)((input0Format != VX_TYPE_FLOAT32) && (input1Format != VX_TYPE_FLOAT32) && (outputFormat != VX_TYPE_FLOAT32));
    else
        shExe_flag = vx_true_e;

    if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) && !swExe_flag)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            vx_enum    policyEnum   = policy->value->e;
            vx_enum    roundingEnum = rounding->value->e;
            vx_float32 scale_val    = scale->value->f32;

            if ((vxDataType_GetSize(input1Format) == 1 && vxDataType_GetSize(input0Format) == 2) || (input1Format == VX_TYPE_INT16 && input0Format == VX_TYPE_FLOAT16))
            {
                if (roundingEnum == VX_ROUND_POLICY_TO_NEAREST_EVEN && policyEnum == VX_CONVERT_POLICY_SATURATE && scale_val == 1.0f)
                    shaderExecutable = vxnneGetTensorMulSatRTEShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MUL_SAT_RTE, &node->kernelAttributes.borderMode, input1, input0, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_MUL, output);
                else
                    shaderExecutable = vxnneGetTensorMulShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MUL, &node->kernelAttributes.borderMode, input1, input0, scale, policy, rounding, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_MUL, output);
            }
            else
            {
                if (roundingEnum == VX_ROUND_POLICY_TO_NEAREST_EVEN && policyEnum == VX_CONVERT_POLICY_SATURATE && scale_val == 1.0f)
                    shaderExecutable = vxnneGetTensorMulSatRTEShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MUL_SAT_RTE, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_MUL, output);
                else
                    shaderExecutable = vxnneGetTensorMulShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MUL, &node->kernelAttributes.borderMode, input0, input1, scale, policy, rounding, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_MUL, output);
            }
        }
        else
        {
            shaderExecutable = vxnneGetGPUTensorElewiseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MUL, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_MUL, output);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&tensor_mul_layer->tensorMulSH,
                                        &tensor_mul_layer->base,
                                        VXNNE_OPERATOR_TENSOR_MUL,
                                        batchCount,
                                        shaderExecutable);

        if (status != VX_SUCCESS)
            goto exit;

        if (batchCount != 1 && batchCount0 != batchCount1)
        {
            vx_tensor src0 = (vx_tensor)shaderExecutable->param[0];
            vx_uint32 batch0 = (TENSOR_SIZE_INDEX(src0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(src0, 3);

            if (batch0 == 1)
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 0, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
            else
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        }

        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSH.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSH.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSH.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &tensor_mul_layer->base,
            &tensor_mul_layer->tensorMulSH.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&tensor_mul_layer->tensorMulSW.base,
                                &tensor_mul_layer->base,
                                VXNNE_OPERATION_TARGET_SW,
                                VXNNE_OPERATOR_TENSOR_MUL,
                                /*vxnneExecuteSWEltwise,*/
                                vxnneExecuteSWTensorMul,
                                VX_NULL,
                                batchCount,
                                0);

        vxnneLayer_SetOperation(
            &tensor_mul_layer->base,
            &tensor_mul_layer->tensorMulSW.base,
            0);

        tensor_mul_layer->tensorMulSW.input0    = input0;
        tensor_mul_layer->tensorMulSW.input1    = input1;
        tensor_mul_layer->tensorMulSW.scale     = scale;
        tensor_mul_layer->tensorMulSW.overflow  = policy;
        tensor_mul_layer->tensorMulSW.rounding  = rounding;
        tensor_mul_layer->tensorMulSW.output    = output;

        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSW.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSW.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_mul_layer->tensorMulSW.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &tensor_mul_layer->base;
    return status;

exit:
    if (tensor_mul_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_mul_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMul_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}
//end tensor mul
//Tensor div
vx_status vxnneExecuteSWTensorDiv(vxnne_operation operation)
{
    vxnne_tensor_div_operation eltwiseOperation   = (vxnne_tensor_div_operation)operation;

    vx_tensor input1 = eltwiseOperation->input0;
    vx_tensor input2 = eltwiseOperation->input1;
    vx_tensor output = eltwiseOperation->output;

    vx_int32 dim1 = input1->viewRegion.dimCount;
    vx_int32 dim2 = input2->viewRegion.dimCount;
    vx_enum overflow = eltwiseOperation->overflow->value->e;

    if (dim1 == dim2)
    {
        vx_enum rounding = eltwiseOperation->rounding->value->e;
        vx_float32 scale = eltwiseOperation->scale->value->f32;
        eltwise(input1, input2, scale, overflow, rounding, VX_TENSOR_OP_DIV, output);
    }
    else
        vxError("Difference dim\n");

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorDiv(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorDiv_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorDiv_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorDiv_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input0   = (vx_tensor)parameters[0];
    vx_tensor input1   = (vx_tensor)parameters[1];
    vx_scalar scale    = (vx_scalar)parameters[2];
    vx_scalar overflow = (vx_scalar)parameters[3];
    vx_scalar rounding = (vx_scalar)parameters[4];
    vx_tensor output   = (vx_tensor)parameters[5];

    vx_type_e input0Format = TENSOR_DATA_TYPE(input0);
    vx_type_e input1Format = TENSOR_DATA_TYPE(input1);
    vx_type_e outputFormat = TENSOR_DATA_TYPE(output);

    vx_bool shExe_flag   = vx_true_e;
    vx_bool swExe_flag   = vx_false_e;
    vx_uint32 batchCount0 = (TENSOR_SIZE_INDEX(input0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input0, 3);
    vx_uint32 batchCount1 = (TENSOR_SIZE_INDEX(input1, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(input1, 3);
    vx_uint32 batchCount = (TENSOR_SIZE_INDEX(output, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(output, 3);

    vxnne_tensor_div_layer tensor_div_layer = VX_NULL;

    swExe_flag = (TENSOR_DIM_NUM(output) > 4);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_div_layer_s), (gctPOINTER*)&tensor_div_layer);
    if (!tensor_div_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_div_layer, sizeof(vxnne_tensor_div_layer_s));

    vxnneLayer_Initialize(&tensor_div_layer->base,
                          "TensorDiv",
                          node,
                          vxmOPERATION_COUNT(tensor_div_layer),
                          tensor_div_layer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
        shExe_flag = (vx_bool)((input0Format != VX_TYPE_FLOAT32) && (input1Format != VX_TYPE_FLOAT32) && (outputFormat != VX_TYPE_FLOAT32));
    else
        shExe_flag = vx_true_e;

    if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) && !swExe_flag)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetTensorDivShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_DIV, &node->kernelAttributes.borderMode, input0, input1, scale, overflow, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_DIV, output);
        }
        else
        {
            shaderExecutable = vxnneGetGPUTensorElewiseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_DIV, &node->kernelAttributes.borderMode, input0, input1, VX_NN_ACTIVATION_NONE, VX_TENSOR_OP_DIV, output);
        }
        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&tensor_div_layer->tensorDivSH,
                                        &tensor_div_layer->base,
                                        VXNNE_OPERATOR_TENSOR_DIV,
                                        batchCount,
                                        shaderExecutable);

        if (status != VX_SUCCESS)
            goto exit;

        if (batchCount != 1 && batchCount0 != batchCount1)
        {
            vx_tensor src0 = (vx_tensor)shaderExecutable->param[0];
            vx_uint32 batch0 = (TENSOR_SIZE_INDEX(src0, 3) == 0) ? 1 : TENSOR_SIZE_INDEX(src0, 3);

            if (batch0 == 1)
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 0, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
            else
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
        }

        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSH.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSH.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSH.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &tensor_div_layer->base,
            &tensor_div_layer->tensorDivSH.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&tensor_div_layer->tensorDivSW.base,
                                &tensor_div_layer->base,
                                VXNNE_OPERATION_TARGET_SW,
                                VXNNE_OPERATOR_TENSOR_DIV,
                                vxnneExecuteSWTensorDiv,
                                VX_NULL,
                                batchCount,
                                0);

        vxnneLayer_SetOperation(
            &tensor_div_layer->base,
            &tensor_div_layer->tensorDivSW.base,
            0);

        tensor_div_layer->tensorDivSW.input0    = input0;
        tensor_div_layer->tensorDivSW.input1    = input1;
        tensor_div_layer->tensorDivSW.scale = scale;
        tensor_div_layer->tensorDivSW.overflow  = overflow;
        tensor_div_layer->tensorDivSW.rounding = rounding;
        tensor_div_layer->tensorDivSW.output    = output;

        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSW.base, (vx_reference)input0, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSW.base, (vx_reference)input1, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_div_layer->tensorDivSW.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &tensor_div_layer->base;
    return status;

exit:
    if (tensor_div_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_div_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorDiv_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}
//end tensor div
VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorTrans(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorTrans_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorTrans_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorTranspose(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_trans_operation transOperation = (vxnne_tensor_trans_operation)operation;

    vx_tensor input  = (vx_tensor)transOperation->input;
    vx_tensor output = (vx_tensor)transOperation->output;

    vx_uint32_ptr perm = (vx_uint32_ptr)transOperation->perm->memory.logicals[0];
    vx_uint32 pnum = transOperation->pnum->value->u32;

    vx_uint8_ptr inaddr, outaddr;
    vx_uint32 dims[VX_CONTEXT_TENSOR_MAX_DIMENSION], strides[VX_CONTEXT_TENSOR_MAX_DIMENSION], tstrides[VX_CONTEXT_TENSOR_MAX_DIMENSION];

    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&inaddr, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&outaddr, VX_NULL);

    vxoTensor_GetTensorDimStride(input, &pnum, dims, strides);
    vxoTensor_GetTensorDimStride(output, &pnum, VX_NULL, tstrides);

    if (pnum == 1)
    {
        memcpy(outaddr, inaddr, dims[0] * strides[0]);
    }
    else
    {
        _TransposeTensor(inaddr, outaddr,TENSOR_DATA_SIZE(input), dims, strides, tstrides, perm, pnum-1);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorTrans_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_context context = vxGetContext((vx_reference)node);

    vx_tensor input   = (vx_tensor)parameters[0];
    vx_array  perm    = (vx_array)parameters[1];
    vx_scalar pnum    = (vx_scalar)parameters[2];
    vx_tensor output  = (vx_tensor)parameters[3];
    vx_uint32 batchCount = 1, batchCount2 = 1;

    vxnne_tensor_trans_layer tensor_trans_layer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_trans_layer_s), (gctPOINTER*)&tensor_trans_layer);
    if (!tensor_trans_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_trans_layer, sizeof(vxnne_tensor_trans_layer_s));

    vxnneLayer_Initialize(&tensor_trans_layer->base,
                          "TensorTranspose",
                          node,
                          vxmOPERATION_COUNT(tensor_trans_layer),
                          tensor_trans_layer->operations,
                          VX_NULL);

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_TRANSPOSE) &&
        vxnneIsTPSupportFormat(context, input, VX_NULL, output) &&
        (pnum->value->u32 > 1))
    {
        vx_op_param_s conv = {0};
        vx_uint32 dnum = pnum->value->u32;

        status = vxnneOperation_Initialize(&tensor_trans_layer->tensor_trans_tp_operation.base,
                                           &tensor_trans_layer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_TENSOR_TRANS,
                                           VX_NULL,
                                           vxnneOperation_TP_Deinitialize,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.conv_rounding_type = 0;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_TRANSPOSE;
        conv.other_ref = (vx_reference)input;
        conv.data_buff = gcvNULL;
        conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        conv.tp_value->u32[0] = dnum;
        conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(vx_uint32) * dnum);
        vxMemCopy(conv.tp_value->p8[0], perm->memory.logicals[0], sizeof(vx_uint32) * dnum);

        vxMemCopy(&tensor_trans_layer->tensor_trans_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneLayer_SetOperation(
            &tensor_trans_layer->base,
            &tensor_trans_layer->tensor_trans_tp_operation.base,
            0);

        tensor_trans_layer->tensor_trans_tp_operation.input  = input;
        tensor_trans_layer->tensor_trans_tp_operation.output = output;

        vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_tp_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        vx_uint32_ptr  pPerm                   = (vx_uint32_ptr)perm->memory.logicals[0];
        vx_uint32      num                     = pnum->value->u32, num2 = 0;
        vx_uint32      batch                   = TENSOR_VIEW_SIZE_INDEX(input, 3);
        vx_bool        shExe_flag              = vx_true_e;
        vx_bool        shExe_copy_flag         = vx_true_e;
        vx_bool        enable_4Dtensor         = vx_false_e;
        vx_bool        enable_batch_sh         = vx_false_e;
        vx_bool        enable_dataFormat       = vx_false_e;
        vx_enum        inputFormat             = TENSOR_DATA_TYPE(input);
        vx_enum        outputFormat            = TENSOR_DATA_TYPE(output);
        vx_uint32      i                       = 0;

        if(context->evisNoInst.supportEVIS)
        {
            enable_dataFormat = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                          (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)     ||
                                          (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)     ||
                                          (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8));
        }
        else
        {
            enable_dataFormat = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                          (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)     ||
                                          (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8));
        }

        enable_4Dtensor = (vx_bool)(enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 2 && pPerm[2] == 3 && pPerm[3] == 0 && num == 4 && batch == 1);
        enable_batch_sh = (vx_bool)(enable_dataFormat &&  pPerm[3] == 3 && num == 4 && TENSOR_DIM_NUM(input) == 4);

        shExe_flag    = (vx_bool)((enable_dataFormat && pPerm[0] == 2 && pPerm[1] == 0 && pPerm[2] == 1  && num == 3)
                                ||(enable_dataFormat && pPerm[0] == 2 && pPerm[1] == 1 && pPerm[2] == 0  && num == 3)
                                ||(enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 2 && pPerm[2] == 0  && num == 3)
                                ||(enable_dataFormat && pPerm[0] == 0 && pPerm[1] == 2 && pPerm[2] == 1  && num == 3)
                                ||(enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 0 && num <= 3 && num >= 2)
                                || (enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 3 && pPerm[2] == 2 && pPerm[3] == 0 && num == 4)
                                || enable_4Dtensor || enable_batch_sh);

        for (i = 0; i < gcmMIN(TENSOR_DIM_NUM(input), num); i++)
        {
            shExe_copy_flag &= (pPerm[i] == i);
        }

        if(context->evisNoInst.supportEVIS)
        {
            shExe_copy_flag &= (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                                        || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
                                        || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)
                                        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16)));
        }
        else
        {
            shExe_copy_flag &= (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                                        || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                                        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT32)
                                        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                        || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16)));
        }

        if (shExe_copy_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_tensor src          = NULL;
            vx_tensor dst          = NULL;
            vx_uint32 sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION];
            vx_uint32 dims = 0;

            vxoElementOptimization_GetTensorShape(input, sizes, &dims);

            src     = vxoTensor_ReshapeTensor(input, (vx_int32*)sizes, dims);
            dst     = vxoTensor_ReshapeTensor(output, (vx_int32*)sizes, dims);

            if(node->base.context->evisNoInst.supportEVIS)
            {
                if (src && dst)
                    shaderExecutable = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, src, dst);
            }
            else
            {
                if (src && dst)
                    shaderExecutable = vxnneGPUTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, src, dst);
            }

            if (src) vxoTensor_ReleaseTensor(&src);
            if (dst) vxoTensor_ReleaseTensor(&dst);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&tensor_trans_layer->tensor_copy_sh_operation,
                &tensor_trans_layer->base,
                VXNNE_OPERATOR_CONVERT_FORMAT,
                1,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&tensor_trans_layer->tensor_copy_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_trans_layer->tensor_copy_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_trans_layer->base,
                &tensor_trans_layer->tensor_copy_sh_operation.base,
                0);
        }
        else if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vx_uint32 width            = 0;
            vx_uint32 height           = 0;
            vx_uint32 depth            = 0;
            vx_int32  size[4]          = {0, 0, 0, 0};
            vx_uint32 permArray[4]     = {1, 2, 0, 3}, permArray2[4] = { 0, 2, 1, 3 };
            vx_uint32 dims             = 3;
            vx_tensor src = NULL, src2 = NULL;
            vx_tensor dst = NULL, dst2 = NULL;
            vxnne_shader_executable shaderExecutable = VX_NULL;

            width       = TENSOR_VIEW_SIZE_INDEX(input, 0);
            height      = TENSOR_DIM_NUM(input) > 1 ? TENSOR_VIEW_SIZE_INDEX(input, 1) : 1;
            depth       = TENSOR_DIM_NUM(input) > 2 ? TENSOR_VIEW_SIZE_INDEX(input, 2) : 1;
            batch       = TENSOR_DIM_NUM(input) > 3 ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;

            batchCount = TENSOR_DIM_NUM(input) > 3  ? batch : 1;

            if (enable_batch_sh)
            {
                num = 3;
            }

            if (pPerm[0] == 2 && pPerm[1] == 0 && pPerm[2] == 1 && width * height < IMG_MAX_WIDTH && depth < IMG_MAX_WIDTH)
            {
                size[0] = width * height;
                size[1] = depth;
                size[2] = 1;
                size[3] = batch;
                dims    = TENSOR_DIM_NUM(input);
                src = vxoTensor_ReshapeTensor(input, size, dims);

                size[0] = depth;
                size[1] = width * height;
                size[2] = 1;
                size[3] = batch;
                dims    = TENSOR_DIM_NUM(input);
                dst = vxoTensor_ReshapeTensor(output, size, dims);

                num = 2;
            }
            else if (enable_4Dtensor)
            {
                size[0] = width;
                size[1] = height;
                size[2] = depth;
                dims    = 3;
                src = vxoTensor_ReshapeTensor(input, size, dims);

                size[0] = height;
                size[1] = depth;
                size[2] = width;
                dims    = 3;
                dst = vxoTensor_ReshapeTensor(output, size, dims);

                num = 3;
            }
            else if (pPerm[0] == 1 && pPerm[1] == 3 && pPerm[2] == 2 && pPerm[3] == 0 && width * height < IMG_MAX_WIDTH && depth < IMG_MAX_WIDTH)
            {
                size[0] = width;
                size[1] = height * depth * batch;
                size[2] = 1;
                dims = 3;
                src = vxoTensor_ReshapeTensor(input, size, dims);

                size[0] = height * depth * batch;
                size[1] = width;
                size[2] = 1;
                dims = 3;

                {
                    vx_tensor_create_params_t param = {dims, VX_NULL, TENSOR_DATA_TYPE(input), TENSOR_QUANT_TYPE(input), };
                    param.sizes = (vx_uint32_ptr)size;

                    if (TENSOR_QUANT_TYPE(input) == VX_QUANT_DYNAMIC_FIXED_POINT)
                        param.quant_data.dfp.fixed_point_pos = TENSOR_POS(input);
                    else if (TENSOR_QUANT_TYPE(input) == VX_QUANT_AFFINE_SCALE)
                    {
                        param.quant_data.affine.scale = TENSOR_TF_SCALE(input);
                        param.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input);
                    }

                    dst = vxoTensor_CreateTensor2(node->base.context, &param, sizeof(vx_tensor_create_params_t));
                }

                num = 3;
                permArray[0] = 1;
                permArray[1] = 0;
                permArray[2] = 2;

                size[0] = height;
                size[1] = depth;
                size[2] = batch;
                size[3] = width;
                dims = 4;
                src2 = vxoTensor_ReshapeTensor(dst, size, dims);

                dst2 = output;
                batchCount2 = width;
                num2 = 3;
            }

            if (src && dst)
            {
                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src, permArray, num, dst);
                }
                else
                {
                    shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src, permArray, num, dst);
                }

                vxoTensor_ReleaseTensor(&src);
                vxoTensor_ReleaseTensor(&dst);
            }
            else
            {
                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, pPerm, num, output);
                }
                else
                {
                    shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, pPerm, num, output);
                }
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&tensor_trans_layer->tensor_trans_shader_operation,
                &tensor_trans_layer->base,
                VXNNE_OPERATOR_TENSOR_TRANS,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_shader_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_shader_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_trans_layer->base,
                &tensor_trans_layer->tensor_trans_shader_operation.base,
                0);

            if (src2 && dst2)
            {
                vxnne_shader_executable shaderExecutable2 = VX_NULL;

                if (node->base.context->evisNoInst.supportEVIS)
                    shaderExecutable2 = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src2, permArray2, num2, dst2);
                else
                    shaderExecutable2 = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src2, permArray2, num2, dst2);

                status = vxnneShaderOperation_Initialize(&tensor_trans_layer->tensor_trans_shader_operation2,
                    &tensor_trans_layer->base,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    batchCount2,
                    shaderExecutable2);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneLayer_SetOperation(
                    &tensor_trans_layer->base,
                    &tensor_trans_layer->tensor_trans_shader_operation2.base,
                    1);

                vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_shader_operation2.base, (vx_reference)src2, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_shader_operation2.base, (vx_reference)dst2, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxoTensor_ReleaseTensor(&src2);
            }
        }
        else
        {
            vxnneOperation_Initialize(&tensor_trans_layer->tensor_trans_sw_operation.base,
                                      &tensor_trans_layer->base,
                                      VXNNE_OPERATION_TARGET_SW,
                                      VXNNE_OPERATOR_TENSOR_TRANS,
                                      vxnneExecuteSWTensorTranspose,
                                      VX_NULL,
                                      batchCount,
                                      0);

            vxnneLayer_SetOperation(
                &tensor_trans_layer->base,
                &tensor_trans_layer->tensor_trans_sw_operation.base,
                0);

            tensor_trans_layer->tensor_trans_sw_operation.input   = input;
            tensor_trans_layer->tensor_trans_sw_operation.perm    = perm;
            tensor_trans_layer->tensor_trans_sw_operation.pnum    = pnum;
            tensor_trans_layer->tensor_trans_sw_operation.output  = output;

            vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_trans_layer->tensor_trans_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
    }

    node->layer = &tensor_trans_layer->base;
    return status;

exit:
    if (tensor_trans_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_trans_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorTrans_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNRPNLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}



/*RPN - CPU*/
vx_status vxnneExecuteSWRPN(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_tensor_rpn_operation rpnOperation = (vxnne_tensor_rpn_operation)operation;

    /*
        width: W, height: H, anchor number: k

        input  score:    1x(k*2)xWxH
        input  bbox:     1x(k*4)xWxH
        input  anchor:   kx4x1x1
        input  img_info: 1x4x1x1
        output roi:      300x5x1x1
        output score:    300x1x1x1
    */
    vx_tensor score         = rpnOperation->score;
    vx_tensor bbox          = rpnOperation->bbox;
    vx_tensor anchor        = rpnOperation->anchors;
    vx_tensor img_info      = rpnOperation->img_info;
    vx_tensor roi_output    = rpnOperation->roi_output;
    vx_tensor score_output  = rpnOperation->score_output;

    vx_uint32 feat_stride   = rpnOperation->feature_stride->value->u32;
    vx_uint32 min_size      = rpnOperation->min_size->value->u32;
    vx_uint32 pre_nms_topn  = rpnOperation->pre_nms_topn->value->u32;
    vx_uint32 post_nms_topn = rpnOperation->post_nms_topn->value->u32;
    vx_float32 nms_thresh   = rpnOperation->nms_thresh->value->f32;

    vx_uint32 score_channel = TENSOR_VIEW_SIZE_INDEX(score, 2);
    vx_uint32 score_height  = TENSOR_VIEW_SIZE_INDEX(score, 1);
    vx_uint32 score_width   = TENSOR_VIEW_SIZE_INDEX(score, 0);
    vx_uint32 score_count   = score_width * score_height * score_channel/2; /*14 x 14 x (2x42) -> 17901 x 2 */

    vx_uint32 bbox_height   = TENSOR_VIEW_SIZE_INDEX(bbox, 1);
    vx_uint32 bbox_width    = TENSOR_VIEW_SIZE_INDEX(bbox, 0);

    vx_uint32 anchor_count  = TENSOR_VIEW_SIZE_INDEX(anchor, 3); /* anchor batch = anchor number */

    vx_type_e in_score_format   = (vx_type_e)TENSOR_DATA_TYPE(score);
    vx_type_e in_bbox_format    = (vx_type_e)TENSOR_DATA_TYPE(bbox);
    vx_type_e in_anchor_format  = (vx_type_e)TENSOR_DATA_TYPE(anchor);
    vx_type_e in_img_format     = (vx_type_e)TENSOR_DATA_TYPE(img_info);
    vx_type_e out_roi_format    = (vx_type_e)TENSOR_DATA_TYPE(roi_output);
    vx_int8 in_score_fp         = TENSOR_POS(score);
    vx_int8 in_bbox_fp          = TENSOR_POS(bbox);
    vx_int8 in_anchor_fp        = TENSOR_POS(anchor);
    vx_int8 in_img_fp           = TENSOR_POS(img_info);
    vx_int8 out_roi_fp          = TENSOR_POS(roi_output);
    vx_enum in_score_quant_format   = TENSOR_QUANT_TYPE(score);
    vx_enum in_bbox_quant_format    = TENSOR_QUANT_TYPE(bbox);
    vx_enum in_anchor_quant_format  = TENSOR_QUANT_TYPE(anchor);
    vx_enum in_img_quant_format     = TENSOR_QUANT_TYPE(img_info);
    vx_enum out_roi_quant_format    = TENSOR_QUANT_TYPE(roi_output);
    vx_int32 in_score_zp            = TENSOR_TF_ZEROPOINT(score);
    vx_int32 in_bbox_zp             = TENSOR_TF_ZEROPOINT(bbox);
    vx_int32 in_anchor_zp           = TENSOR_TF_ZEROPOINT(anchor);
    vx_int32 in_img_zp              = TENSOR_TF_ZEROPOINT(img_info);
    vx_int32 out_roi_zp             = TENSOR_TF_ZEROPOINT(roi_output);
    vx_float32 in_score_scale       = TENSOR_TF_SCALE(score);
    vx_float32 in_bbox_scale        = TENSOR_TF_SCALE(bbox);
    vx_float32 in_anchor_scale      = TENSOR_TF_SCALE(anchor);
    vx_float32 in_img_scale         = TENSOR_TF_SCALE(img_info);
    vx_float32 out_roi_scale        = TENSOR_TF_SCALE(roi_output);
    vx_enum out_roi_rMode           = TENSOR_ROUNDING_MODE(roi_output);

    vx_uint32 proposal_width, proposal_height, proposal_area, proposal_count;
    vx_float32 img_W,img_H,img_scale_H,img_scale_W;
    vx_float32 min_box_H,min_box_W;
    vx_uint8_ptr score_data,bbox_data,img_data,anchor_data,out_roi_data;
    vx_float32_ptr score_buffer = NULL, foreground_score = NULL;
    vx_float32_ptr proposals    = NULL, p_proposals = NULL;
    vx_uint32_ptr roi_indices   = NULL;
    vx_uint32 i = 0, w = 0, h = 0, k = 0;
    vx_uint32 real_roi = 0;

    vx_type_e out_score_format = 0;
    vx_int8 out_score_fp = 0;
    vx_enum out_score_quant_format = 0;
    vx_int32 out_score_zp = 0;
    vx_float32 out_score_scale = 0;
    vx_enum out_score_rMode = 0;
    vx_uint8_ptr out_score_data = NULL;

    vx_bool input_stage,output_stage;

    vxoBinaryGraph_SaveSWOperation(operation);

    if(score_height != bbox_height || score_width != bbox_width)
    {
        vxError("parameter error: score_H[%u] != bbox_H[%u] || score_W[%u] != bbox_W[%u]\n",
            score_height, bbox_height, score_width, bbox_width);
        return VX_FAILURE;
    }

    proposal_width  = score_width;
    proposal_height = score_height;
    proposal_area   = proposal_width * proposal_height;
    proposal_count  = proposal_area * anchor_count;

    score_buffer    = (vx_float32_ptr)vxAllocateAndZeroMemory((score_count * 2) * sizeof(vx_float32)); /* foreground + background */
    proposals       = (vx_float32_ptr)vxAllocateAndZeroMemory((proposal_count * 5) * sizeof(vx_float32)); /* 5: score,x1,y1,x2,y2 */
    roi_indices     = (vx_uint32_ptr)vxAllocateAndZeroMemory(post_nms_topn * sizeof(vx_uint32));

    if (score_buffer == NULL || proposals == NULL || roi_indices == NULL)
    {
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        if(score_buffer) vxFree(score_buffer);
        if(proposals) vxFree(proposals);
        if(roi_indices) vxFree(roi_indices);
        return VX_ERROR_NO_MEMORY;
    }

    input_stage = vx_true_e;
    output_stage = vx_true_e;
    vxnneGetTensorMemeory(score, (vx_ptr_ptr)&score_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(bbox, (vx_ptr_ptr)&bbox_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(img_info, (vx_ptr_ptr)&img_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(anchor, (vx_ptr_ptr)&anchor_data, input_stage, vx_false_e);

    vxnneGetTensorMemeory(roi_output, (vx_ptr_ptr)&out_roi_data, output_stage, vx_true_e);

    if(score_output)
    {
        out_score_format        = (vx_type_e)TENSOR_DATA_TYPE(score_output);
        out_score_fp            = TENSOR_POS(score_output);
        out_score_quant_format  = TENSOR_QUANT_TYPE(score_output);
        out_score_zp            = TENSOR_TF_ZEROPOINT(score_output);
        out_score_scale         = TENSOR_TF_SCALE(score_output);
        out_score_rMode         = TENSOR_ROUNDING_MODE(score_output);
        vxnneGetTensorMemeory(score_output, (vx_ptr_ptr)&out_score_data, output_stage, vx_true_e);
    }

    img_W       = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 0, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_H       = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 1, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_scale_W = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 2, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_scale_H = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 3, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    min_box_W   = min_size * img_scale_W;
    min_box_H   = min_size * img_scale_H;

    /*
        1. prepare the score softmax
          1.1 resharp score data
          1.2 softmax the score data
          1.3 resharp score data back
    */
    for (i = 0; i < score_count; i++)
    {
        vx_float32 score0 = (vx_float32)vxnneGetDataExt(in_score_format, in_score_quant_format, i, (vx_uint8_ptr)score_data, in_score_fp, in_score_zp, in_score_scale);
        vx_float32 score1 = (vx_float32)vxnneGetDataExt(in_score_format, in_score_quant_format, i + score_count, (vx_uint8_ptr)score_data, in_score_fp, in_score_zp, in_score_scale);
        vx_float32 sum = 0.0f, max = gcmMAX(score0, score1);

        score0 -= max;
        score1 -= max;

        score0 = expf(score0);
        score1 = expf(score1);
        sum = score0 + score1;

        /*
            score_buffer:
                0 ~ score_count:                Background scores
                score_count ~ score_count*2:    Foreground scores
        */
        score_buffer[i] = score0 / sum;
        score_buffer[i + score_count] = score1 / sum;
    }
    foreground_score = score_buffer + score_count;

    /*
        2. fill proposal
          2.1 bbox regression
          2.2 filter out too small boxes
          2.3 fill score and bbox into proposal buffer
    */
    p_proposals = proposals;
    for(h=0; h<proposal_height; h++)
    {
        for(w=0; w<proposal_width; w++)
        {
            vx_uint32 x = w * feat_stride;
            vx_uint32 y = h * feat_stride;
            vx_uint32 offset    = h * proposal_width + w;
            vx_uint8_ptr p_box  = bbox_data + offset * vxnneGetTypeSize(in_bbox_format);
            vx_float32 *p_score = foreground_score + offset;

            for(k=0; k<anchor_count; k++)
            {
                vx_float32 dx = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+0)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 dy = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+1)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 d_log_w = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+2)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 d_log_h = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+3)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);

                p_proposals[0] = x + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+0), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                p_proposals[1] = y + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+1), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                p_proposals[2] = x + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+2), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                p_proposals[3] = y + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+3), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);

                p_proposals[4] = vx_nn_rpn_transform_box(
                                    p_proposals,
                                    dx, dy,
                                    d_log_w, d_log_h,
                                    img_W, img_H,
                                    min_box_W, min_box_H
                                  ) * p_score[k * proposal_area];
                p_proposals += 5;
            }
        }
    }

    /* 3. Sort the proposal buffer */
    vx_nn_rpn_qsort_box(proposals, 0, proposal_count-1, pre_nms_topn);

    /* 4. NMS */
    vx_nn_rpn_nms_cpu(pre_nms_topn, proposals, roi_indices, &real_roi, 0, nms_thresh, post_nms_topn);

    /* 5. Retrieve the rois, output proposal buffer to roi_output & score_output */
    for(i = 0; i < real_roi; i++)
    {
        vx_float32 item_index = 0.0f; /* item_index = input score batch, but we only supported single batch */
        p_proposals = proposals + roi_indices[i] * 5;

        /* Copy proposals coordinate(x1, y1, x2, y2) to roi output tensor */
        vxnneSaveDataExt(out_roi_format, out_roi_quant_format, (i * 5 + 0), item_index, (vx_uint8_ptr)out_roi_data, out_roi_fp, out_roi_zp, out_roi_scale, out_roi_rMode);
        vxnneSaveDataExt(out_roi_format, out_roi_quant_format, (i * 5 + 1), p_proposals[0], (vx_uint8_ptr)out_roi_data, out_roi_fp, out_roi_zp, out_roi_scale, out_roi_rMode);
        vxnneSaveDataExt(out_roi_format, out_roi_quant_format, (i * 5 + 2), p_proposals[1], (vx_uint8_ptr)out_roi_data, out_roi_fp, out_roi_zp, out_roi_scale, out_roi_rMode);
        vxnneSaveDataExt(out_roi_format, out_roi_quant_format, (i * 5 + 3), p_proposals[2], (vx_uint8_ptr)out_roi_data, out_roi_fp, out_roi_zp, out_roi_scale, out_roi_rMode);
        vxnneSaveDataExt(out_roi_format, out_roi_quant_format, (i * 5 + 4), p_proposals[3], (vx_uint8_ptr)out_roi_data, out_roi_fp, out_roi_zp, out_roi_scale, out_roi_rMode);

        /* Copy proposals score to score output tensor */
        if(score_output)
        {
            vxnneSaveDataExt(out_score_format, out_score_quant_format, i, p_proposals[4], (vx_uint8_ptr)out_score_data, out_score_fp, out_score_zp, out_score_scale, out_score_rMode);
        }
    }

    if(score_buffer) vxFree(score_buffer);
    if(proposals) vxFree(proposals);
    if(roi_indices) vxFree(roi_indices);

    if (input_stage)
    {
        vxFree(score_data);
        vxFree(bbox_data);
        vxFree(img_data);
        vxFree(anchor_data);
    }

    if (output_stage)
    {
        vx_uint32 roi_output_size = 0;
        vx_ptr roi_output_logical = VX_NULL;
        vxoTensor_GetTensorSize(roi_output, &roi_output_size);
        vxoTensor_GetTensorViewMemory(roi_output, &roi_output_logical, VX_NULL);
        gcoOS_MemCopy(roi_output_logical, out_roi_data, roi_output_size);

        vxFree(out_roi_data);
    }

    if(score_output && output_stage == vx_true_e)
    {
        vx_uint32 score_output_size = 0;
        vx_ptr score_output_logical = VX_NULL;
        vxoTensor_GetTensorSize(score_output, &score_output_size);
        vxoTensor_GetTensorViewMemory(score_output, &score_output_logical, VX_NULL);
        gcoOS_MemCopy(score_output_logical, out_score_data, score_output_size);

        vxFree(out_score_data);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_Initializer_cpu(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  score                      = (vx_tensor)parameters[0];
    vx_tensor  bbox                       = (vx_tensor)parameters[1];
    vx_tensor  anchors                    = (vx_tensor)parameters[2];
    vx_tensor  img_info                   = (vx_tensor)parameters[3];
    vx_scalar  feature_stride             = (vx_scalar)parameters[4];
    vx_scalar  min_size                   = (vx_scalar)parameters[5];
    vx_scalar  pre_nms_topn               = (vx_scalar)parameters[6];
    vx_scalar  post_nms_topn              = (vx_scalar)parameters[7];
    vx_scalar  nms_thresh                 = (vx_scalar)parameters[8];
    vx_tensor  roi_output                 = (vx_tensor)parameters[9];
    vx_tensor  score_output               = (vx_tensor)parameters[10];
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(score, 3);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        vxnne_tensor_rpn_layer rpnLayer;

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_rpn_layer_s), (gctPOINTER*)&rpnLayer);
        if (!rpnLayer)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }

        gcoOS_ZeroMemory(rpnLayer, sizeof(vxnne_tensor_rpn_layer_s));

        vxnneLayer_Initialize(&rpnLayer->base,
            "RpnLayer",
            node,
            vxmOPERATION_COUNT(rpnLayer),
            rpnLayer->operations,
            VX_NULL);

        vxnneOperation_Initialize(&rpnLayer->tensorRpnSW.base,
            &rpnLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_RPN,
            vxnneExecuteSWRPN,
            VX_NULL,
            batchCount,
            0);

        rpnLayer->tensorRpnSW.score           = score;
        rpnLayer->tensorRpnSW.bbox            = bbox;
        rpnLayer->tensorRpnSW.anchors         = anchors;
        rpnLayer->tensorRpnSW.img_info        = img_info;
        rpnLayer->tensorRpnSW.feature_stride  = feature_stride;
        rpnLayer->tensorRpnSW.min_size        = min_size;
        rpnLayer->tensorRpnSW.pre_nms_topn    = pre_nms_topn;
        rpnLayer->tensorRpnSW.post_nms_topn   = post_nms_topn;
        rpnLayer->tensorRpnSW.nms_thresh      = nms_thresh;
        rpnLayer->tensorRpnSW.roi_output      = roi_output;
        rpnLayer->tensorRpnSW.score_output    = score_output;

        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)score, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)bbox, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)anchors, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)img_info, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)roi_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        vxnneOperation_AddReference(&rpnLayer->tensorRpnSW.base, (vx_reference)score_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);


        vxnneLayer_SetOperation(
            &rpnLayer->base,
            &rpnLayer->tensorRpnSW.base,
            0);
        //rpnLayer->operations[0] = (vxnne_operation)&rpnLayer->tensorRpnSW;
        node->layer = &rpnLayer->base;
    }

exit:

    return status;
}
/*RPN - SHD*/
vx_status vxnneExecuteSWRPN_Softmax(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_softmax_operation rpnSoftmaxOperation = (vxnne_tensor_rpn_softmax_operation)operation;

    vx_tensor input     = rpnSoftmaxOperation->input;
    vx_tensor output    = rpnSoftmaxOperation->output;

    vx_uint32 channel   = TENSOR_VIEW_SIZE_INDEX(input, 2);
    vx_uint32 height    = TENSOR_VIEW_SIZE_INDEX(input, 1);
    vx_uint32 width     = TENSOR_VIEW_SIZE_INDEX(input, 0);
    vx_uint32 count     = width * height * channel / 2;

    vx_type_e in_format     = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e out_format    = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_int8 in_fp           = TENSOR_POS(input);
    vx_int8 out_fp          = TENSOR_POS(output);

    vx_enum in_quant_format = TENSOR_QUANT_TYPE(input);
    vx_enum out_quant_format = TENSOR_QUANT_TYPE(output);
    vx_int32 in_zp = TENSOR_TF_ZEROPOINT(input);
    vx_int32 out_zp = TENSOR_TF_ZEROPOINT(output);
    vx_float32 in_scale = TENSOR_TF_SCALE(input);
    vx_float32 out_scale = TENSOR_TF_SCALE(output);

    vx_bool input_stage = rpnSoftmaxOperation->input_stage;
    vx_bool output_stage = rpnSoftmaxOperation->output_stage;

    vx_uint32 i;
    vx_uint8_ptr input_data,output_data;

    vxnneGetTensorMemeory(input, (vx_ptr_ptr)&input_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(output, (vx_ptr_ptr)&output_data, output_stage, vx_true_e);

    for(i = 0; i < count; i++)
    {
        vx_float32 value0,value1;
        vx_float32 score0 = (vx_float32)vxnneGetDataExt(in_format, in_quant_format, i, (vx_uint8_ptr)input_data, in_fp, in_zp, in_scale);
        vx_float32 score1 = (vx_float32)vxnneGetDataExt(in_format, in_quant_format, i + count, (vx_uint8_ptr)input_data, in_fp, in_zp, in_scale);
        vx_float32 sum = 0.0f, max = gcmMAX(score0, score1);

        score0 -= max;
        score1 -= max;

        score0 = expf(score0);
        score1 = expf(score1);
        sum = score0 + score1;

        value0 = score0 / sum;
        value1 = score1 / sum;
        vxnneSaveDataExt(out_format, out_quant_format, i, value0, (vx_uint8_ptr)output_data, out_fp, out_zp, out_scale, TENSOR_ROUNDING_MODE(output));
        vxnneSaveDataExt(out_format, out_quant_format, (i + count), value1, (vx_uint8_ptr)output_data, out_fp, out_zp, out_scale, TENSOR_ROUNDING_MODE(output));
    }

    if(input_stage)
    {
        vxFree(input_data);
    }
    if(output_stage)
    {
        vx_uint32 output_size = 0;
        vx_ptr output_logical = VX_NULL;
        vxoTensor_GetTensorSize(output, &output_size);
        vxoTensor_GetTensorViewMemory(output, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, output_data, output_size);

        vxFree(output_data);
    }

    return status;
}

vx_status vxnneExecuteSWRPN_Regression(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_regression_operation rpnRegOperation = (vxnne_tensor_rpn_regression_operation)operation;

    vx_tensor score         = rpnRegOperation->score_buffer;
    vx_tensor bbox          = rpnRegOperation->bbox;
    vx_tensor anchor        = rpnRegOperation->anchors;
    vx_tensor img_info      = rpnRegOperation->img_info;
    vx_tensor output        = rpnRegOperation->output;

    vx_uint32 feat_stride   = rpnRegOperation->feature_stride->value->u32;
    vx_uint32 min_size      = rpnRegOperation->min_size->value->u32;

    vx_bool input_stage     = rpnRegOperation->input_stage;
    vx_bool output_stage    = rpnRegOperation->output_stage;

    vx_type_e in_score_format   = (vx_type_e)TENSOR_DATA_TYPE(score);
    vx_type_e in_bbox_format    = (vx_type_e)TENSOR_DATA_TYPE(bbox);
    vx_type_e in_anchor_format  = (vx_type_e)TENSOR_DATA_TYPE(anchor);
    vx_type_e in_img_format     = (vx_type_e)TENSOR_DATA_TYPE(img_info);
    vx_type_e output_format     = (vx_type_e)TENSOR_DATA_TYPE(output);

    vx_enum in_score_quant_format = TENSOR_QUANT_TYPE(score);
    vx_enum in_bbox_quant_format = TENSOR_QUANT_TYPE(bbox);
    vx_enum in_anchor_quant_format = TENSOR_QUANT_TYPE(anchor);
    vx_enum in_img_quant_format = TENSOR_QUANT_TYPE(img_info);
    vx_enum output_quant_format = TENSOR_QUANT_TYPE(output);

    vx_int8 in_bbox_fp          = TENSOR_POS(bbox);
    vx_int8 in_anchor_fp        = TENSOR_POS(anchor);
    vx_int8 in_img_fp           = TENSOR_POS(img_info);
    vx_int8 in_score_fp         = TENSOR_POS(score);
    vx_int8 output_fp           = TENSOR_POS(output);

    vx_int32 in_bbox_zp         = TENSOR_TF_ZEROPOINT(bbox);
    vx_int32 in_anchor_zp       = TENSOR_TF_ZEROPOINT(anchor);
    vx_int32 in_img_zp          = TENSOR_TF_ZEROPOINT(img_info);
    vx_int32 in_score_zp        = TENSOR_TF_ZEROPOINT(score);
    vx_int32 output_zp          = TENSOR_TF_ZEROPOINT(output);

    vx_float32 in_bbox_scale    = TENSOR_TF_SCALE(bbox);
    vx_float32 in_anchor_scale  = TENSOR_TF_SCALE(anchor);
    vx_float32 in_img_scale     = TENSOR_TF_SCALE(img_info);
    vx_float32 in_score_scale   = TENSOR_TF_SCALE(score);
    vx_float32 output_scale     = TENSOR_TF_SCALE(output);

    vx_uint32 score_channel = TENSOR_VIEW_SIZE_INDEX(score, 2);
    vx_uint32 score_height  = TENSOR_VIEW_SIZE_INDEX(score, 1);
    vx_uint32 score_width   = TENSOR_VIEW_SIZE_INDEX(score, 0);
    vx_uint32 score_count   = score_width * score_height * score_channel/2;

    vx_uint32 proposal_width    = score_width;
    vx_uint32 proposal_height   = score_height;
    vx_uint32 anchor_count      = TENSOR_VIEW_SIZE_INDEX(anchor, 3); /* anchor batch = anchor number */

    vx_uint32 proposal_area     = proposal_width * proposal_height;

    vx_uint8_ptr bbox_data = VX_NULL, img_data = VX_NULL, anchor_data = VX_NULL, score_data = VX_NULL,out_data = VX_NULL;
    vx_float32_ptr /*foreground_score,*/ proposals, p_proposals;

    vx_float32 img_W,img_H,img_scale_H,img_scale_W,min_box_H,min_box_W;
    vx_uint32 w, h, k;
    vx_float32 tmp_output[5];

    vxmONERROR(vxnneGetTensorMemeory(score, (vx_ptr_ptr)&score_data, input_stage, vx_false_e));
    vxmONERROR(vxnneGetTensorMemeory(bbox, (vx_ptr_ptr)&bbox_data, input_stage, vx_false_e));
    vxmONERROR(vxnneGetTensorMemeory(img_info, (vx_ptr_ptr)&img_data, input_stage, vx_false_e));
    vxmONERROR(vxnneGetTensorMemeory(anchor, (vx_ptr_ptr)&anchor_data, input_stage, vx_false_e));
    vxmONERROR(vxnneGetTensorMemeory(output, (vx_ptr_ptr)&out_data, output_stage, vx_true_e));

    proposals        = (vx_float32_ptr)out_data;

    img_W       = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 0, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_H       = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 1, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_scale_W = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 2, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    img_scale_H = (vx_float32)vxnneGetDataExt(in_img_format, in_img_quant_format, 3, (vx_uint8_ptr)img_data, in_img_fp, in_img_zp, in_img_scale);
    min_box_W   = min_size * img_scale_W;
    min_box_H   = min_size * img_scale_H;

    p_proposals = proposals;
    for(h=0; h<proposal_height; h++)
    {
        for(w=0; w<proposal_width; w++)
        {
            vx_uint32 x = w * feat_stride;
            vx_uint32 y = h * feat_stride;
            vx_uint32 offset    = h * proposal_width + w;
            vx_uint32 offset_ouptput    = h * proposal_width * anchor_count + w * anchor_count;
            vx_uint8_ptr p_box  = bbox_data + offset * vxnneGetTypeSize(in_bbox_format);
            //vx_float32 *p_score = foreground_score + offset;
            vx_uint8_ptr p_score  = score_data + (offset +  score_count)* vxnneGetTypeSize(in_score_format);

            for(k=0; k<anchor_count; k++)
            {
                vx_float32 dx = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+0)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 dy = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+1)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 d_log_w = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+2)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);
                vx_float32 d_log_h = vxnneGetDataExt(in_bbox_format, in_bbox_quant_format, (k*4+3)*proposal_area, (vx_uint8_ptr)p_box, in_bbox_fp, in_bbox_zp, in_bbox_scale);

                /* proposals = {x1, y1, x2, y2, score} */
                vx_float32 cur_score = vxnneGetDataExt(in_score_format, in_score_quant_format, k*proposal_area, (vx_uint8_ptr)p_score, in_score_fp, in_score_zp, in_score_scale);

                tmp_output[0] = x + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+0), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                tmp_output[1] = y + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+1), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                tmp_output[2] = x + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+2), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);
                tmp_output[3] = y + vxnneGetDataExt(in_anchor_format, in_anchor_quant_format, (k*4+3), (vx_uint8_ptr)anchor_data, in_anchor_fp, in_anchor_zp, in_anchor_scale);

                tmp_output[4] = vx_nn_rpn_transform_box(
                                    tmp_output,
                                    dx, dy,
                                    d_log_w, d_log_h,
                                    img_W, img_H,
                                    min_box_W, min_box_H
                                  ) * cur_score;

                vxnneSaveDataExt(output_format, output_quant_format, (offset_ouptput + k) * 5 + 0, tmp_output[0], (vx_uint8_ptr)out_data, output_fp, output_zp, output_scale, TENSOR_ROUNDING_MODE(output));
                vxnneSaveDataExt(output_format, output_quant_format, (offset_ouptput + k) * 5 + 1, tmp_output[1], (vx_uint8_ptr)out_data, output_fp, output_zp, output_scale, TENSOR_ROUNDING_MODE(output));
                vxnneSaveDataExt(output_format, output_quant_format, (offset_ouptput + k) * 5 + 2, tmp_output[2], (vx_uint8_ptr)out_data, output_fp, output_zp, output_scale, TENSOR_ROUNDING_MODE(output));
                vxnneSaveDataExt(output_format, output_quant_format, (offset_ouptput + k) * 5 + 3, tmp_output[3], (vx_uint8_ptr)out_data, output_fp, output_zp, output_scale, TENSOR_ROUNDING_MODE(output));
                vxnneSaveDataExt(output_format, output_quant_format, (offset_ouptput + k) * 5 + 4, tmp_output[4], (vx_uint8_ptr)out_data, output_fp, output_zp, output_scale, TENSOR_ROUNDING_MODE(output));
            }
        }
    }

OnError:
    if (input_stage)
    {
        vxFree(score_data);
        vxFree(bbox_data);
        vxFree(img_data);
        vxFree(anchor_data);
    }

    if (output_stage)
    {
        vx_uint32 output_size = 0;
        vx_ptr output_logical = VX_NULL;
        vxoTensor_GetTensorSize(output, &output_size);
        vxoTensor_GetTensorViewMemory(output, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, out_data, output_size);

        vxFree(out_data);
    }

    return status;
}

vx_status vxnneExecuteSWRPN_Sort(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_sort_operation rpnSortOperation = (vxnne_tensor_rpn_sort_operation)operation;
    //vx_uint32 w;

    vx_tensor proposals     = rpnSortOperation->proposal;
    vx_uint32 pre_nms_topn  = rpnSortOperation->pre_nms_topn->value->u32;

    vx_bool output_stage    = rpnSortOperation->output_stage;

    //vx_uint32 proposal_count = TENSOR_VIEW_SIZE_INDEX(proposals, 3);
    vx_uint32 proposal_count =  TENSOR_VIEW_SIZE_INDEX(proposals, 0) * TENSOR_VIEW_SIZE_INDEX(proposals, 1)*
                                TENSOR_VIEW_SIZE_INDEX(proposals, 2) * TENSOR_VIEW_SIZE_INDEX(proposals, 3)/5;
    vx_type_e proposals_data_format   = (vx_type_e)TENSOR_DATA_TYPE(proposals);
    vx_uint8_ptr proposals_data = NULL;
    vxnneGetTensorMemeory(proposals, (vx_ptr_ptr)&proposals_data, output_stage, vx_false_e);

    if (proposals_data_format == VX_TYPE_FLOAT32){
        vx_float32_ptr proposals_ptr = (vx_float32_ptr)proposals_data;
        vx_nn_rpn_qsort_box(proposals_ptr, 0, proposal_count-1, pre_nms_topn);
    }
    else if (proposals_data_format == VX_TYPE_FLOAT16)
    {
        vx_int16_ptr proposals_ptr = (vx_int16_ptr)proposals_data;

        // xzq test

        vx_nn_rpn_qsort_box_fp16(proposals_ptr, 0, proposal_count-1, pre_nms_topn);
    }
    else
    {
        // only surpported F32 and F16 data type
        // todo...
        status = VX_ERROR_INVALID_FORMAT;
        vxError("Not support format %d", proposals_data_format);
    }

    if(output_stage)
    {
        vx_uint32 output_size = 0;
        vx_ptr output_logical = VX_NULL;
        vxoTensor_GetTensorSize(proposals, &output_size);
        vxoTensor_GetTensorViewMemory(proposals, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, proposals_data, output_size);

        vxFree(proposals_data);
    }

    return status;
}

vx_status vxnneExecuteSWRPN_NMS(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_nms_operation rpnNmsOperation = (vxnne_tensor_rpn_nms_operation)operation;

    vx_tensor proposal      = rpnNmsOperation->proposal;
    vx_tensor roi_indices   = rpnNmsOperation->roi_indices;
    vx_scalar real_roi_t    = rpnNmsOperation->real_roi_t;
    vx_uint32 pre_nms_topn  = rpnNmsOperation->pre_nms_topn->value->u32;
    vx_uint32 post_nms_topn = rpnNmsOperation->post_nms_topn->value->u32;
    vx_float32 nms_thresh   = rpnNmsOperation->nms_thresh->value->f32;
    vx_bool output_stage    = rpnNmsOperation->output_stage;

    vx_uint32 roi_count         = TENSOR_VIEW_SIZE_INDEX(roi_indices, 0);
    vx_type_e roi_ind_format    = (vx_type_e)TENSOR_DATA_TYPE(roi_indices);
    vx_int8 roi_ind_fp          = TENSOR_POS(roi_indices);
    vx_enum roi_ind_rMode       = TENSOR_ROUNDING_MODE(roi_indices);

    vx_uint8_ptr proposals_data = NULL, roi_indices_data = NULL;

    vx_uint32_ptr roi_indices_ptr = NULL;
    vx_uint32 i,real_roi = 0;
    vx_type_e proposal_data_format   = (vx_type_e)TENSOR_DATA_TYPE(proposal);
    vx_uint32 proposal_count =  TENSOR_VIEW_SIZE_INDEX(proposal, 0) * TENSOR_VIEW_SIZE_INDEX(proposal, 1)*
                                TENSOR_VIEW_SIZE_INDEX(proposal, 2) * TENSOR_VIEW_SIZE_INDEX(proposal, 3)/5;

    roi_indices_ptr = (vx_uint32_ptr)vxAllocateAndZeroMemory(roi_count * sizeof(vx_uint32));

    gcoOS_MemFill(roi_indices_ptr, 0, roi_count * sizeof(vx_uint32));
    vxnneGetTensorMemeory(proposal, (vx_ptr_ptr)&proposals_data, output_stage, vx_false_e);
    vxnneGetTensorMemeory(roi_indices, (vx_ptr_ptr)&roi_indices_data, output_stage, vx_true_e);

    if(pre_nms_topn>proposal_count)
        pre_nms_topn = proposal_count;

    if (proposal_data_format == VX_TYPE_FLOAT32){
        vx_float32_ptr proposals_ptr = (vx_float32_ptr)proposals_data;
        vx_nn_rpn_nms_cpu(pre_nms_topn, proposals_ptr, roi_indices_ptr, &real_roi, 0, nms_thresh, post_nms_topn);
    }
    else if (proposal_data_format == VX_TYPE_FLOAT16){
        vx_int16_ptr proposals_ptr = (vx_int16_ptr)proposals_data;
        vx_nn_rpn_nms_cpu_f16(pre_nms_topn, proposals_ptr, roi_indices_ptr, &real_roi, 0, nms_thresh, post_nms_topn);
    }
    else{
        // only surpported F32 and F16 data type
        // todo...
        status = VX_ERROR_INVALID_FORMAT;
        vxError("Not support format %d", proposal_data_format);
    }
    real_roi_t->value->u32 = real_roi;

    for(i = 0; i < roi_count; i++)
    {
        vxnneSaveDataExt(roi_ind_format, TENSOR_QUANT_TYPE(roi_indices), i, roi_indices_ptr[i], roi_indices_data, roi_ind_fp, TENSOR_TF_ZEROPOINT(roi_indices), TENSOR_TF_SCALE(roi_indices), roi_ind_rMode);
    }

    if(roi_indices_ptr)
        vxFree(roi_indices_ptr);

    if(output_stage)
    {
        vx_uint32 output_size = 0;
        vx_ptr output_logical = VX_NULL;

        vxoTensor_GetTensorSize(proposal, &output_size);
        vxoTensor_GetTensorViewMemory(proposal, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, proposals_data, output_size);

        vxoTensor_GetTensorSize(roi_indices, &output_size);
        vxoTensor_GetTensorViewMemory(roi_indices, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, roi_indices_data, output_size);

        vxFree(proposals_data);
        vxFree(roi_indices_data);
    }

    return status;
}

vx_status vxnneExecuteSWRPN_Retrieve(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_retrieve_operation rpnRetOperation = (vxnne_tensor_rpn_retrieve_operation)operation;

    vx_tensor proposal      = rpnRetOperation->proposal;
    vx_tensor roi_indices   = rpnRetOperation->roi_indices;
    vx_scalar real_roi_t    = rpnRetOperation->real_roi_t;
    vx_tensor roi_output    = rpnRetOperation->roi_output;
    vx_tensor score_output  = rpnRetOperation->score_output;

    vx_bool input_stage     = rpnRetOperation->input_stage;
    vx_bool output_stage    = rpnRetOperation->output_stage;

    vx_type_e roi_out_format   = (vx_type_e)TENSOR_DATA_TYPE(roi_output);
    vx_int8 roi_out_fp         = TENSOR_POS(roi_output);
    vx_enum roi_out_rMode      = TENSOR_ROUNDING_MODE(roi_output);
    vx_enum roi_out_quant_format = TENSOR_QUANT_TYPE(roi_output);
    vx_int32 roi_out_zp = TENSOR_TF_ZEROPOINT(roi_output);
    vx_float32 roi_out_scale = TENSOR_TF_SCALE(roi_output);

    vx_uint8_ptr proposal_data = NULL, roi_indices_data = NULL;
    vx_uint8_ptr roi_output_data = NULL;
    vx_float32_ptr roi_indices_ptr = NULL;
    vx_uint32 i,real_roi = 0;

    vx_type_e out_score_format = 0;
    vx_int8 out_score_fp = 0;
    vx_uint8_ptr out_score_data = NULL;
    vx_enum out_score_rMode = 0;
    vx_enum out_score_quant_format = 0;
    vx_int32 out_score_zp = 0;
    vx_float32 out_score_scale = 1.0f;

    vx_type_e proposal_data_format   = (vx_type_e)TENSOR_DATA_TYPE(proposal);
    vxnneGetTensorMemeory(proposal, (vx_ptr_ptr)&proposal_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(roi_indices, (vx_ptr_ptr)&roi_indices_data, input_stage, vx_false_e);
    vxnneGetTensorMemeory(roi_output, (vx_ptr_ptr)&roi_output_data, output_stage, vx_true_e);

    roi_indices_ptr = (vx_float32_ptr)roi_indices_data;

    if(score_output)
    {
        out_score_format    = (vx_type_e)TENSOR_DATA_TYPE(score_output);
        out_score_fp        = TENSOR_POS(score_output);
        out_score_rMode     = TENSOR_ROUNDING_MODE(score_output);
        vxnneGetTensorMemeory(score_output, (vx_ptr_ptr)&out_score_data, output_stage, vx_true_e);

        out_score_quant_format = TENSOR_QUANT_TYPE(score_output);
        out_score_zp = TENSOR_TF_ZEROPOINT(score_output);
        out_score_scale = TENSOR_TF_SCALE(score_output);
    }


    real_roi = real_roi_t->value->u32;

    for(i = 0; i < real_roi; i++)
    {
        vx_float32 output[5];
        vx_float32 item_index = 0.0f; /* item_index = input score batch, but we only supported single batch */
        vx_float32 findex = roi_indices_ptr[i];
        vx_uint32 index = (vx_uint32)findex;
        if (proposal_data_format == VX_TYPE_FLOAT32){
            vx_float32_ptr proposal_ptr = NULL, p_proposal_ptr = NULL;
            proposal_ptr = (vx_float32_ptr)proposal_data;
            p_proposal_ptr = proposal_ptr + index * 5;

            output[0]= p_proposal_ptr[0];
            output[1]= p_proposal_ptr[1];
            output[2]= p_proposal_ptr[2];
            output[3]= p_proposal_ptr[3];
            output[4]= p_proposal_ptr[4];
        }
        else if (proposal_data_format == VX_TYPE_FLOAT16){
            vx_int16_ptr proposal_ptr = NULL, p_proposal_ptr = NULL;
            proposal_ptr = (vx_int16_ptr)proposal_data;
            p_proposal_ptr = proposal_ptr + index * 5;

            output[0]= Fp16toFp32(p_proposal_ptr[0]);
            output[1]= Fp16toFp32(p_proposal_ptr[1]);
            output[2]= Fp16toFp32(p_proposal_ptr[2]);
            output[3]= Fp16toFp32(p_proposal_ptr[3]);
            output[4]= Fp16toFp32(p_proposal_ptr[4]);
        }
        else{
            // only surpported F32 and F16 data type
            // todo...
            status = VX_ERROR_INVALID_FORMAT;
            vxError("Not support format %d", proposal_data_format);
        }


        /* Copy proposals coordinate(x1, y1, x2, y2) to roi output tensor */
        vxnneSaveDataExt(roi_out_format, roi_out_quant_format, (i * 5 + 0), item_index, (vx_uint8_ptr)roi_output_data, roi_out_fp, roi_out_zp, roi_out_scale, roi_out_rMode);
        vxnneSaveDataExt(roi_out_format, roi_out_quant_format, (i * 5 + 1), output[0], (vx_uint8_ptr)roi_output_data, roi_out_fp, roi_out_zp, roi_out_scale, roi_out_rMode);
        vxnneSaveDataExt(roi_out_format, roi_out_quant_format, (i * 5 + 2), output[1], (vx_uint8_ptr)roi_output_data, roi_out_fp, roi_out_zp, roi_out_scale, roi_out_rMode);
        vxnneSaveDataExt(roi_out_format, roi_out_quant_format, (i * 5 + 3), output[2], (vx_uint8_ptr)roi_output_data, roi_out_fp, roi_out_zp, roi_out_scale, roi_out_rMode);
        vxnneSaveDataExt(roi_out_format, roi_out_quant_format, (i * 5 + 4), output[3], (vx_uint8_ptr)roi_output_data, roi_out_fp, roi_out_zp, roi_out_scale, roi_out_rMode);

        /* Copy proposals score to score output tensor */
        if(score_output)
        {
            vxnneSaveDataExt(out_score_format, out_score_quant_format, i, output[4], (vx_uint8_ptr)out_score_data, out_score_fp, out_score_zp, out_score_scale, out_score_rMode);
        }

    }

    if (input_stage)
    {
        vxFree(proposal_data);
        vxFree(roi_indices_data);
    }

    if (output_stage)
    {
        vx_uint32 roi_output_size = 0;
        vx_ptr roi_output_logical = VX_NULL;
        vxoTensor_GetTensorSize(roi_output, &roi_output_size);
        vxoTensor_GetTensorViewMemory(roi_output, &roi_output_logical, VX_NULL);
        gcoOS_MemCopy(roi_output_logical, roi_output_data, roi_output_size);

        vxFree(roi_output_data);
    }

    if(score_output && output_stage == vx_true_e)
    {
        vx_uint32 score_output_size = 0;
        vx_ptr score_output_logical = VX_NULL;
        vxoTensor_GetTensorSize(score_output, &score_output_size);
        vxoTensor_GetTensorViewMemory(score_output, &score_output_logical, VX_NULL);
        gcoOS_MemCopy(score_output_logical, out_score_data, score_output_size);
    }

    if (out_score_data)
    {
        vxFree(out_score_data);
        out_score_data = VX_NULL;
    }
    return status;
}

vx_status vxnneExecuteSWRPN_SortNMS(struct _vxnne_operation_s *operation)
{
    vx_status  status = VX_SUCCESS;
    vxnne_tensor_rpn_sort_nms_operation rpnSortNmsOperation = (vxnne_tensor_rpn_sort_nms_operation)operation;

    vx_tensor proposal      = rpnSortNmsOperation->proposal;
    vx_tensor roi_output   = rpnSortNmsOperation->roi_output;
    vx_tensor score_output  = rpnSortNmsOperation->score_output;
    vx_uint32 pre_nms_topn  = rpnSortNmsOperation->pre_nms_topn->value->u32;
    vx_uint32 post_nms_topn = rpnSortNmsOperation->post_nms_topn->value->u32;
    vx_float32 nms_thresh   = rpnSortNmsOperation->nms_thresh->value->f32;
    vx_bool output_stage    = rpnSortNmsOperation->output_stage;

    vx_int8 roi_output_fp          = TENSOR_POS(roi_output);
    vx_enum roi_output_rMode       = TENSOR_ROUNDING_MODE(roi_output);

    vx_uint8_ptr proposals_data = NULL, roi_output_data = NULL,score_output_data = NULL;

    vx_uint32 i,real_roi = 0;
    vx_type_e proposal_data_format   = (vx_type_e)TENSOR_DATA_TYPE(proposal);
    vx_uint32 proposal_count =  TENSOR_VIEW_SIZE_INDEX(proposal, 0) * TENSOR_VIEW_SIZE_INDEX(proposal, 1)*
                                TENSOR_VIEW_SIZE_INDEX(proposal, 2) * TENSOR_VIEW_SIZE_INDEX(proposal, 3)/5;


    vx_float32_ptr proposals_ptr = NULL;

    vxnneGetTensorMemeory(proposal, (vx_ptr_ptr)&proposals_data, output_stage, vx_false_e);
    vxnneGetTensorMemeory(roi_output, (vx_ptr_ptr)&roi_output_data, output_stage, vx_false_e);
    vxnneGetTensorMemeory(score_output,(vx_ptr_ptr)&score_output_data, output_stage, vx_false_e);


    if(pre_nms_topn>proposal_count)
        pre_nms_topn = proposal_count;

    if (proposal_data_format == VX_TYPE_FLOAT32){
        proposals_ptr = (vx_float32_ptr)proposals_data;
        vx_nn_rpn_qsort_box(proposals_ptr, 0, proposal_count-1, pre_nms_topn);
        vx_nn_rpn_nms_cpu(pre_nms_topn, proposals_ptr, NULL, &real_roi, 0, nms_thresh, post_nms_topn);
    }
    else if (proposal_data_format == VX_TYPE_FLOAT16){
        proposals_ptr = (vx_float32_ptr)proposals_data;
        proposal_count = proposal_count/2;
        vx_nn_rpn_qsort_box(proposals_ptr, 0, proposal_count-1, pre_nms_topn);
        vx_nn_rpn_nms_cpu(pre_nms_topn, proposals_ptr, NULL, &real_roi, 0, nms_thresh, post_nms_topn);

    }
    else{
        // only surpported F32 and F16 data type
        // todo...
        status = VX_ERROR_INVALID_FORMAT;
        vxError("Not support format %d", proposal_data_format);
    }

    for(i = 0; i < real_roi; i++)
    {
        vx_int32 index = 5*i;
        vx_float32 * ptr = proposals_ptr+index;
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(roi_output), TENSOR_QUANT_TYPE(roi_output), index+0, 0, (vx_uint8_ptr)roi_output_data, roi_output_fp, TENSOR_TF_ZEROPOINT(roi_output), TENSOR_TF_SCALE(roi_output), roi_output_rMode);
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(roi_output), TENSOR_QUANT_TYPE(roi_output), index+1, ptr[0], (vx_uint8_ptr)roi_output_data, roi_output_fp, TENSOR_TF_ZEROPOINT(roi_output), TENSOR_TF_SCALE(roi_output), roi_output_rMode);
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(roi_output), TENSOR_QUANT_TYPE(roi_output), index+2, ptr[1], (vx_uint8_ptr)roi_output_data, roi_output_fp, TENSOR_TF_ZEROPOINT(roi_output), TENSOR_TF_SCALE(roi_output), roi_output_rMode);
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(roi_output), TENSOR_QUANT_TYPE(roi_output), index+3, ptr[2], (vx_uint8_ptr)roi_output_data, roi_output_fp, TENSOR_TF_ZEROPOINT(roi_output), TENSOR_TF_SCALE(roi_output), roi_output_rMode);
        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(roi_output), TENSOR_QUANT_TYPE(roi_output), index+4, ptr[3], (vx_uint8_ptr)roi_output_data, roi_output_fp, TENSOR_TF_ZEROPOINT(roi_output), TENSOR_TF_SCALE(roi_output), roi_output_rMode);

        if(score_output)
        {
            vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(score_output),
                TENSOR_QUANT_TYPE(score_output), i, ptr[4], (vx_uint8_ptr)score_output_data, TENSOR_POS(score_output),
                TENSOR_TF_ZEROPOINT(score_output), TENSOR_TF_SCALE(score_output), TENSOR_ROUNDING_MODE(score_output));
        }
    }

    if(output_stage)
    {
        vx_uint32 output_size = 0;
        vx_ptr output_logical = VX_NULL;

        vxoTensor_GetTensorSize(proposal, &output_size);
        vxoTensor_GetTensorViewMemory(proposal, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, proposals_data, output_size);

        vxoTensor_GetTensorSize(roi_output, &output_size);
        vxoTensor_GetTensorViewMemory(roi_output, &output_logical, VX_NULL);
        gcoOS_MemCopy(output_logical, roi_output_data, output_size);

        if(score_output)
        {
            vxoTensor_GetTensorSize(score_output, &output_size);
            vxoTensor_GetTensorViewMemory(score_output, &output_logical, VX_NULL);
            gcoOS_MemCopy(output_logical, score_output_data, output_size);
        }
        vxFree(proposals_data);
        vxFree(roi_output_data);
    }

    if (score_output_data)
    {
        vxFree(score_output_data);
        score_output_data = VX_NULL;
    }
    return status;
}
/*SHADER+CPU*/
VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_Initializer_shd_cpu(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  score                      = (vx_tensor)parameters[0];
    vx_tensor  bbox                       = (vx_tensor)parameters[1];
    vx_tensor  anchors                    = (vx_tensor)parameters[2];
    vx_tensor  img_info                   = (vx_tensor)parameters[3];
    vx_scalar  feature_stride             = (vx_scalar)parameters[4];
    vx_scalar  min_size                   = (vx_scalar)parameters[5];
    vx_scalar  pre_nms_topn               = (vx_scalar)parameters[6];
    vx_scalar  post_nms_topn              = (vx_scalar)parameters[7];
    vx_scalar  nms_thresh                 = (vx_scalar)parameters[8];
    vx_tensor  roi_output                 = (vx_tensor)parameters[9];
    vx_tensor  score_output               = (vx_tensor)parameters[10];

    vxnne_tensor_rpn_layer rpnLayer;
    vx_uint32 dims,sizes[4] = {0,0,0,0};
    vx_tensor socreBufferTensor = VX_NULL;
    vx_tensor proposalTensor = VX_NULL;


    vx_enum tmpBuf_format   = VX_TYPE_FLOAT32;
    vx_bool input_stage     = vx_true_e;
    vx_bool output_stage    = vx_true_e;

    vx_enum score_format = TENSOR_DATA_TYPE(score);
    vx_enum bbox_format  = TENSOR_DATA_TYPE(bbox);
    vx_enum anchors_format = TENSOR_DATA_TYPE(anchors);

    vx_bool enable_condition0 = (vx_bool)((score_format   == VX_TYPE_FLOAT16)
                                        && (bbox_format    == VX_TYPE_FLOAT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition1 = (vx_bool)((score_format   == VX_TYPE_INT16)
                                        && (bbox_format    == VX_TYPE_INT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition2 = (vx_bool)((score_format   == VX_TYPE_INT8)
                                        && (bbox_format    == VX_TYPE_INT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition3 = (vx_bool)((score_format   == VX_TYPE_UINT8)
                                        && (bbox_format    == VX_TYPE_UINT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));


    vx_bool enable_gpu_soft_max   = (vx_bool)(enable_condition0 || enable_condition1 || enable_condition2 || enable_condition3);
    vx_bool enable_gpu_regression = enable_gpu_soft_max;

    vx_uint32 batchCount = TENSOR_SIZE_INDEX(score, 3);
    vx_tensor_create_params_t tensor_create_params;
    vxnne_operation softmax_op, regression_op, sort_nms_op;

    if(enable_gpu_soft_max)
        tmpBuf_format = VX_TYPE_FLOAT16;
    else
        tmpBuf_format = VX_TYPE_FLOAT32;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        rpnLayer = VX_NULL;
        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_rpn_layer_s), (gctPOINTER*)&rpnLayer);
        if (!rpnLayer)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }
        gcoOS_ZeroMemory(rpnLayer, sizeof(vxnne_tensor_rpn_layer_s));
        vxnneLayer_Initialize(&rpnLayer->base,
                                "RpnLayer",
                                node,
                                vxmOPERATION_COUNT(rpnLayer),
                                rpnLayer->operations,
                                VX_NULL);

        /* -----------RPN Softmax------------ */
        /* create a temp tensor to store the scores. */
        dims        = TENSOR_DIM_NUM(score);
        sizes[0]    = TENSOR_SIZE_INDEX(score, 0);
        sizes[1]    = TENSOR_SIZE_INDEX(score, 1);
        sizes[2]    = TENSOR_SIZE_INDEX(score, 2);
        sizes[3]    = TENSOR_SIZE_INDEX(score, 3);

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = tmpBuf_format;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;

        socreBufferTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (socreBufferTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        // 1. gpu softmax process
        if (enable_gpu_soft_max && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            // reshap tensor objs from dim(4) to dim(3)
            vx_tensor rs_score = NULL, rs_socreBufferTensor = NULL;
            vx_bool rs_score_flag               = (vx_bool)(score->dimCount             == 4);
            vx_bool rs_socreBufferTensor_flag   = (vx_bool)(socreBufferTensor->dimCount == 4);
            if (rs_score_flag){
                vx_int32 new_size[6] = {score->dims[0], score->dims[1],
                                        score->dims[2] * score->dims[3], 1, 1, 1};
                rs_score              = vxoTensor_ReshapeTensor(score, new_size, 3);
            }
            if (rs_socreBufferTensor_flag){
                vx_int32 new_size[6] = {socreBufferTensor->dims[0], socreBufferTensor->dims[1],
                                        socreBufferTensor->dims[2] * socreBufferTensor->dims[3], 1, 1, 1};
                rs_socreBufferTensor = vxoTensor_ReshapeTensor(socreBufferTensor, new_size, 3);
            }
            // --end reshape
            shaderExecutable =
                vxnneRPNSoftMaxShaderExecutable(node->base.context, VXNNE_KERNEL_RPN_SOFTMAX, &node->kernelAttributes.borderMode,
                                                rs_score_flag               ? rs_score              : score,
                                                rs_socreBufferTensor_flag   ? rs_socreBufferTensor  : socreBufferTensor);

            if(rs_score_flag && (NULL != rs_score)){
                vxoTensor_ReleaseTensor(&rs_score);
                rs_score = NULL;
            }
            if(rs_socreBufferTensor_flag && (NULL != rs_socreBufferTensor)){
                vxoTensor_ReleaseTensor(&rs_socreBufferTensor);
                rs_socreBufferTensor = NULL;
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&rpnLayer->tensorRpnSoftmaxSH, &rpnLayer->base,
                                                     VXNNE_OPERATOR_RPN_SOFTMAX, batchCount, shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSH.base, (vx_reference)score, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSH.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnSoftmaxSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_SOFTMAX,
                                        vxnneExecuteSWRPN_Softmax,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnSoftmaxSW.input          = score;
            rpnLayer->tensorRpnSoftmaxSW.output         = socreBufferTensor;

            rpnLayer->tensorRpnSoftmaxSW.input_stage    = input_stage;
            rpnLayer->tensorRpnSoftmaxSW.output_stage   = output_stage;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSW.base, (vx_reference)score, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSW.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }

        /* -----------RPN bbox regression------------- */
        /*
            create a temp tensor to store the proposal buffer
            proposal buffer: N*5*1*1
                N: score_width*score_height*anchor_number
                5: 2 sets of coordinates + scores --- (x1, y1, x2, y2, score)
        */
        dims = 3;
 //     sizes[0]    = TENSOR_SIZE_INDEX(score, 0)* 5;
        sizes[0]    = TENSOR_SIZE_INDEX(score, 0)* 5*2;
        sizes[1]    = TENSOR_SIZE_INDEX(score, 1);
        sizes[2]    = TENSOR_SIZE_INDEX(anchors, 3);

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = tmpBuf_format;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;

        proposalTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (proposalTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }
        if (enable_gpu_regression && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_type_e in_img_format     = (vx_type_e)TENSOR_DATA_TYPE(img_info);
            vx_int8 in_img_fp           = TENSOR_POS(img_info);
            vx_uint32 min_size_v    = min_size->value->u32;
            vx_uint32 i;
            vx_scalar *pScalarObj = NULL;
            vx_float32 img_W, img_H, min_box_W, min_box_H;

            vx_uint8_ptr img_data;
            vx_float32 img_scale_H,img_scale_W;

            vx_tensor rs_socreBufferTensor = NULL, rs_bbox = NULL, rs_anchors = NULL;
            vx_bool rs_anchors_flag             = (vx_bool)(anchors->dimCount == 4);
            vx_bool rs_box_flag                 = (vx_bool)(bbox->dimCount == 4);
            vx_bool rs_socreBufferTensor_flag   = (vx_bool)(socreBufferTensor->dimCount == 4);

            // reshap tensor objs from dim(4) to dim(3)
            if (rs_socreBufferTensor_flag){
                vx_int32 new_size[6] = {socreBufferTensor->dims[0], socreBufferTensor->dims[1], socreBufferTensor->dims[2] * socreBufferTensor->dims[3], 1, 1, 1};
                rs_socreBufferTensor = vxoTensor_ReshapeTensor(socreBufferTensor, new_size, 3);
            }
            if (rs_box_flag){
                vx_int32 new_size[6] = {bbox->dims[0], bbox->dims[1], bbox->dims[2] * bbox->dims[3], 1, 1, 1};
                rs_bbox = vxoTensor_ReshapeTensor(bbox, new_size, 3);
            }
            if (rs_anchors_flag){

                vx_int32 new_size[6] = {anchors->dims[0]*anchors->dims[1]*anchors->dims[2] * anchors->dims[3], 1, 1, 1, 1, 1};
                rs_anchors = vxoTensor_ReshapeTensor(anchors, new_size, 3);
            }
            // get img_info as input paramerters
            vxnneGetTensorMemeory(img_info, (vx_ptr_ptr)&img_data, vx_false_e, vx_false_e);
            img_W   = (vx_float32)vxnneGetData(in_img_format, 0, (vx_uint8_ptr)img_data, in_img_fp);
            img_H   = (vx_float32)vxnneGetData(in_img_format, 1, (vx_uint8_ptr)img_data, in_img_fp);
            img_scale_W         = (vx_float32)vxnneGetData(in_img_format, 2, (vx_uint8_ptr)img_data, in_img_fp);
            img_scale_H         = (vx_float32)vxnneGetData(in_img_format, 3, (vx_uint8_ptr)img_data, in_img_fp);
            min_box_W   = min_size_v * img_scale_W;
            min_box_H   = min_size_v * img_scale_H;

            pScalarObj = (vx_scalar *)calloc(4, sizeof(vx_scalar));
            pScalarObj[0] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &img_W);
            pScalarObj[1] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &img_H);
            pScalarObj[2] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &min_box_W);
            pScalarObj[3] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &min_box_H);

            shaderExecutable =
                vxnneRPNRegressionShaderExecutable(node->base.context, VXNNE_KERNEL_RPN_REGRESSION, &node->kernelAttributes.borderMode,
                                                rs_socreBufferTensor_flag   ? rs_socreBufferTensor  : socreBufferTensor,
                                                rs_box_flag ? rs_bbox : bbox,
                                                rs_anchors_flag ? rs_anchors : anchors,
                                                proposalTensor,
                                                feature_stride, pScalarObj[0],pScalarObj[1],pScalarObj[2],pScalarObj[3]);

            if(rs_socreBufferTensor_flag && (NULL != rs_socreBufferTensor)){
                vxoTensor_ReleaseTensor(&rs_socreBufferTensor);
                rs_socreBufferTensor = NULL;
            }
            if(rs_box_flag && (NULL != rs_bbox)){
                vxoTensor_ReleaseTensor(&rs_bbox);
                rs_bbox = NULL;
            }
            if(rs_anchors_flag && (NULL != rs_anchors)){
                vxoTensor_ReleaseTensor(&rs_anchors);
                rs_anchors = NULL;
            }

            if (NULL != pScalarObj){
                for(i=0; i< 4; i++){
                    if(NULL != (pScalarObj)[i])
                        vxReleaseScalar(&((pScalarObj)[i]));
                }
                vxFree(pScalarObj);
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(
                &rpnLayer->tensorRpnRegressionSH,
                &rpnLayer->base,
                VXNNE_OPERATOR_RPN_REGRESSION,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)bbox, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)anchors, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)img_info, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnRegressionSW.base,
                &rpnLayer->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_RPN_REGRESSION,
                vxnneExecuteSWRPN_Regression,
                VX_NULL,
                batchCount,
                0);

            rpnLayer->tensorRpnRegressionSW.feature_stride  = feature_stride;
            rpnLayer->tensorRpnRegressionSW.min_size        = min_size;
            rpnLayer->tensorRpnRegressionSW.score_buffer    = socreBufferTensor;
            rpnLayer->tensorRpnRegressionSW.bbox            = bbox;
            rpnLayer->tensorRpnRegressionSW.anchors         = anchors;
            rpnLayer->tensorRpnRegressionSW.img_info        = img_info;
            rpnLayer->tensorRpnRegressionSW.output          = proposalTensor;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)bbox, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)anchors, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)img_info, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnRegressionSW.input_stage     = input_stage;
            rpnLayer->tensorRpnRegressionSW.output_stage    = output_stage;
        }

// sort and nms
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnSortNmsSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_SORT_NMS,
                                        vxnneExecuteSWRPN_SortNMS,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnSortNmsSW.pre_nms_topn   = pre_nms_topn;
            rpnLayer->tensorRpnSortNmsSW.post_nms_topn  = post_nms_topn;
            rpnLayer->tensorRpnSortNmsSW.nms_thresh     = nms_thresh;
            rpnLayer->tensorRpnSortNmsSW.proposal       = proposalTensor;
            rpnLayer->tensorRpnSortNmsSW.roi_output    = roi_output;
            rpnLayer->tensorRpnSortNmsSW.score_output     = score_output;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSortNmsSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSortNmsSW.base, (vx_reference)roi_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSortNmsSW.base, (vx_reference)score_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnSortNmsSW.input_stage     = input_stage;
            rpnLayer->tensorRpnSortNmsSW.output_stage    = output_stage;
        }


        rpnLayer->base.num_temp_tensors     = 2;
        rpnLayer->base.temp_tensors[0]      = socreBufferTensor;
        rpnLayer->base.temp_tensors[1]      = proposalTensor;

        if (enable_gpu_soft_max&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            softmax_op         = (vxnne_operation)&rpnLayer->tensorRpnSoftmaxSH.base;
        else
            softmax_op         = (vxnne_operation)&rpnLayer->tensorRpnSoftmaxSW.base;

        if (enable_gpu_regression&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            regression_op       = (vxnne_operation)&rpnLayer->tensorRpnRegressionSH.base;
        else
            regression_op       = (vxnne_operation)&rpnLayer->tensorRpnRegressionSW.base;

         sort_nms_op         = (vxnne_operation)&rpnLayer->tensorRpnSortNmsSW.base;

        vxnneLayer_SetOperation(
            &rpnLayer->base,
            softmax_op,
            0);
        vxnneLayer_SetOperation(
            &rpnLayer->base,
            regression_op,
            1);

        vxnneLayer_SetOperation(
            &rpnLayer->base,
            sort_nms_op,
            2);


        node->layer = &rpnLayer->base;
        return VX_SUCCESS;
    }

exit:
    if (rpnLayer)
    {
        gcoOS_Free(gcvNULL, rpnLayer);
        rpnLayer = VX_NULL;
    }
    return status;
}

/*RPN - ALL SHADER*/
VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_Initializer_shd(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status  status                     = VX_SUCCESS;

    vx_tensor  score                      = (vx_tensor)parameters[0];
    vx_tensor  bbox                       = (vx_tensor)parameters[1];
    vx_tensor  anchors                    = (vx_tensor)parameters[2];
    vx_tensor  img_info                   = (vx_tensor)parameters[3];
    vx_scalar  feature_stride             = (vx_scalar)parameters[4];
    vx_scalar  min_size                   = (vx_scalar)parameters[5];
    vx_scalar  pre_nms_topn               = (vx_scalar)parameters[6];
    vx_scalar  post_nms_topn              = (vx_scalar)parameters[7];
    vx_scalar  nms_thresh                 = (vx_scalar)parameters[8];
    vx_tensor  roi_output                 = (vx_tensor)parameters[9];
    vx_tensor  score_output               = (vx_tensor)parameters[10];

    vxnne_tensor_rpn_layer rpnLayer;
    vx_uint32 dims,sizes[4] = {0,0,0,0};
    vx_tensor socreBufferTensor = VX_NULL;
    vx_tensor proposalTensor = VX_NULL;
    vx_tensor roiIndicesTensor = VX_NULL;
    vx_scalar realRoiScalar = VX_NULL;
    vx_uint32 realRoi = 0;

    vx_enum temp_format     = VX_TYPE_FLOAT32; /* To avoid the loss of accuracy */
    vx_enum tmpBuf_format   = VX_TYPE_FLOAT16;
    vx_bool input_stage     = vx_true_e;
    vx_bool output_stage    = vx_true_e;

    vx_enum score_format = TENSOR_DATA_TYPE(score);
    vx_enum bbox_format  = TENSOR_DATA_TYPE(bbox);
    vx_enum anchors_format = TENSOR_DATA_TYPE(anchors);

    vx_bool enable_condition0 = (vx_bool)((score_format   == VX_TYPE_FLOAT16)
                                        && (bbox_format    == VX_TYPE_FLOAT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition1 = (vx_bool)((score_format   == VX_TYPE_INT8)
                                        && (bbox_format    == VX_TYPE_INT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition2 = (vx_bool)((score_format   == VX_TYPE_UINT8)
                                        && (bbox_format    == VX_TYPE_UINT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition3 = (vx_bool)((score_format   == VX_TYPE_INT16)
                                        && (bbox_format    == VX_TYPE_INT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_gpu_soft_max   = (vx_bool)(enable_condition0 || enable_condition1 || enable_condition2 || enable_condition3);
    vx_bool enable_gpu_regression = enable_gpu_soft_max  ;
    vx_bool enable_gpu_nms = vx_false_e;
    vx_bool enable_gpu_retrive = vx_false_e;
    vx_bool enable_gpu_sort = vx_false_e;
    vx_uint32 batchCount = TENSOR_SIZE_INDEX(score, 3);
    vx_tensor_create_params_t tensor_create_params;
    vxnne_operation softmax_op, regression_op, sort_op, nms_op, retrieve_op;


    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        rpnLayer = VX_NULL;
        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_rpn_layer_s), (gctPOINTER*)&rpnLayer);
        if (!rpnLayer)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }
        gcoOS_ZeroMemory(rpnLayer, sizeof(vxnne_tensor_rpn_layer_s));
        vxnneLayer_Initialize(&rpnLayer->base,
                                "RpnLayer",
                                node,
                                vxmOPERATION_COUNT(rpnLayer),
                                rpnLayer->operations,
                                VX_NULL);

        /* -----------RPN Softmax------------ */
        /* create a temp tensor to store the scores. */
        dims        = TENSOR_DIM_NUM(score);
        sizes[0]    = TENSOR_SIZE_INDEX(score, 0);
        sizes[1]    = TENSOR_SIZE_INDEX(score, 1);
        sizes[2]    = TENSOR_SIZE_INDEX(score, 2);
        sizes[3]    = TENSOR_SIZE_INDEX(score, 3);

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = tmpBuf_format;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;

        socreBufferTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (socreBufferTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        // 1. gpu softmax process
        if (enable_gpu_soft_max && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            // reshap tensor objs from dim(4) to dim(3)
            vx_tensor rs_score = NULL, rs_socreBufferTensor = NULL;
            vx_bool rs_score_flag               = (vx_bool)(score->dimCount             == 4);
            vx_bool rs_socreBufferTensor_flag   = (vx_bool)(socreBufferTensor->dimCount == 4);
            if (rs_score_flag){
                vx_int32 new_size[6] = {score->dims[0], score->dims[1],
                                        score->dims[2] * score->dims[3], 1, 1, 1};
                rs_score              = vxoTensor_ReshapeTensor(score, new_size, 3);
            }
            if (rs_socreBufferTensor_flag){
                vx_int32 new_size[6] = {socreBufferTensor->dims[0], socreBufferTensor->dims[1],
                                        socreBufferTensor->dims[2] * socreBufferTensor->dims[3], 1, 1, 1};
                rs_socreBufferTensor = vxoTensor_ReshapeTensor(socreBufferTensor, new_size, 3);
            }
            // --end reshape
            shaderExecutable =
                vxnneRPNSoftMaxShaderExecutable(node->base.context, VXNNE_KERNEL_RPN_SOFTMAX, &node->kernelAttributes.borderMode,
                                                rs_score_flag               ? rs_score              : score,
                                                rs_socreBufferTensor_flag   ? rs_socreBufferTensor  : socreBufferTensor);

            if(rs_score_flag && (NULL != rs_score)){
                vxoTensor_ReleaseTensor(&rs_score);
                rs_score = NULL;
            }
            if(rs_socreBufferTensor_flag && (NULL != rs_socreBufferTensor)){
                vxoTensor_ReleaseTensor(&rs_socreBufferTensor);
                rs_socreBufferTensor = NULL;
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&rpnLayer->tensorRpnSoftmaxSH, &rpnLayer->base,
                                                     VXNNE_OPERATOR_RPN_SOFTMAX, batchCount, shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSH.base, (vx_reference)score, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSH.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnSoftmaxSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_SOFTMAX,
                                        vxnneExecuteSWRPN_Softmax,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnSoftmaxSW.input          = score;
            rpnLayer->tensorRpnSoftmaxSW.output         = socreBufferTensor;

            rpnLayer->tensorRpnSoftmaxSW.input_stage    = input_stage;
            rpnLayer->tensorRpnSoftmaxSW.output_stage   = output_stage;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSW.base, (vx_reference)score, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnSoftmaxSW.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }

        /* -----------RPN bbox regression------------- */
        /*
            create a temp tensor to store the proposal buffer
            proposal buffer: N*5*1*1
                N: score_width*score_height*anchor_number
                5: 2 sets of coordinates + scores --- (x1, y1, x2, y2, score)
        */
        dims = 3;
        sizes[0]    = TENSOR_SIZE_INDEX(score, 0)* 5;
        sizes[1]    = TENSOR_SIZE_INDEX(score, 1);
        sizes[2]    = TENSOR_SIZE_INDEX(anchors, 3);

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = tmpBuf_format;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;

        proposalTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (proposalTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }
        if (enable_gpu_regression && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_type_e in_img_format     = (vx_type_e)TENSOR_DATA_TYPE(img_info);
            vx_int8 in_img_fp           = TENSOR_POS(img_info);
            vx_uint32 min_size_v    = min_size->value->u32;
            vx_uint32 i;
            vx_scalar *pScalarObj = NULL;
            vx_float32 img_W, img_H, min_box_W, min_box_H;

            vx_uint8_ptr img_data;
            vx_float32 img_scale_H,img_scale_W;

            vx_tensor rs_socreBufferTensor = NULL, rs_bbox = NULL, rs_anchors = NULL;
            vx_bool rs_anchors_flag             = (vx_bool)(anchors->dimCount == 4);
            vx_bool rs_box_flag                 = (vx_bool)(bbox->dimCount == 4);
            vx_bool rs_socreBufferTensor_flag   = (vx_bool)(socreBufferTensor->dimCount == 4);

            // reshap tensor objs from dim(4) to dim(3)
            if (rs_socreBufferTensor_flag){
                vx_int32 new_size[6] = {socreBufferTensor->dims[0], socreBufferTensor->dims[1], socreBufferTensor->dims[2] * socreBufferTensor->dims[3], 1, 1, 1};
                rs_socreBufferTensor = vxoTensor_ReshapeTensor(socreBufferTensor, new_size, 3);
            }
            if (rs_box_flag){
                vx_int32 new_size[6] = {bbox->dims[0], bbox->dims[1], bbox->dims[2] * bbox->dims[3], 1, 1, 1};
                rs_bbox = vxoTensor_ReshapeTensor(bbox, new_size, 3);
            }
            if (rs_anchors_flag){

                vx_int32 new_size[6] = {anchors->dims[0]*anchors->dims[1]*anchors->dims[2] * anchors->dims[3], 1, 1, 1, 1, 1};
                rs_anchors = vxoTensor_ReshapeTensor(anchors, new_size, 3);

            }
            // get img_info as input paramerters
            vxnneGetTensorMemeory(img_info, (vx_ptr_ptr)&img_data, vx_false_e, vx_false_e);
            img_W   = (vx_float32)vxnneGetData(in_img_format, 0, (vx_uint8_ptr)img_data, in_img_fp);
            img_H   = (vx_float32)vxnneGetData(in_img_format, 1, (vx_uint8_ptr)img_data, in_img_fp);
            img_scale_W         = (vx_float32)vxnneGetData(in_img_format, 2, (vx_uint8_ptr)img_data, in_img_fp);
            img_scale_H         = (vx_float32)vxnneGetData(in_img_format, 3, (vx_uint8_ptr)img_data, in_img_fp);
            min_box_W   = min_size_v * img_scale_W;
            min_box_H   = min_size_v * img_scale_H;

            pScalarObj = (vx_scalar *)calloc(4, sizeof(vx_scalar));
            pScalarObj[0] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &img_W);
            pScalarObj[1] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &img_H);
            pScalarObj[2] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &min_box_W);
            pScalarObj[3] = vxCreateScalar(node->base.context, VX_TYPE_FLOAT32, &min_box_H);

            shaderExecutable =
                vxnneRPNRegressionShaderExecutable(node->base.context, VXNNE_KERNEL_RPN_REGRESSION, &node->kernelAttributes.borderMode,
                                                rs_socreBufferTensor_flag   ? rs_socreBufferTensor  : socreBufferTensor,
                                                rs_box_flag ? rs_bbox : bbox,
                                                rs_anchors_flag ? rs_anchors : anchors,
                                                proposalTensor,
                                                feature_stride, pScalarObj[0],pScalarObj[1],pScalarObj[2],pScalarObj[3]);

            if(rs_socreBufferTensor_flag && (NULL != rs_socreBufferTensor)){
                vxoTensor_ReleaseTensor(&rs_socreBufferTensor);
                rs_socreBufferTensor = NULL;
            }
            if(rs_box_flag && (NULL != rs_bbox)){
                vxoTensor_ReleaseTensor(&rs_bbox);
                rs_bbox = NULL;
            }
            if(rs_anchors_flag && (NULL != rs_anchors)){
                vxoTensor_ReleaseTensor(&rs_anchors);
                rs_anchors = NULL;
            }

            if (NULL != pScalarObj){
                for(i=0; i< 4; i++){
                    if(NULL != (pScalarObj)[i])
                        vxReleaseScalar(&((pScalarObj)[i]));
                }
                vxFree(pScalarObj);
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(
                &rpnLayer->tensorRpnRegressionSH,
                &rpnLayer->base,
                VXNNE_OPERATOR_RPN_REGRESSION,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)bbox, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)anchors, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)img_info, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSH.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnRegressionSW.base,
                &rpnLayer->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_RPN_REGRESSION,
                vxnneExecuteSWRPN_Regression,
                VX_NULL,
                batchCount,
                0);

            rpnLayer->tensorRpnRegressionSW.feature_stride  = feature_stride;
            rpnLayer->tensorRpnRegressionSW.min_size        = min_size;
            rpnLayer->tensorRpnRegressionSW.score_buffer    = socreBufferTensor;
            rpnLayer->tensorRpnRegressionSW.bbox            = bbox;
            rpnLayer->tensorRpnRegressionSW.anchors         = anchors;
            rpnLayer->tensorRpnRegressionSW.img_info        = img_info;
            rpnLayer->tensorRpnRegressionSW.output          = proposalTensor;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)socreBufferTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)bbox, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)anchors, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)img_info, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRegressionSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnRegressionSW.input_stage     = input_stage;
            rpnLayer->tensorRpnRegressionSW.output_stage    = output_stage;
        }

        /* ------------RPN Sort--------------------- */
        if (enable_gpu_sort == vx_false_e)
        {
            enable_gpu_sort = vx_true_e;
            if (TENSOR_DATA_TYPE(proposalTensor) != VX_TYPE_FLOAT16)
                enable_gpu_sort = vx_false_e;
        }

        if (enable_gpu_sort && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_tensor rs_proposalTensor = NULL;
            if (proposalTensor != NULL)
            {
                vx_int32 len = proposalTensor->dims[0] * proposalTensor->dims[1] * proposalTensor->dims[2] * proposalTensor->dims[3];
                vx_int32 new_size[6] = {5, len/5, 1, 1, 1, 1};
                rs_proposalTensor = vxoTensor_ReshapeTensor(proposalTensor, new_size, 3);
            }
            shaderExecutable =  vxnneRPNSortShaderExecutable(
                                node->base.context,
                                VXNNE_KERNEL_RPN_SORT,
                                &node->kernelAttributes.borderMode,
                                rs_proposalTensor);

            if(NULL != rs_proposalTensor){
                vxoTensor_ReleaseTensor(&rs_proposalTensor);
                rs_proposalTensor = NULL;
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(
                &rpnLayer->tensorRpnSortSH,
                &rpnLayer->base,
                VXNNE_OPERATOR_RPN_SORT,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSortSH.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnSortSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_SORT,
                                        vxnneExecuteSWRPN_Sort,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnSortSW.pre_nms_topn  = pre_nms_topn;
            rpnLayer->tensorRpnSortSW.proposal      = proposalTensor;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnSortSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnSortSW.input_stage     = input_stage;
            rpnLayer->tensorRpnSortSW.output_stage    = output_stage;
        }

        /* ------------RPN NMS--------------------- */
        /* create a temp tensor to store the index of the nms's proposal */
        dims        = 3;
        sizes[0]    = post_nms_topn->value->u32;
        sizes[1]    = 1;
        sizes[2]    = 1;
        sizes[3]    = 1;

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dims;
        tensor_create_params.sizes = sizes;
        tensor_create_params.data_format = temp_format;
        tensor_create_params.quant_format = VX_QUANT_DYNAMIC_FIXED_POINT;

        roiIndicesTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (roiIndicesTensor == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        /* create a temp tensor to store the real proposal number. because maybe real_roi < post_nms_topn */
        realRoiScalar = vxCreateScalar(node->base.context,VX_TYPE_UINT32,&realRoi);
        if (realRoiScalar==NULL)
        {
            vxError("vxCreateScalar fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        if (enable_gpu_nms == vx_false_e)
        {
            enable_gpu_nms = vx_true_e;
            if (TENSOR_DATA_TYPE(proposalTensor) != VX_TYPE_FLOAT16)
                enable_gpu_nms = vx_false_e;
            if (TENSOR_DATA_TYPE(roiIndicesTensor) != VX_TYPE_FLOAT32)
                enable_gpu_nms = vx_false_e;
        }
        if(enable_gpu_nms && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_tensor rs_proposalTensor = NULL;
            if (proposalTensor != NULL)
            {
                vx_int32 len = proposalTensor->dims[0] * proposalTensor->dims[1] * proposalTensor->dims[2] * proposalTensor->dims[3];
                vx_int32 new_size[6] = {5, len/5, 1, 1, 1, 1};
                rs_proposalTensor = vxoTensor_ReshapeTensor(proposalTensor, new_size, 3);
            }

            shaderExecutable =  vxnneRPNNmsShaderExecutable(
                                node->base.context,
                                VXNNE_KERNEL_RPN_NMS,
                                &node->kernelAttributes.borderMode,
                                 rs_proposalTensor,
                                 roiIndicesTensor,
                                 realRoiScalar,
                                 pre_nms_topn,
                                 post_nms_topn,
                                 nms_thresh);
            if(rs_proposalTensor)
                vxoTensor_ReleaseTensor(&rs_proposalTensor);

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&rpnLayer->tensorRpnNmsSH,
                                                      &rpnLayer->base,
                                                     VXNNE_OPERATOR_RPN_NMS,
                                                     batchCount,
                                                     shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnNmsSH.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnNmsSH.base, (vx_reference)roiIndicesTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnNmsSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_NMS,
                                        vxnneExecuteSWRPN_NMS,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnNmsSW.pre_nms_topn   = pre_nms_topn;
            rpnLayer->tensorRpnNmsSW.post_nms_topn  = post_nms_topn;
            rpnLayer->tensorRpnNmsSW.nms_thresh     = nms_thresh;
            rpnLayer->tensorRpnNmsSW.proposal       = proposalTensor;
            rpnLayer->tensorRpnNmsSW.roi_indices    = roiIndicesTensor;
            rpnLayer->tensorRpnNmsSW.real_roi_t     = realRoiScalar;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnNmsSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnNmsSW.base, (vx_reference)roiIndicesTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnNmsSW.input_stage     = input_stage;
            rpnLayer->tensorRpnNmsSW.output_stage    = output_stage;
        }

        /* ------------RPN Retrieve--------------------- */
        if (enable_gpu_retrive == vx_false_e)
        {
            enable_gpu_retrive = vx_true_e;
            if (TENSOR_DATA_TYPE(proposalTensor) != VX_TYPE_FLOAT16)
                enable_gpu_retrive = vx_false_e;
            if (TENSOR_DATA_TYPE(roiIndicesTensor) != VX_TYPE_FLOAT32)
                enable_gpu_retrive = vx_false_e;
            if (TENSOR_DATA_TYPE(roi_output) != VX_TYPE_FLOAT16)
                enable_gpu_retrive = vx_false_e;
            if (score_output != NULL)
                if(TENSOR_DATA_TYPE(score_output) != VX_TYPE_FLOAT16)
                    enable_gpu_retrive = vx_false_e;
        }
        if (enable_gpu_retrive && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_tensor rs_proposalTensor = NULL, rs_roi_output = NULL, rs_score_output = NULL;
            if (proposalTensor != NULL)
            {
                vx_int32 len = proposalTensor->dims[0] * proposalTensor->dims[1] * proposalTensor->dims[2] * proposalTensor->dims[3];
                vx_int32 new_size[6] = {5, len/5, 1, 1, 1, 1};
                rs_proposalTensor = vxoTensor_ReshapeTensor(proposalTensor, new_size, 3);
            }
            if (roi_output != NULL)
            {
                vx_int32 len = roi_output->dims[0] * roi_output->dims[1] * roi_output->dims[2] * roi_output->dims[3];
                vx_int32 new_size[6] = {5, len/5, 1, 1, 1, 1};
                rs_roi_output = vxoTensor_ReshapeTensor(roi_output, new_size, 3);
            }
            if (score_output != NULL)
            {
                vx_int32 len = score_output->dims[0] * score_output->dims[1] * score_output->dims[2] * score_output->dims[3];
                vx_int32 new_size[6] = {len, 1, 1, 1, 1, 1};
                rs_score_output = vxoTensor_ReshapeTensor(score_output, new_size, 3);
            }
            shaderExecutable =  vxnneRPNRetrieveShaderExecutable(
                                node->base.context,
                                VXNNE_KERNEL_RPN_RETRIEVE,
                                &node->kernelAttributes.borderMode,
                                rs_proposalTensor,
                                roiIndicesTensor,
                                realRoiScalar,
                                rs_roi_output,
                                rs_score_output);

            if(NULL != rs_proposalTensor){
                vxoTensor_ReleaseTensor(&rs_proposalTensor);
                rs_proposalTensor = NULL;
            }
            if(NULL != rs_roi_output){
                vxoTensor_ReleaseTensor(&rs_roi_output);
                rs_roi_output = NULL;
            }
            if(NULL != rs_score_output){
                vxoTensor_ReleaseTensor(&rs_score_output);
                rs_score_output = NULL;
            }

            if (!shaderExecutable){
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&rpnLayer->tensorRpnRetrieveSH, &rpnLayer->base,
                VXNNE_OPERATOR_RPN_RETRIEVE, batchCount, shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSH.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSH.base, (vx_reference)roiIndicesTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSH.base, (vx_reference)roi_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSH.base, (vx_reference)score_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
        else
        {
            vxnneOperation_Initialize(&rpnLayer->tensorRpnRetrieveSW.base,
                                        &rpnLayer->base,
                                        VXNNE_OPERATION_TARGET_SW,
                                        VXNNE_OPERATOR_RPN_RETRIEVE,
                                        vxnneExecuteSWRPN_Retrieve,
                                        VX_NULL,
                                        batchCount,
                                        0);

            rpnLayer->tensorRpnRetrieveSW.proposal       = proposalTensor;
            rpnLayer->tensorRpnRetrieveSW.roi_indices    = roiIndicesTensor;
            rpnLayer->tensorRpnRetrieveSW.real_roi_t     = realRoiScalar;
            rpnLayer->tensorRpnRetrieveSW.roi_output     = roi_output;
            rpnLayer->tensorRpnRetrieveSW.score_output   = score_output;

            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSW.base, (vx_reference)proposalTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSW.base, (vx_reference)roiIndicesTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSW.base, (vx_reference)roi_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneOperation_AddReference(&rpnLayer->tensorRpnRetrieveSW.base, (vx_reference)score_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            rpnLayer->tensorRpnRetrieveSW.input_stage    = input_stage;
            rpnLayer->tensorRpnRetrieveSW.output_stage   = output_stage;
        }
        /* --------------------------------- */

        rpnLayer->base.num_temp_tensors     = 3;
        rpnLayer->base.temp_tensors[0]      = socreBufferTensor;
        rpnLayer->base.temp_tensors[1]      = proposalTensor;
        rpnLayer->base.temp_tensors[2]      = roiIndicesTensor;


        if (enable_gpu_soft_max&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            softmax_op         = (vxnne_operation)&rpnLayer->tensorRpnSoftmaxSH.base;
        else
            softmax_op         = (vxnne_operation)&rpnLayer->tensorRpnSoftmaxSW.base;

        if (enable_gpu_regression&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            regression_op       = (vxnne_operation)&rpnLayer->tensorRpnRegressionSH.base;
        else
            regression_op       = (vxnne_operation)&rpnLayer->tensorRpnRegressionSW.base;

        if (enable_gpu_sort&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            sort_op         = (vxnne_operation)&rpnLayer->tensorRpnSortSH.base;
        else
            sort_op         = (vxnne_operation)&rpnLayer->tensorRpnSortSW.base;

        if (enable_gpu_nms&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            nms_op         = (vxnne_operation)&rpnLayer->tensorRpnNmsSH.base;
        else
            nms_op         = (vxnne_operation)&rpnLayer->tensorRpnNmsSW.base;


        if (enable_gpu_retrive&& (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            retrieve_op         = (vxnne_operation)&rpnLayer->tensorRpnRetrieveSH.base;
        else
            retrieve_op         = (vxnne_operation)&rpnLayer->tensorRpnRetrieveSW.base;

        vxnneLayer_SetOperation(
            &rpnLayer->base,
            softmax_op,
            0);
        vxnneLayer_SetOperation(
            &rpnLayer->base,
            regression_op,
            1);
        vxnneLayer_SetOperation(
            &rpnLayer->base,
            sort_op,
            2);
        vxnneLayer_SetOperation(
            &rpnLayer->base,
            nms_op,
            3);
       vxnneLayer_SetOperation(
            &rpnLayer->base,
            retrieve_op,
            4);

        node->layer = &rpnLayer->base;
    }

    return VX_SUCCESS;

exit:
    if (rpnLayer)
    {
        gcoOS_Free(VX_NULL, rpnLayer);
    }

    return status;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status;
    vx_int32 rpn_type = 0;
    gctSTRING envctrl = gcvNULL;
    vx_tensor  score                      = (vx_tensor)parameters[0];
    vx_tensor  bbox                       = (vx_tensor)parameters[1];
    vx_tensor  anchors                    = (vx_tensor)parameters[2];

    vx_enum score_format = TENSOR_DATA_TYPE(score);
    vx_enum bbox_format  = TENSOR_DATA_TYPE(bbox);
    vx_enum anchors_format = TENSOR_DATA_TYPE(anchors);

    vx_bool enable_condition0 = (vx_bool)((score_format   == VX_TYPE_FLOAT16)
                                        && (bbox_format    == VX_TYPE_FLOAT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition1 = (vx_bool)((score_format   == VX_TYPE_INT16)
                                        && (bbox_format    == VX_TYPE_INT16)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition2 = (vx_bool)((score_format   == VX_TYPE_INT8)
                                        && (bbox_format    == VX_TYPE_INT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));

    vx_bool enable_condition3 = (vx_bool)((score_format   == VX_TYPE_UINT8)
                                        && (bbox_format    == VX_TYPE_UINT8)
                                        && (anchors_format == VX_TYPE_FLOAT32));




    /*USE_RPN_MODE-0: SHD+CPU 1: ALL CPU 2: ALL SHD*/
    if (gcmIS_SUCCESS(gcoOS_GetEnv(gcvNULL, "USE_RPN_MODE", &envctrl)) && envctrl)
    {
        rpn_type = atoi(envctrl);
    }
    if(!(enable_condition0 | enable_condition1 | enable_condition2 | enable_condition3))
        rpn_type = 1;
    if(rpn_type == 0)
        status = vxoNNRPNLayer_Initializer_shd_cpu(node,parameters,num);
    else if(rpn_type == 1)
        status = vxoNNRPNLayer_Initializer_cpu(node,parameters,num);
    else
        status = vxoNNRPNLayer_Initializer_shd(node,parameters,num);
    return status;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoNNRPNLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/**************************************************************************************
 *                     ROI POOL
 *************************************************************************************/
vx_status vxnneExecuteSWROIPooling(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;

    vxnne_tensor_roipool_operation roipoolOperation = (vxnne_tensor_roipool_operation)operation;

    vx_tensor input_data  = roipoolOperation->input_data;
    vx_tensor input_roi   = roipoolOperation->input_rois;
    vx_tensor output      = roipoolOperation->output;

    vx_float32 spatial_scale    = roipoolOperation->spatial_scale->value->f32;
    vx_int32 pooled_height      = roipoolOperation->pooled_height->value->u32;
    vx_int32 pooled_width       = roipoolOperation->pooled_width->value->u32;

    vx_int32 num_rois       = TENSOR_VIEW_SIZE_INDEX(output, 3);
    vx_int32 channel        = TENSOR_VIEW_SIZE_INDEX(input_data, 2);
    vx_int32 height         = TENSOR_VIEW_SIZE_INDEX(input_data, 1);
    vx_int32 width          = TENSOR_VIEW_SIZE_INDEX(input_data, 0);

    vx_int32 stride_w       = TENSOR_VIEW_SIZE_INDEX(input_roi, 2); /* 5 */

    vx_type_e in_data_format    = (vx_type_e)TENSOR_DATA_TYPE(input_data);
    vx_type_e in_roi_format     = (vx_type_e)TENSOR_DATA_TYPE(input_roi);
    vx_type_e out_format        = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_int8 in_data_fp          = TENSOR_POS(input_data);
    vx_int8 in_roi_fp           = TENSOR_POS(input_roi);
    vx_int8 out_fp              = TENSOR_POS(output);
    vx_enum in_roi_rMode        = TENSOR_ROUNDING_MODE(input_roi);
    vx_enum out_rMode           = TENSOR_ROUNDING_MODE(output);

    vx_int32 in_data_items      = vxnneGetTypeSize(in_data_format);
    vx_int32 in_roi_items       = vxnneGetTypeSize(in_roi_format);
    vx_int32 out_items          = vxnneGetTypeSize(out_format);

    vx_uint8_ptr input_data_ptr,rois_data_ptr,output_data_ptr;
    vx_int32 n, c, ph, pw, h, w;

    vx_bool enable_relu = vx_false_e;

    if (roipoolOperation->relu != VX_NULL)
        enable_relu = roipoolOperation->relu->value->b;

    vxoTensor_GetTensorViewMemory(input_data, (gctPOINTER *)&input_data_ptr, VX_NULL);
    vxoTensor_GetTensorViewMemory(input_roi, (gctPOINTER *)&rois_data_ptr, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER *)&output_data_ptr, VX_NULL);

    //vx_int32 test_index = 0; // for debug
    for(n = 0; n < num_rois; n++)
    {
        vx_int32 offset = 0, roi_batch_ind = 0, roi_start_w = 0, roi_start_h = 0, roi_end_w = 0, roi_end_h = 0;
        vx_int32 roi_height = 0, roi_width = 0;
        vx_float32 roi_size_scale_h = 0, roi_size_scale_w = 0;
        vx_uint8_ptr batch_data = VX_NULL;

        if (stride_w == 5)
        {
            offset = 1;
            roi_batch_ind = (vx_int32)vxnneGetDataExt(in_roi_format, TENSOR_QUANT_TYPE(input_roi), 0, rois_data_ptr, in_roi_fp, TENSOR_TF_ZEROPOINT(input_roi), TENSOR_TF_SCALE(input_roi));
        }

        /* map the roi coordinates to the feature map */
        roi_start_w = (vx_int32)vxnneRound((vx_float32)vxnneGetDataExt(in_roi_format, TENSOR_QUANT_TYPE(input_roi), offset, rois_data_ptr, in_roi_fp, TENSOR_TF_ZEROPOINT(input_roi), TENSOR_TF_SCALE(input_roi)) * spatial_scale, in_roi_rMode);
        roi_start_h = (vx_int32)vxnneRound((vx_float32)vxnneGetDataExt(in_roi_format, TENSOR_QUANT_TYPE(input_roi), offset + 1, rois_data_ptr, in_roi_fp, TENSOR_TF_ZEROPOINT(input_roi), TENSOR_TF_SCALE(input_roi)) * spatial_scale, in_roi_rMode);
        roi_end_w = (vx_int32)vxnneRound((vx_float32)vxnneGetDataExt(in_roi_format, TENSOR_QUANT_TYPE(input_roi), offset + 2, rois_data_ptr, in_roi_fp, TENSOR_TF_ZEROPOINT(input_roi), TENSOR_TF_SCALE(input_roi)) * spatial_scale, in_roi_rMode);
        roi_end_h = (vx_int32)vxnneRound((vx_float32)vxnneGetDataExt(in_roi_format, TENSOR_QUANT_TYPE(input_roi), offset + 3, rois_data_ptr, in_roi_fp, TENSOR_TF_ZEROPOINT(input_roi), TENSOR_TF_SCALE(input_roi)) * spatial_scale, in_roi_rMode);

        /* compute the roi rectangle on the feature map */
        roi_height = (vx_int32)gcmMAX(roi_end_h - roi_start_h + 1, 1);
        roi_width = (vx_int32)gcmMAX(roi_end_w - roi_start_w + 1, 1);
        roi_size_scale_h = (vx_float32)(roi_height) / (vx_float32)(pooled_height);
        roi_size_scale_w = (vx_float32)(roi_width) / (vx_float32)(pooled_width);

        batch_data = input_data_ptr + roi_batch_ind * channel * width * height * in_data_items;

        for(c = 0; c < channel; c++)
        {
            for(ph = 0; ph < pooled_height; ph++)
            {
                for(pw = 0; pw < pooled_width; pw++)
                {
                    vx_int32 pool_index = 0;
                    vx_bool is_empty = vx_false_e;
                    vx_float32 output_data_v = 0;

                    /*
                        Compute pooling region for this output unit
                        so we can compute its upper left and lower right coordinates.
                    */
                    vx_int32 hstart = (vx_int32)(floor((vx_float32)(ph) * roi_size_scale_h));
                    vx_int32 wstart = (vx_int32)(floor((vx_float32)(pw) * roi_size_scale_w));
                    vx_int32 hend = (vx_int32)(ceil((vx_float32)(ph + 1) * roi_size_scale_h));
                    vx_int32 wend = (vx_int32)(ceil((vx_float32)(pw + 1) * roi_size_scale_w));
                    hstart = gcmMIN(gcmMAX(hstart + roi_start_h, 0), height);
                    hend = gcmMIN(gcmMAX(hend + roi_start_h, 0), height);
                    wstart = gcmMIN(gcmMAX(wstart + roi_start_w, 0), width);
                    wend = gcmMIN(gcmMAX(wend + roi_start_w, 0), width);

                    pool_index = ph * pooled_width + pw;

                    /* remove some rectangles that do not meet the requirements */
                    is_empty = (vx_bool)((hend <= hstart) || (wend <= wstart));
                    if(is_empty)
                    {
                        output_data_v = 0;
                    }
                    else
                    {
                        /* find the max value in the current pooling region */
                        for(h = hstart; h < hend; h++)
                        {
                            for(w = wstart; w < wend; w++)
                            {
                                const vx_int32 index = h * width + w;
                                vx_float32 batch_data_v = 0.0f;

                                batch_data_v = (vx_float32)vxnneGetDataExt(in_data_format, TENSOR_QUANT_TYPE(input_data), index, batch_data, in_data_fp, TENSOR_TF_ZEROPOINT(input_data), TENSOR_TF_SCALE(input_data));

                                if (batch_data_v > output_data_v)
                                    output_data_v = batch_data_v;
                            }
                        }
                    }

                    if (enable_relu)
                    {
                        if (output_data_v < 0)
                            output_data_v = 0;
                    }

                    /* Save the max value to the output */
                    vxnneSaveDataExt(out_format, TENSOR_QUANT_TYPE(output), pool_index, output_data_v, output_data_ptr, out_fp, TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), out_rMode);
                }
            }

            /* Increment all data pointers by one channel*/
            batch_data      += width * height * in_data_items;
            output_data_ptr += pooled_width * pooled_height * out_items;
        }

        /* Increment ROI data pointer */
        if (stride_w == 5)
            rois_data_ptr += 5 * in_roi_items;
        else
            rois_data_ptr += 4 * in_roi_items;
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNROIPoolLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneROIPoolLayer_Initializer(
    vx_node    node,
    char*      name,
    vx_tensor  input_data,
    vx_tensor  input_rois,
    vx_scalar  pool_types,
    vx_scalar  spatial_scales,
    vx_scalar  pooled_heights,
    vx_scalar  pooled_widths,
    vx_tensor  outputs,
    vx_scalar  relu
    )
{
    vx_status  status                     = VX_SUCCESS;
    vx_context context                    = vxGetContext((vx_reference)node);

    vx_float32 spatial_scale              = spatial_scales->value->f32;
    vx_uint32  pool_width                 = pooled_widths->value->u32;
    vx_uint32  pool_height                = pooled_heights->value->u32;
    vx_bool    shExe_flag                 = vx_true_e;
    vx_uint32  width                      = TENSOR_VIEW_SIZE_INDEX(input_data, 0);
    vx_uint32  height                     = TENSOR_VIEW_SIZE_INDEX(input_data, 1);
    vx_uint32  depth                      = TENSOR_VIEW_SIZE_INDEX(input_data, 2);
    vx_uint32  roi_stride                 = TENSOR_VIEW_SIZE_INDEX(input_rois, 2);
    vx_uint32  rois_num                   = TENSOR_VIEW_SIZE_INDEX(input_rois, 3);
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(input_data);
    vx_enum    roisFormat                 = TENSOR_DATA_TYPE(input_rois);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(input_data, 3);

    vx_tensor_create_params_t  tensor_create_params;
    vxnne_tensor_roipool_layer roipoolLayer=VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    {
        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_roipool_layer_s), (gctPOINTER*)&roipoolLayer);
        if (!roipoolLayer)
        {
            status = VX_ERROR_NO_MEMORY;
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            goto exit;
        }


        gcoOS_ZeroMemory(roipoolLayer, sizeof(vxnne_tensor_roipool_layer_s));

        vxnneLayer_Initialize(&roipoolLayer->base,
            name,
            node,
            vxmOPERATION_COUNT(roipoolLayer),
            roipoolLayer->operations,
            VX_NULL);

        if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_ROI_POOLING) &&
            vxnneIsTPSupportFormat(context, input_data, VX_NULL, outputs) &&
            (roisFormat == VX_TYPE_FLOAT16))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_uint32 num, size, maxpool, poolx, pooly, poolz;
            vx_op_param_s conv = {0};
            vx_tensor tmpTensor = VX_NULL;
            vx_tensor list = VX_NULL;
            vx_tensor split_end = VX_NULL;
            vx_tensor_create_params_t tensor_create_params;
            vx_uint32 core = context->nnConfig.fixedFeature.tpCoreCount;
            vx_bool mult = context->options.enableMultiTP && core > 1;
            vx_uint32 slice = !mult ? 1 : gcmMIN(TENSOR_VIEW_SIZE_INDEX(outputs, 3), core);
            vx_uint32 roi_size = TENSOR_VIEW_SIZE_INDEX(outputs, 3);
            vx_uint32 split_size_array[TP_TENSOR_COUNT] = {0};
            vx_uint32 split_offset_array[TP_TENSOR_COUNT] = {0};
            vx_uint32 splitEnds[TP_TENSOR_COUNT] = {0};
            vx_uint32 i = 0;

            calculateSplitSize(roi_size, slice, split_size_array, split_offset_array);

            splitEnds[0] = split_size_array[0] - 1;
            for (i = 1; i < slice; i++)
            {
                splitEnds[i] = splitEnds[i - 1] + split_size_array[i];
            }

            status = vxnneOperation_Initialize(&roipoolLayer->roipool_tp_operation[0].base,
                                                &roipoolLayer->base,
                                                VXNNE_OPERATION_TARGET_TP,
                                                VXNNE_OPERATOR_ROIPOOL,
                                                VX_NULL,
                                                vxnneOperation_TP_Deinitialize,
                                                batchCount,
                                                0);
            if (status != VX_SUCCESS) goto exit;

            status = vxnneOperation_Initialize(&roipoolLayer->roipool_tp_operation[1].base,
                                                &roipoolLayer->base,
                                                VXNNE_OPERATION_TARGET_TP,
                                                VXNNE_OPERATOR_ROIPOOL,
                                                VX_NULL,
                                                vxnneOperation_TP_Deinitialize,
                                                batchCount,
                                                0);
            if (status != VX_SUCCESS) goto exit;

            /* Prepare ROI intermediate output buffer. */
            maxpool = (TENSOR_VIEW_SIZE_INDEX(input_data, 0) + pool_width - 1) / pool_width;
            poolx = 1 << (vx_uint32) ceil(log(TENSOR_VIEW_SIZE_INDEX(input_data, 0)) / log(2));
            pooly = 1 << (vx_uint32) ceil(log(TENSOR_VIEW_SIZE_INDEX(input_data, 1)) / log(2));
            poolz = 1 << (vx_uint32) ceil(log(TENSOR_VIEW_SIZE_INDEX(input_data, 2)) / log(2));
            size = poolx * pooly * poolz * maxpool * TENSOR_DATA_SIZE(input_data);

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 1;
            tensor_create_params.sizes = &size;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(input_data);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(input_data);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(input_data);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(input_data);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input_data);
            }

            tmpTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
            if (tmpTensor == VX_NULL)
            {
                vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                status = VX_ERROR_NO_MEMORY;
                goto exit;
            }

            conv.pad_x_left = 0;
            conv.pad_y_top = 0;
            conv.pool_size_x = 0;
            conv.pool_size_y = 0;
            conv.pool_stride = 1;
            conv.enable_relu = vx_false_e;
            conv.conv_rounding_type = 0;
            conv.pad_mode = VX_PAD_CONSTANT;
            conv.pad_const = 0;
            conv.tpType = TP_ROI_POOLING_STEP_1;
            conv.other_ref = (vx_reference)input_rois;
            conv.data_buff = gcvNULL;
            conv.tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
            conv.tp_value->u32[0] = pool_width;
            conv.tp_value->u32[1] = pool_height;
            conv.tp_value->f32[0] = spatial_scale;
            conv.tp_value->u32[2] = maxpool;
            conv.tp_value->u32[3] = poolx;
            conv.tp_value->u32[4] = pooly;
            conv.tp_value->u32[5] = poolz;
            conv.tp_value->u32[6] = TENSOR_VIEW_SIZE_INDEX(outputs, 3);
            conv.tp_value->e32[0] = 0;

            vxMemCopy(&roipoolLayer->roipool_tp_operation[0].base.parameter, &conv, sizeof(vx_op_param_s));

            vxnneLayer_SetOperation(
                &roipoolLayer->base,
                &roipoolLayer->roipool_tp_operation[0].base,
                0);

            roipoolLayer->roipool_tp_operation[0].input  = input_data;
            roipoolLayer->roipool_tp_operation[0].output = tmpTensor;
            vxnneOperation_AddReference(&roipoolLayer->roipool_tp_operation[0].base, (vx_reference)input_data, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&roipoolLayer->roipool_tp_operation[0].base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            /* Prepare ROI list. */
            num = TENSOR_VIEW_SIZE_INDEX(outputs, 3) * sizeof(vx_tp_roi_pool) / sizeof(vx_uint32) * 2;
            list = vxnneAllocateTPROIListBuffer(context, node, num, VX_TYPE_UINT16);
            if (list == VX_NULL)
            {
                status = VX_ERROR_NO_MEMORY;
                goto exit;
            }
            /* Prepare Split end list. */
            split_end = vxnneAllocateTPROIListBuffer(context, node, slice, VX_TYPE_UINT32);
            if (split_end == VX_NULL)
            {
                status = VX_ERROR_NO_MEMORY;
                goto exit;
            }

            vxnneInitROITensorFromBuffer(split_end, splitEnds, slice * sizeof(vx_uint32));

            shaderExecutable = vxnneROIRect2ROIListShaderExecutable(node->base.context, VXNNE_KERNEL_ROIRECT2ROILIST, &node->kernelAttributes.borderMode, input_rois, roi_stride, rois_num, pool_width, pool_height, spatial_scale, slice, split_end, list);
            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&roipoolLayer->tensorROIPoolSH,
                &roipoolLayer->base,
                VXNNE_OPERATOR_PRETREATEDRECT,
                batchCount,
                shaderExecutable);

            vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)input_rois, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)split_end, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)list, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                &roipoolLayer->base,
                &roipoolLayer->tensorROIPoolSH.base,
                1);
            conv.tpType = TP_ROI_POOLING_STEP_2;
            conv.other_ref = (vx_reference)input_data;
            conv.data_buff = list;
            if (relu != VX_NULL)
                conv.enable_relu = relu->value->b;
            else
                conv.enable_relu = vx_false_e;
            conv.tp_value = (vx_tp_value_cmd_s*)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
            conv.tp_value->u32[0] = pool_width;
            conv.tp_value->u32[1] = pool_height;
            conv.tp_value->f32[0] = spatial_scale;
            conv.tp_value->u32[2] = maxpool;
            conv.tp_value->u32[3] = poolx;
            conv.tp_value->u32[4] = pooly;
            conv.tp_value->u32[5] = poolz;
            conv.tp_value->u32[6] = TENSOR_VIEW_SIZE_INDEX(outputs, 3);
            conv.tp_value->e32[0] = 1;

            vxMemCopy(&roipoolLayer->roipool_tp_operation[1].base.parameter, &conv, sizeof(vx_op_param_s));

            vxnneLayer_SetOperation(
                &roipoolLayer->base,
                &roipoolLayer->roipool_tp_operation[1].base,
                2);

            roipoolLayer->roipool_tp_operation[1].input  = tmpTensor;
            roipoolLayer->roipool_tp_operation[1].output = outputs;

            vxnneOperation_AddReference(&roipoolLayer->roipool_tp_operation[1].base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&roipoolLayer->roipool_tp_operation[1].base, (vx_reference)list, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&roipoolLayer->roipool_tp_operation[1].base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            roipoolLayer->base.num_temp_tensors = 2;
            roipoolLayer->base.temp_tensors[0] = tmpTensor;
            roipoolLayer->base.temp_tensors[1] = split_end;
            node->layer = &roipoolLayer->base;
        }
        else
        {
            vx_bool    shExe_flag0  = (vx_bool)(width == 20 && height == 16 && roi_stride == 5 && pool_width == 6 && pool_height == 6 && inputFormat == VX_TYPE_FLOAT16 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16);
            vx_bool    shExe_flag1  = (vx_bool)(width == 51 && height == 39 && roi_stride == 5 && pool_width == 6 && pool_height == 6 && inputFormat == VX_TYPE_FLOAT16 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16);
            vx_bool    shExe_flag2  = (vx_bool)(width == 51 && height == 39 && roi_stride == 5 && pool_width == 6 && pool_height == 6 && inputFormat == VX_TYPE_INT8 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT8);
            vx_bool    shExe_flag3  = (vx_bool)(width == 20 && height == 16 && roi_stride == 5 && pool_width == 6 && pool_height == 6 && inputFormat == VX_TYPE_INT8 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT8);
            vx_bool    shExe_flag4  = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) /*||
                                                (inputFormat == VX_TYPE_INT16 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT16)||
                                                (inputFormat == VX_TYPE_UINT8 && roisFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_UINT8)*/);

            shExe_flag =  shExe_flag2 || shExe_flag3 || shExe_flag4;

            if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;
                vx_bool enable_relu = relu ? relu->value->b : vx_false_e;

                if (shExe_flag0 || shExe_flag1 || shExe_flag2 || shExe_flag3)
                {
                    vx_uint32 imgNum                = width == 51 ? 7 : 4;
                    vx_uint32 dims                  = 3;
                    vx_uint32 tmp_sizes0[3]         = {width, height, depth * imgNum };
                    vx_uint32 tmp_sizes1[3]         = {84, rois_num, 1 };
                    vx_uint32 outputs_dims          = outputs->dimCount;
                    vx_tensor outputs_reshp         = NULL;
                    vx_tensor vertMaxPoolTensor, preTreatedRectTensor;

                    gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                    tensor_create_params.num_of_dims = dims;
                    tensor_create_params.sizes = tmp_sizes0;
                    tensor_create_params.data_format = TENSOR_DATA_TYPE(outputs);
                    tensor_create_params.quant_format = TENSOR_QUANT_TYPE(outputs);
                    if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                    {
                        tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(outputs);
                    }
                    else
                    {
                        tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(outputs);
                        tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(outputs);
                    }

                    vertMaxPoolTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
                    if (vertMaxPoolTensor == VX_NULL)
                    {
                        vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                        status = VX_ERROR_NO_MEMORY;
                        goto exit;
                    }

                    tensor_create_params.data_format = TENSOR_DATA_TYPE(input_rois);
                    tensor_create_params.quant_format = TENSOR_QUANT_TYPE(input_rois);
                    if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                    {
                        tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(input_rois);
                    }
                    else
                    {
                        tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(input_rois);
                        tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input_rois);
                    }
                    tensor_create_params.sizes = tmp_sizes1;
                    preTreatedRectTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
                    if (preTreatedRectTensor == VX_NULL)
                    {
                        vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                        status = VX_ERROR_NO_MEMORY;
                        goto exit;
                    }

                    //operation1:vertMaxPool
                    shaderExecutable = vxnneVertMaxPoolShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_VERTMAXPOOL, &node->kernelAttributes.borderMode, input_data, pool_width, pool_height, enable_relu, vertMaxPoolTensor);

                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        goto exit;
                    }
                    status = vxnneShaderOperation_Initialize(&roipoolLayer->vertmaxpool_operation.vertmaxpool_SHoperation,
                        &roipoolLayer->base,
                        VXNNE_OPERATOR_VERTMAXPOOL,
                        batchCount,
                        shaderExecutable);

                    if (status != VX_SUCCESS)
                        goto exit;

                    vxnneOperation_AddReference(&roipoolLayer->vertmaxpool_operation.vertmaxpool_SHoperation.base, (vx_reference)input_data, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->vertmaxpool_operation.vertmaxpool_SHoperation.base, (vx_reference)vertMaxPoolTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    //operation2:preTreatedRect
                    shaderExecutable = vxnnePreTreatedRectShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_PRETREATEDRECT, &node->kernelAttributes.borderMode, input_rois, roi_stride, rois_num, width, height, spatial_scale, preTreatedRectTensor);

                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        goto exit;
                    }
                    status = vxnneShaderOperation_Initialize(&roipoolLayer->pretreatedrect_operation.pretreatedrect_SHoperation,
                        &roipoolLayer->base,
                        VXNNE_OPERATOR_PRETREATEDRECT,
                        batchCount,
                        shaderExecutable);

                    if (status != VX_SUCCESS)
                        goto exit;

                    vxnneOperation_AddReference(&roipoolLayer->pretreatedrect_operation.pretreatedrect_SHoperation.base, (vx_reference)input_rois, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->pretreatedrect_operation.pretreatedrect_SHoperation.base, (vx_reference)preTreatedRectTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    //operation3:horzMaxPool
                    if(outputs_dims == 4)
                    {
                        vx_int32 new_size[3] = {pool_width * pool_height, depth, rois_num};
                        outputs_dims = 3;
                        outputs_reshp = vxoTensor_ReshapeTensor(outputs, new_size, outputs_dims);
                        TENSOR_POS(outputs_reshp) = TENSOR_POS(outputs);
                    }

                    TENSOR_POS(vertMaxPoolTensor) = TENSOR_POS(input_data);
                    if(outputs_reshp)
                    {
                        shaderExecutable = vxnneHorzMaxPoolShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_HORZMAXPOOL, &node->kernelAttributes.borderMode, vertMaxPoolTensor, preTreatedRectTensor, outputs_reshp);
                        vxoTensor_ReleaseTensor(&outputs_reshp);
                    }
                    else
                    {
                        shaderExecutable = vxnneHorzMaxPoolShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_HORZMAXPOOL, &node->kernelAttributes.borderMode, vertMaxPoolTensor, preTreatedRectTensor, outputs);
                    }

                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        goto exit;
                    }
                    status = vxnneShaderOperation_Initialize(&roipoolLayer->horzmaxpool_operation.horzmaxpool_SHoperation,
                        &roipoolLayer->base,
                        VXNNE_OPERATOR_HORZMAXPOOL,
                        batchCount,
                        shaderExecutable);

                    if (status != VX_SUCCESS)
                        goto exit;

                    vxnneOperation_AddReference(&roipoolLayer->horzmaxpool_operation.horzmaxpool_SHoperation.base, (vx_reference)vertMaxPoolTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->horzmaxpool_operation.horzmaxpool_SHoperation.base, (vx_reference)preTreatedRectTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->horzmaxpool_operation.horzmaxpool_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    vxnneLayer_SetOperation(
                        &roipoolLayer->base,
                        &roipoolLayer->vertmaxpool_operation.vertmaxpool_SHoperation.base,
                        0);
                    vxnneLayer_SetOperation(
                        &roipoolLayer->base,
                        &roipoolLayer->pretreatedrect_operation.pretreatedrect_SHoperation.base,
                        1);
                    vxnneLayer_SetOperation(
                        &roipoolLayer->base,
                        &roipoolLayer->horzmaxpool_operation.horzmaxpool_SHoperation.base,
                        2);

                    roipoolLayer->base.num_temp_tensors                  = 2;
                    roipoolLayer->base.temp_tensors[0] = vertMaxPoolTensor;
                    roipoolLayer->base.temp_tensors[1] = preTreatedRectTensor;
                }
                else if (shExe_flag4)
                {
                    shaderExecutable = vxnneROIPoolShaderExecutable(node->base.context, VXNNE_KERNEL_ROIPOOL, &node->kernelAttributes.borderMode, input_data, input_rois, pool_width, pool_height, spatial_scale, enable_relu, outputs);

                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        goto exit;
                    }
                    status = vxnneShaderOperation_Initialize(&roipoolLayer->tensorROIPoolSH,
                        &roipoolLayer->base,
                        VXNNE_OPERATOR_ROIPOOL,
                        batchCount,
                        shaderExecutable);

                    if (status != VX_SUCCESS)
                        goto exit;

                    vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)input_data, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)input_rois, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSH.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    vxnneLayer_SetOperation(
                             &roipoolLayer->base,
                             &roipoolLayer->tensorROIPoolSH.base,
                             0);
                }

                node->layer = &roipoolLayer->base;
            }
            else
            {
                vxnneOperation_Initialize(&roipoolLayer->tensorROIPoolSW.base,
                    &roipoolLayer->base,
                    VXNNE_OPERATION_TARGET_SW,
                    VXNNE_OPERATOR_ROIPOOL,
                    vxnneExecuteSWROIPooling,
                    VX_NULL,
                    batchCount,
                    0);

                vxnneLayer_SetOperation(
                         &roipoolLayer->base,
                         &roipoolLayer->tensorROIPoolSW.base,
                         0);

                roipoolLayer->tensorROIPoolSW.input_data      = input_data;
                roipoolLayer->tensorROIPoolSW.input_rois      = input_rois;
                roipoolLayer->tensorROIPoolSW.pool_type       = pool_types;
                roipoolLayer->tensorROIPoolSW.pooled_height   = pooled_heights;
                roipoolLayer->tensorROIPoolSW.pooled_width    = pooled_widths;
                roipoolLayer->tensorROIPoolSW.spatial_scale   = spatial_scales;
                roipoolLayer->tensorROIPoolSW.output          = outputs;
                roipoolLayer->tensorROIPoolSW.relu            = relu;

                vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSW.base, (vx_reference)input_data, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSW.base, (vx_reference)input_rois, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&roipoolLayer->tensorROIPoolSW.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                node->layer = &roipoolLayer->base;
            }
        }
    }
    return status;

exit:
    if(roipoolLayer) gcoOS_Free(NULL, (gctPOINTER)roipoolLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_tensor  input_data                 = (vx_tensor)parameters[0];
    vx_tensor  input_rois                 = (vx_tensor)parameters[1];
    vx_scalar  pool_types                 = (vx_scalar)parameters[2];
    vx_scalar  spatial_scales             = (vx_scalar)parameters[3];
    vx_scalar  pooled_heights             = (vx_scalar)parameters[4];
    vx_scalar  pooled_widths              = (vx_scalar)parameters[5];
    vx_tensor  outputs                    = (vx_tensor)parameters[6];

    return vxnneROIPoolLayer_Initializer(
        node,
        "ROIPoolLayer",
        input_data,
        input_rois,
        pool_types,
        spatial_scales,
        pooled_heights,
        pooled_widths,
        outputs,
        VX_NULL
        );


}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoInternalKernel_NNROIPoolReluLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolReluLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolReluLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolReluLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_tensor  input_data                 = (vx_tensor)parameters[0];
    vx_tensor  input_rois                 = (vx_tensor)parameters[1];
    vx_scalar  pool_types                 = (vx_scalar)parameters[2];
    vx_scalar  spatial_scales             = (vx_scalar)parameters[3];
    vx_scalar  pooled_heights             = (vx_scalar)parameters[4];
    vx_scalar  pooled_widths              = (vx_scalar)parameters[5];
    vx_tensor  outputs                    = (vx_tensor)parameters[6];
    vx_scalar  relu                       = (vx_scalar)parameters[7];

    return vxnneROIPoolLayer_Initializer(
        node,
        "ROIPoolReluLayer",
        input_data,
        input_rois,
        pool_types,
        spatial_scales,
        pooled_heights,
        pooled_widths,
        outputs,
        relu
        );
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNROIPoolReluLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

#define DUMP_ONE_PIXEL_CPU_CONV 0

#if QUANT8_SUPPORT
vx_int32 Add(vx_int32 a, vx_int32 b) {
    return a + b;
}
vx_int32 BitAnd(vx_int32 a, vx_int32 b) {
    return a & b;
}
vx_int64 ShiftRight(vx_int32 a, int offset) {
    return a >> offset;
}
vx_int32 BitNot(vx_int32 a) {
    return ~a;
}
vx_int32 MaskIfNonZero(vx_int32 a) {
    static vx_int32 zero = 0;
    return a ? BitNot(zero) : zero;
}
vx_int32 MaskIfLessThan(vx_int32 a, vx_int32 b) {
    return MaskIfNonZero(a < b);
}
vx_int64 MaskIfGreaterThan(vx_int32 a, vx_int32 b) {
    return MaskIfNonZero(a > b);
}
vx_int32 RoundingDivideByPOT(vx_int32 x, vx_int32 exponent) {
    assert(exponent >= 0);
    assert(exponent <= 31);
    const vx_int32 mask = ((1ll << exponent) - 1);
    const vx_int32 zero = 0;
    const vx_int32 one = 1;
    const vx_int32 remainder = BitAnd(x, mask);
    const vx_int32 threshold =
        Add(ShiftRight(mask, 1), BitAnd(MaskIfLessThan(x, zero), one));
    return Add(ShiftRight(x, exponent),
        BitAnd(MaskIfGreaterThan(remainder, threshold), one));
}
vx_int32 SaturatingRoundingDoublingHighMul(vx_int32 a, vx_int32 b) {
    vx_bool overflow = a == b && a == 0x80000000;
    vx_int64 ab_64 = (vx_int64)a * (vx_int64)b;
    vx_int32 nudge = ab_64 >= 0 ? (1 << 30) : (1 - (1 << 30));
    vx_int32 ab_x2_high32 =
        (vx_int32)((ab_64 + nudge) / (1ll << 31));
    return overflow ? 0x7fffffff : ab_x2_high32;
}
#endif

vx_status vxnneExecuteSWConvolution(vxnne_operation operation)
{
    vxnne_convolution_operation convolutionOperation   = (vxnne_convolution_operation)operation;

    vx_tensor inputs                = convolutionOperation->inputs;
    vx_tensor weights               = convolutionOperation->weights;
    vx_tensor biases                = convolutionOperation->biases;
    vx_scalar padX                  = convolutionOperation->padX;
    vx_scalar padY                  = convolutionOperation->padY;
    vx_scalar padX_Right            = convolutionOperation->padXRight;
    vx_scalar padY_Bottom           = convolutionOperation->padYBottom;
    vx_scalar dilationX             = convolutionOperation->dilationX;
    vx_scalar dilationY             = convolutionOperation->dilationY;
    vx_scalar strideX               = convolutionOperation->strideX;
    vx_scalar strideY               = convolutionOperation->strideY;
    vx_scalar relu                  = convolutionOperation->relu;
    vx_scalar downScaleSizeRounding = convolutionOperation->downScaleSizeRounding;
    vx_tensor outputs               = convolutionOperation->outputs;

    vx_int32 batch = 1;
    void * inputBaseLogical;
    void * outputBaseLogical;

    void *weightsBaseLogical = VX_NULL;
    void *biasesBaseLogical = VX_NULL;

    vx_int32 inputWidth, inputHeight, inputDepth, outputWidth, outputHeight, outputDepth;
    vx_int32 kernelXSize, kernelYSize, stride_x, stride_y;
    vx_int32 k, p, j, i;
    vx_uint8_ptr dataSrc;
    vx_uint8_ptr dataDst;
    vx_uint8_ptr dataWeight;
    vx_uint8_ptr dataBias;
    vx_type_e inputFormat;
    vx_type_e weightFormat;
    vx_type_e biasFormat;
    vx_type_e outputFormat;

    vx_enum downScaleSizeRoundingValue = downScaleSizeRounding->value->e;
    vx_uint32 padXLeft;
    vx_uint32 padXRight;
    vx_uint32 padYTop;
    vx_uint32 padYBottom;

    vx_int32 dilation_x = (dilationX)?dilationX->value->n32 + 1:1, dilation_y = (dilationY)?dilationY->value->n32 + 1:1;

    if ((padX_Right != VX_NULL) && (padY_Bottom != VX_NULL))
    {
        padXLeft    = padX->value->n32;
        padXRight   = padX_Right->value->n32;
        padYTop     = padY->value->n32;
        padYBottom  = padY_Bottom->value->n32;
    }
    else
    {
        padXLeft = padX->value->u32;
        padXRight = padXLeft;
        padYTop = padY->value->u32;
        padYBottom = padYTop;
    }

    batch = (TENSOR_VIEW_SIZE_INDEX(inputs, 3) == 0) ? 1 : TENSOR_VIEW_SIZE_INDEX(inputs, 3);

    vxoTensor_GetTensorViewMemory(inputs, &inputBaseLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(weights, &weightsBaseLogical, VX_NULL);
    if (biases != VX_NULL)
        vxoTensor_GetTensorViewMemory(biases, &biasesBaseLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputBaseLogical, VX_NULL);

    dataSrc      = (vx_uint8_ptr)inputBaseLogical;
    dataDst      = (vx_uint8_ptr)outputBaseLogical;
    dataWeight   = (vx_uint8_ptr)weightsBaseLogical;
    dataBias     = (vx_uint8_ptr)biasesBaseLogical;
    inputFormat  = (vx_type_e)(TENSOR_DATA_TYPE(inputs));
    weightFormat = (vx_type_e)(TENSOR_DATA_TYPE(weights));
    biasFormat   = (biases != VX_NULL) ? (vx_type_e)(TENSOR_DATA_TYPE(biases)) : VX_TYPE_FLOAT32;
    outputFormat = (vx_type_e)(TENSOR_DATA_TYPE(outputs));

    inputWidth   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    inputHeight  = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
    inputDepth   = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
    outputWidth  = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    outputHeight = TENSOR_VIEW_SIZE_INDEX(outputs, 1);
    outputDepth  = TENSOR_VIEW_SIZE_INDEX(outputs, 2);

    kernelXSize = TENSOR_VIEW_SIZE_INDEX(weights, 0);
    kernelYSize = TENSOR_VIEW_SIZE_INDEX(weights, 1);

    if (strideX != VX_NULL && strideY != VX_NULL)
    {
        stride_x    = strideX->value->n32;
        stride_y    = strideY->value->n32;
    }
    else
    {

        if (inputWidth == 1 && inputHeight == 1)
        {
            stride_x = 1;
            stride_y = 1;
        }
        else
        {
            /* Calculate stride = (w + padXLeft + padXRight - weight)/(output_w - 1) */
            stride_x = vxoNNExternsionConvlutionRound((vx_float32)(inputWidth + padXLeft + padXRight - kernelXSize) / (outputWidth - 1), downScaleSizeRoundingValue);
            stride_y = vxoNNExternsionConvlutionRound((vx_float32)(inputHeight + padYTop + padYBottom - kernelYSize) / (outputHeight - 1), downScaleSizeRoundingValue);
        }
    }

    gcmASSERT(stride_x > 0 && stride_y > 0);

    gcoOS_MemFill(outputBaseLogical, 0, outputWidth * outputHeight * outputDepth * vxnneGetTypeSize(outputFormat));

    for (k = 0; k < batch; k++)
    {
#if DUMP_ONE_PIXEL_CPU_CONV
        vx_int32 my_count;
#endif
        dataSrc    = (vx_uint8_ptr)inputBaseLogical + k * inputWidth * inputHeight * inputDepth * vxnneGetTypeSize(inputFormat);
        dataWeight = (vx_uint8_ptr)weightsBaseLogical;
        dataDst    = (vx_uint8_ptr)outputBaseLogical + k * outputWidth * outputHeight * outputDepth * vxnneGetTypeSize(outputFormat);

        for (p = 0; p < outputDepth; p ++)
        {
            for (j = 0; j < outputHeight; j ++)
            {
                for (i = 0; i < outputWidth; i ++)
                {
#if DUMP_ONE_PIXEL_CPU_CONV
                    FILE * pfile = NULL;
                    // unit_test will print the index of mismatched pixel. set this to dump exact pixel
                    vx_int32 dump_index = 0x0;
#endif
                    vx_int32 hStart = j * stride_y - padYTop;
                    vx_int32 wStart = i * stride_x - padXLeft;
                    vx_int32 hEnd = gcmMIN(hStart + kernelYSize * dilation_y, inputHeight);
                    vx_int32 wEnd = gcmMIN(wStart + kernelXSize * dilation_x, inputWidth);
                    vx_int32 indexOut = 0;
                    vx_int32 indexBias = 0;
                    vx_int32 h, w = 0;
                    vx_int32 m, n = 0;
                    vx_int32 d;
#if QUANT8_SUPPORT
                    vx_int32 sum = 0;
#else
                    vx_float32 sum = 0;
#endif
                    vx_uint32 kernelXStart = 0, kernelYStart = 0;
#if DUMP_ONE_PIXEL_CPU_CONV
                    my_count = p*outputDepth*outputHeight*outputWidth + j*outputWidth + i;
#endif
                    kernelYStart = hStart < 0 ? (gcmALIGN_NP2_SAFE(-hStart, dilation_y)/dilation_y) : 0;
                    kernelXStart = wStart < 0 ? (gcmALIGN_NP2_SAFE(-wStart, dilation_x)/dilation_x) : 0;

                    if (hStart < 0 && dilation_y > 1)
                        hStart = gcmMAX(hStart, (j * stride_y)%dilation_y);
                    else
                        hStart = gcmMAX(hStart, 0);

                    if (wStart < 0 && dilation_x > 1)
                        wStart = gcmMAX(wStart, (i * stride_x)%dilation_x);
                    else
                        wStart = gcmMAX(wStart, 0);

                    indexOut = j * (outputWidth) + i;
#if DUMP_ONE_PIXEL_CPU_CONV
                    if(my_count == dump_index)
                    {
                        pfile = fopen("one_pixel_conv_dump.txt", "a");
                        if(pfile == NULL)
                        {
                            vxError("openfile error\n");
                        }
                    }
#endif

                    for (d = 0; d < inputDepth; d++)
                    {
                        for (h = hStart, n = kernelYStart; h < hEnd; h += dilation_y, n++)
                        {
                            for (w = wStart, m = kernelXStart; w < wEnd; w += dilation_x, m++)
                            {
                                const vx_int32 indexSrc = d * inputWidth * inputHeight + h * (inputWidth) + w;
                                const vx_int32 indexWeight = d * kernelXSize * kernelYSize + n * kernelXSize + m;
#if QUANT8_SUPPORT
                                if (TENSOR_DATA_TYPE(inputs) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(inputs) == VX_QUANT_AFFINE_SCALE)
                                {
                                    vx_int32 inImg_data = dataSrc[indexSrc] - TENSOR_TF_ZEROPOINT(inputs);
                                    vx_int32 weight_data = dataWeight[indexWeight] - TENSOR_TF_ZEROPOINT(weights);

                                    sum += inImg_data * weight_data;
                                }

#else
                                vx_float32 inImg_data, weight_data;

                                inImg_data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), indexSrc, (vx_uint8_ptr)dataSrc, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                                weight_data = vxnneGetDataExt(weightFormat, TENSOR_QUANT_TYPE(weights), indexWeight, (vx_uint8_ptr)dataWeight, TENSOR_POS(weights), TENSOR_TF_ZEROPOINT(weights), TENSOR_TF_SCALE(weights));
                                sum +=  inImg_data* weight_data;
#if DUMP_ONE_PIXEL_CPU_CONV
                                if(pfile != NULL && my_count == dump_index)
                                {
                                    fprintf(pfile, "indexSrc: %d, indexWeight:%d, X:%d, Y:%d, Z:%d\n",
                                        indexSrc, indexWeight, w, h, d);
                                    fprintf(pfile, "float in * float weight = %0.9f * %0.9f = %0.9f. sum:%0.9f\n", inImg_data, weight_data, inImg_data* weight_data, sum);
                                }
#endif
#endif
                            }
                        }
                    }

                    indexBias = p;
#if QUANT8_SUPPORT
                    if (biasFormat == VX_TYPE_FLOAT32 || biasFormat == VX_TYPE_INT32 || biasFormat == VX_TYPE_FLOAT16)
                    {
                        vxmASSERT(gcmABS(TENSOR_TF_SCALE(biases) - TENSOR_TF_SCALE(inputs) * TENSOR_TF_SCALE(weights)) < 0.000001);

                        vx_int32 bias = ((vx_int32_ptr)dataBias)[indexBias];

                        sum += bias;
                    }

                    if (TENSOR_DATA_TYPE(outputs) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(outputs) == VX_QUANT_AFFINE_SCALE)
                    {
                        vx_float32 input_product_scale = TENSOR_TF_SCALE(inputs) * TENSOR_TF_SCALE(weights);
                        vx_int32 shift = 0, multiplier = 0;
                        vx_float32 m = frexp(input_product_scale / TENSOR_TF_SCALE(outputs), &shift);


                        vx_int32 left_shift = shift > 0 ? shift : 0;
                        vx_int32 right_shift = shift > 0 ? 0 : -shift;

                        vx_int32 output = RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(sum * (1 << left_shift), roundf(m * (1LL << 31))), right_shift) + TENSOR_TF_ZEROPOINT(outputs);

                        if (output > 0xff)
                            output = 0xff;
                        else if (output < 0)
                            output = 0;

                        dataDst[indexOut] = output;
                    }
#else

                    if (biasFormat == VX_TYPE_INT64 || biasFormat== VX_TYPE_FLOAT32 || biasFormat == VX_TYPE_INT32 || biasFormat == VX_TYPE_FLOAT16)
                    {
                        if (dataBias != VX_NULL)
                        {
                            if (biasFormat == VX_TYPE_INT32 && TENSOR_QUANT_TYPE(biases) == VX_QUANT_AFFINE_SCALE)
                                vxmASSERT(gcmABS(TENSOR_TF_SCALE(biases) - TENSOR_TF_SCALE(inputs) * TENSOR_TF_SCALE(weights)) < 0.000001);
                            sum += vxnneGetDataExt(biasFormat, TENSOR_QUANT_TYPE(biases), indexBias, (vx_uint8_ptr)dataBias, TENSOR_POS(biases), TENSOR_TF_ZEROPOINT(biases), TENSOR_TF_SCALE(biases));
                        }
                    }
                    else
                    {
                        vxError("can't support this bias data format\n");
                        gcmASSERT(0);
                    }

#if DUMP_ONE_PIXEL_CPU_CONV
                    if(pfile != NULL)
                    {
                        fclose(pfile);
                    }
#endif

                    if (relu)
                        sum = vxnneActivation(relu->value->e, 0, 0, sum);

                    vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), indexOut, sum, dataDst, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
#endif
                }
            }

            dataWeight += kernelXSize * kernelYSize * inputDepth * vxnneGetTypeSize(weightFormat);
            dataDst += outputWidth * outputHeight * vxnneGetTypeSize(outputFormat);
        }
    }

    return VX_SUCCESS;
}


VX_PRIVATE_API vx_status vxnneExecuteSWConv_UpSample(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation convOperation   = (vxnne_convolution_operation)operation;

    vx_tensor inputs        = convOperation->inputs;
    vx_tensor outputs       = convOperation->outputs;
    vx_scalar dilationX     = convOperation->dilationX;
    vx_scalar dilationY     = convOperation->dilationY;

    vx_type_e input_format = (vx_type_e)(TENSOR_DATA_TYPE(inputs));
    vx_type_e output_format = (vx_type_e)(TENSOR_DATA_TYPE(outputs));

    vx_int32 in_h = TENSOR_SIZE_INDEX(inputs, 1);
    vx_int32 in_w = TENSOR_SIZE_INDEX(inputs, 0);
    vx_int32 in_n = TENSOR_SIZE_INDEX(inputs, 3);

    vx_int32 out_h = TENSOR_SIZE_INDEX(outputs, 1);
    vx_int32 out_w = TENSOR_SIZE_INDEX(outputs, 0);

    vx_int32 dilation_x = dilationX->value->n32 + 1;
    vx_int32 dilation_y = dilationY->value->n32 + 1;

    vx_int32 conv_out_channels = TENSOR_SIZE_INDEX(outputs, 2);

    vx_uint8_ptr input_ptr = inputs->tensorBuffer->memory.logicals[0];
    vx_uint8_ptr output_ptr = outputs->tensorBuffer->memory.logicals[0];

    vx_int32 i = 0, j = 0, b = 0;
    vx_int32 input_item_size = vxnneGetTypeSize(input_format);
    vx_int32 output_item_size = vxnneGetTypeSize(output_format);

    gcfVX_Flush(gcvTRUE);

    for (b = 0; b < conv_out_channels; b ++)/*1024*/
    {
        vx_uint8_ptr output_base = output_ptr + b * out_w * out_h * output_item_size;
        vx_uint8_ptr input_base = input_ptr;
        for (j = 0; j < out_h; j ++)/*19*/
        {
            for (i = 0; i < out_w; i ++)/*19*/
            {
                vx_int32 output_index = j * out_w + i;
                vx_int32 group_x = i % dilation_x, group_y = j % dilation_y;
                vx_int32 input_index = (j / dilation_y) * in_w + (i / dilation_x);
                vx_float32 input_value = 0;

                if (in_n == 1)
                    input_base = input_ptr + ((group_y * dilation_x + group_x) * in_w * in_h + b * in_w * in_h * dilation_x * dilation_y) * input_item_size;
                else
                    input_base = input_ptr + ((group_y * dilation_x + group_x) * in_w * in_h * conv_out_channels + b * in_w * in_h) * input_item_size;

                input_value = vxnneGetDataExt(input_format, TENSOR_QUANT_TYPE(inputs), input_index, (vx_uint8_ptr)input_base, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(outputs), output_index, input_value, output_base, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
            }
        }
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConvolutionLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneExecuteSWConv_Reshuffle(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_deconvolution_reshuffle_operation convOperation   = (vxnne_deconvolution_reshuffle_operation)operation;

    vx_tensor inputs                = convOperation->inputs;
    vx_tensor weights               = convOperation->weights;
    vx_int32 dilation_w             = convOperation->stride_x->value->n32 + 1;
    vx_int32 dilation_h             = convOperation->stride_y->value->n32 + 1;
    vx_int32 pad_x_left             = convOperation->padding_x_left->value->n32;
    vx_int32 pad_x_right            = convOperation->padding_x_right->value->n32;
    vx_int32 pad_y_top              = convOperation->padding_y_top->value->n32;
    vx_int32 pad_y_bottom           = convOperation->padding_y_bottom->value->n32;

    vx_int32 i = 0, j = 0, w = 0, h = 0, dx = 0, dy = 0, c = 0;

    vx_int32 padding_x_left   = pad_x_left/dilation_w;
    vx_int32 padding_x_right  = pad_x_right/dilation_w;
    vx_int32 padding_y_top    = pad_y_top/dilation_h;
    vx_int32 padding_y_bottom = pad_y_bottom/dilation_h;

    if (convOperation->weights_biaes)
        return status;

    if (inputs)
    {
        vx_type_e input_format = (vx_type_e)(TENSOR_DATA_TYPE(inputs));
        vx_int32 item_size = vxnneGetTypeSize(input_format);

        vx_uint8_ptr inputs_ptr         = inputs->tensorBuffer->memory.logicals[0];
        vx_tensor reshuffle_inputs      = convOperation->reshuffled_inputs;
        vx_uint8_ptr reshuffled_inputs  = reshuffle_inputs->tensorBuffer->memory.logicals[0];
        vx_uint8_ptr data = reshuffled_inputs, buffer = VX_NULL;

        vx_int32 input_w = TENSOR_SIZE_INDEX(inputs, 0);
        vx_int32 input_h = TENSOR_SIZE_INDEX(inputs, 1);
        vx_int32 input_c = TENSOR_SIZE_INDEX(inputs, 2);
        vx_int32 input_n = TENSOR_SIZE_INDEX(inputs, 3);

        vx_int32 reshuffle_width = gcmALIGN_NP2(input_w, dilation_w)/dilation_w, reshuffle_height = gcmALIGN_NP2(input_h, dilation_h)/dilation_h;
        vx_int32 slice_size = input_w * input_h, reshuffled_slice_size = reshuffle_width * reshuffle_height;
        vx_int32 batch = input_n;

        gcoOS_MemFill(reshuffled_inputs, 0, item_size * reshuffled_slice_size * input_c * dilation_w * dilation_h);

        if (convOperation->reshuffled)
            vxMemCopy(reshuffled_inputs, inputs_ptr, item_size * slice_size * input_c * batch);
        else
        {
            buffer = (vx_uint8_ptr)vxAllocateAndZeroMemory(item_size * slice_size * input_c * batch);
            data = reshuffled_inputs;


            for (dy = 0; dy < dilation_h; dy++)
            {
                for (dx = 0; dx < dilation_w; dx++)
                {
                    vx_int32 group_index = reshuffled_slice_size * input_c * (dy * dilation_w + dx);

                    for (c = 0; c < input_c; c++)
                    {
                        vx_int32 slice_index = reshuffled_slice_size * c;
                        vx_uint8_ptr reshuffed_data = reshuffled_inputs + (group_index + slice_index) * item_size;
                        vx_uint8_ptr input_data = inputs_ptr + slice_size * c * item_size;

                        for (h = dy - pad_y_top, j = - pad_y_top/dilation_h; h < input_h; h += dilation_h, j++)
                        {
                            for (w = dx - pad_x_left, i = -pad_x_left/dilation_w; w < input_w; w += dilation_w, i ++)
                            {
                                vx_float32 in_data = 0;
                                if (w >= 0 && h >= 0 && i >= 0 && j >= 0)
                                {
                                    //reshuffed_data[j * reshuffle_width + i] = input_data[h * input_w + w];
                                    in_data = vxnneGetDataExt(input_format, TENSOR_QUANT_TYPE(inputs), h * input_w + w, input_data, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                                    vxnneSaveDataExt(input_format, TENSOR_QUANT_TYPE(inputs), j * reshuffle_width + i, in_data, reshuffed_data, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs), TENSOR_ROUNDING_MODE(inputs));
                                }


                            }
                        }
                    }
                }
            }

            vxFree(buffer);
        }


    }
    if (weights && convOperation->reshuffled_weights)
    {
        vx_tensor biases             = convOperation->bias;
        vx_tensor reshuffled_biases  = convOperation->reshuffled_biases;
        vx_tensor reshuffled_weights = convOperation->reshuffled_weights;
        vx_uint8_ptr weights_ptr     = weights->tensorBuffer->memory.logicals[0];
        vx_uint8_ptr reshuffled_weights_ptr = reshuffled_weights->tensorBuffer->memory.logicals[0];
        vx_type_e weight_format = (vx_type_e)(TENSOR_DATA_TYPE(weights));
        vx_int32 item_size = vxnneGetTypeSize(weight_format);


        vx_int32 kernel_w = TENSOR_SIZE_INDEX(weights, 0);
        vx_int32 kernel_h = TENSOR_SIZE_INDEX(weights, 1);
        vx_int32 kernel_c = TENSOR_SIZE_INDEX(weights, 2);
        vx_int32 kernel_n = TENSOR_SIZE_INDEX(weights, 3);

        vx_int32 reshuffled_kernel_w = TENSOR_SIZE_INDEX(reshuffled_weights, 0);
        vx_int32 reshuffled_kernel_h = TENSOR_SIZE_INDEX(reshuffled_weights, 1);

        gcmASSERT(reshuffled_weights_ptr);

        for (j = 0; j < kernel_n; j ++)/*84*/
        {
            vx_int32 orginal_index = j * item_size * kernel_h * kernel_w * kernel_c;
            for (i = 0; i < dilation_w * dilation_h; i ++)/* 2x2=4 */
            {
                for (c = 0; c < dilation_w * dilation_h; c ++)/* 2x2=4 */
                {
                    vx_int32 reshuffled_index = (j * dilation_w * dilation_h * dilation_w * dilation_h + i * (dilation_w * dilation_h) + c) * item_size * reshuffled_kernel_h * reshuffled_kernel_w * kernel_c;
                    if (c == i)
                        memcpy(reshuffled_weights_ptr + reshuffled_index, weights_ptr + orginal_index, item_size * reshuffled_kernel_h * reshuffled_kernel_w * kernel_c);
                    else
                        memset(reshuffled_weights_ptr + reshuffled_index, 0, item_size * reshuffled_kernel_h * reshuffled_kernel_w * kernel_c);
                }
            }
        }

        if (biases && reshuffled_biases)
        {
            vx_type_e biases_format = (vx_type_e)(TENSOR_DATA_TYPE(biases));
            vx_int32 bias_c = TENSOR_SIZE_INDEX(biases, 3);
            vx_int32 r_bias_c = TENSOR_SIZE_INDEX(reshuffled_biases, 3);

            if (bias_c != r_bias_c)
            {
                vx_float32 bias = .0f;
                for (i = 0; i < r_bias_c; i ++)
                {
                    bias = vxnneGetDataExt(biases_format, TENSOR_QUANT_TYPE(biases), i/(dilation_w * dilation_h), biases->tensorBuffer->memory.logicals[0], TENSOR_POS(reshuffled_biases), TENSOR_TF_ZEROPOINT(biases), TENSOR_TF_SCALE(biases));
                    vxnneSaveDataExt(biases_format, TENSOR_QUANT_TYPE(reshuffled_biases), i, bias, reshuffled_biases->tensorBuffer->memory.logicals[0], TENSOR_POS(reshuffled_biases), TENSOR_TF_ZEROPOINT(reshuffled_biases), TENSOR_TF_SCALE(reshuffled_biases), TENSOR_ROUNDING_MODE(reshuffled_biases));

                }

            }
        }
    }

    if (weights && convOperation->create_wbp && (convOperation->weights_biaes == VX_NULL))
    {
        convOperation->weights_biaes = _createWeightsBiasesParameterFromTensors(
                            vxGetContext((vx_reference)convOperation->weights),
                            VX_NN_CONVOLUTION_LAYER,
                            convOperation->reshuffled_inputs->dims,/*inputs_dims,*/
                            convOperation->reshuffled_inputs->dimCount,
                            convOperation->reshuffled_inputs->dimCount,
                            padding_x_left, padding_x_right, padding_y_top, padding_y_bottom,
                            0,/*pooling_size_x,*/
                            0,/*pooling_size_y,*/
                            0,
                            0,
                            VX_NN_DS_SIZE_ROUNDING_FLOOR,
                            convOperation->outputs->dims,/*convolution_outputs_dims,*/
                            convOperation->outputs->dims,/*pool_outputs_dims,*/
                            convOperation->opt, /*optimizations,*/
                            TENSOR_DATA_TYPE(weights),
                            0,
                            VX_TENSOR_RANK_WHCN,
                            convOperation->reshuffled_weights?convOperation->reshuffled_weights:convOperation->weights,
                            convOperation->reshuffled_biases?convOperation->reshuffled_biases:convOperation->bias,
                            VX_NULL,
                            vx_false_e,
                            vx_false_e
                            );

        convOperation->create_wbp = vx_false_e;
    }

    return status;
}

VX_PRIVATE_API vx_status vxnneTensorConstPad(vx_tensor src, vx_tensor dst, vx_uint32 left, vx_uint32 right, vx_uint32 top, vx_uint32 bottom, vx_uint32 constant)
{
    gctPOINTER srcLogical = VX_NULL;
    gctPOINTER dstLogical = VX_NULL;

    vx_int8 srcFp = TENSOR_POS(src);
    vx_int8 dstFp = TENSOR_POS(dst);
    vx_enum  dstRoundingMode = TENSOR_ROUNDING_MODE(dst);

    vx_uint32 dstSize;

    vx_uint32 inWidth           = TENSOR_VIEW_SIZE_INDEX(src, 0);
    vx_uint32 inHeight          = TENSOR_VIEW_SIZE_INDEX(src, 1);
    vx_uint32 outWidth          = TENSOR_VIEW_SIZE_INDEX(dst, 0);
    vx_uint32 outHeight         = TENSOR_VIEW_SIZE_INDEX(dst, 1);
    vx_uint32 ofm               = TENSOR_VIEW_SIZE_INDEX(dst, 2);
    vx_uint32 batch             = TENSOR_VIEW_SIZE_INDEX(dst, 3);
    vx_uint32 x, y, z, w;

    if (inHeight + top + bottom != outHeight ||
        inWidth + left + right != outWidth)
    {
        vxmASSERT(gcvFALSE);
    }

    vxoTensor_GetTensorViewMemory(src, &srcLogical, VX_NULL);
    vxoTensor_GetTensorViewMemory(dst, &dstLogical, VX_NULL);

    vxoTensor_GetTensorSize(dst,&dstSize);

    if (TENSOR_DATA_TYPE(dst) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(dst) == VX_TYPE_UINT8)
    {
        /*Set const value to dst*/
        memset(dstLogical,
            constant,
            dstSize);

        /* Copy non-padding part*/
        for (w = 0; w < batch; w++)
        {
            for (z = 0; z < ofm; z++)
            {
                for (y = top; y < outHeight - bottom; y++)
                {
                    for (x = left; x < outWidth - right; x++)
                    {
                        vx_float32 src0;
                        vx_uint32 srcOffset = (x - left) + inWidth * (y - top) + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                        vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                        src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                    }
                }
            }
        }
    }
    else
    {
        for (w = 0; w < batch; w++)
        {
            for (z = 0; z < ofm; z++)
            {
                for (y = 0; y < outHeight; y++)
                {
                    for (x = 0; x < outWidth; x++)
                    {
                        vx_float32 src0;
                        vx_uint32 srcOffset;
                        vx_uint32 dstOffset = x + outWidth * y + outWidth * outHeight * z + outWidth * outHeight * ofm * w;

                        if (x < left
                            || x >= outWidth - right
                            || y < top
                            || y >= outHeight - bottom)
                        {
                            src0 = (vx_float32)constant;
                        }
                        else
                        {
                            srcOffset = (x - left) + inWidth * (y - top) + inWidth * inHeight * z + inWidth * inHeight * ofm * w;
                            src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(src), TENSOR_QUANT_TYPE(src), srcOffset, (vx_uint8_ptr)srcLogical, srcFp, TENSOR_TF_ZEROPOINT(src), TENSOR_TF_SCALE(src));
                        }
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(dst), TENSOR_QUANT_TYPE(dst), dstOffset, src0, (vx_uint8_ptr)dstLogical, dstFp, TENSOR_TF_ZEROPOINT(dst), TENSOR_TF_SCALE(dst), dstRoundingMode);
                    }
                }
            }
        }
    }

    return VX_SUCCESS;
}


VX_PRIVATE_API vx_status vxnneExecuteSWConv_Convolution_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation convOperation   = (vxnne_convolution_operation)operation;

    if (convOperation->inputs)
        vxoTensor_ReleaseTensor(&convOperation->inputs);

    if (convOperation->padX)
        vxReleaseScalar(&convOperation->padX);

    if (convOperation->padY)
        vxReleaseScalar(&convOperation->padY);

    if (convOperation->downScaleSizeRounding)
        vxReleaseScalar(&convOperation->downScaleSizeRounding);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSW_Depthwise_Convolution_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation convOperation   = (vxnne_convolution_operation)operation;

    if (convOperation->weights)
        vxoTensor_ReleaseTensor(&convOperation->weights);

    if (convOperation->downScaleSizeRounding)
        vxReleaseScalar(&convOperation->downScaleSizeRounding);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecute_Depthwise_Convolution_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_relu_pooling_operation convOperation   = (vxnne_convolution_relu_pooling_operation)operation;

    if (convOperation->weights_biases)
        vxReleaseWeightsBiasesParameter(&convOperation->weights_biases);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWConv_Convolution_DeInilition2(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation convOperation   = (vxnne_convolution_operation)operation;

    vxnneExecuteSWConv_Convolution_DeInilition(operation);

    if (convOperation->weights)
        vxoTensor_ReleaseTensor(&convOperation->weights);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWConv_UpSample_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation convOperation   = (vxnne_convolution_operation)operation;

    if (convOperation->inputs)
        vxoTensor_ReleaseTensor(&convOperation->inputs);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWConv_Reshuffle_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;


    return status;
}

typedef enum _gcoNNConv_Mode
{
    gcoNNE_CONV_MODE_SW,
    gcoNNE_CONV_MODE_SW1,/*dilation_x x dilation_y convolution*/
    gcoNNE_CONV_MODE_SW2,/*1 convolution*/
    gcoNNE_CONV_MODE_NNE_TP,/*1 convolution*/
    gcoNNE_CONV_MODE_NNE_TP2,/*dilation_x x dilation_y convolution*/
    gcoNNE_CONV_MODE_SH,
}
gcoNNConv_Mode;

enum
{
    gcoNNE_CONV_RESHUFFLED_INPUTS = 0,
    gcoNNE_CONV_RESHUFFLED_OUTPUTS,
    gcoNNE_CONV_RESHUFFLED_WEIGHTS,
    gcoNNE_CONV_RESHUFFLED_BIASES,
};

#define DILATION_SELECT_ENV 1

#define QUANT8CHECK(tensor1, tensor2) \
     if (TENSOR_DATA_TYPE(tensor1) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(tensor1) == VX_QUANT_AFFINE_SCALE) \
     { \
         TENSOR_TF_ZEROPOINT(tensor2) = TENSOR_TF_ZEROPOINT(tensor1); \
         TENSOR_TF_SCALE(tensor2) = TENSOR_TF_SCALE(tensor1); \
         TENSOR_QUANT_TYPE(tensor2) = TENSOR_QUANT_TYPE(tensor1); \
         TENSOR_POS(tensor2) = TENSOR_POS(tensor1); \
         TENSOR_PAD_ZERO_VALUE(tensor2) = TENSOR_PAD_ZERO_VALUE(tensor1); \
     }

vx_status vxoNNDilationConvolutionLayer_Deinitialize(vxnne_layer layer)
{
    vxnne_convolution_layer   convolutionLayer = (vxnne_convolution_layer)layer;

    vxnneExecutionLayer_Deinitialize(layer);

    if (convolutionLayer->dynamic_operations)
        gcoOS_Free(VX_NULL, convolutionLayer->dynamic_operations);

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDilationConvolutionLayerInitializer(vx_node node,
        vx_tensor inputs,
        vx_weights_biases_parameter weights_biases,
        vx_tensor weights,
        vx_tensor biases,
        vx_scalar padXLeft,
        vx_scalar padXRight,
        vx_scalar padYTop,
        vx_scalar padYBottom,
        vx_enum   padMode,
        vx_scalar padConstant,
        vx_scalar dilationX,
        vx_scalar dilationY,
        vx_scalar stridesX,
        vx_scalar stridesY,
        vx_scalar relu_s,
        vx_scalar pooling_s,
        vx_scalar poolingX,
        vx_scalar poolingY,
        vx_scalar downScaleSizeRounding,
        vx_tensor outputs)
{
    vx_status status = VX_SUCCESS;

    vxnne_convolution_layer convolutionLayer = VX_NULL;

    vx_uint32 batchCount = TENSOR_SIZE_INDEX(inputs, 3);

    gcoNNConv_Mode mode = gcoNNE_CONV_MODE_NNE_TP2;

    vx_int32 dilation_x = dilationX->value->n32 + 1;
    vx_int32 dilation_y = dilationY->value->n32 + 1;

    vx_bool relu = relu_s?relu_s->value->b:vx_false_e;

    vx_enum pooling = pooling_s?pooling_s->value->e:(VX_NN_POOLING_MAX-1);
    vx_int32 poolingx = poolingX?poolingX->value->n32:0;
    vx_int32 poolingy = poolingY?poolingY->value->n32:0;
    vx_uint32  width             = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    vx_uint32  height            = TENSOR_VIEW_SIZE_INDEX(outputs, 1);
    vx_enum    inputFormat       = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat      = TENSOR_DATA_TYPE(outputs);
    vx_bool    enable_shader     = vx_false_e;
    vx_bool    has_pool          = (pooling_s == NULL && poolingx == 0) ? vx_false_e : vx_true_e;
    vx_bool    enable_2dTensor   = vx_false_e;

    vx_context context = vxGetContext((vx_reference)inputs);
    vx_bool   nnSupportFormat    = vx_false_e;
    vx_bool   tpSupportFormat    = vx_false_e;
    vx_uint32 idx = 0;
    vx_uint32 numTmpTensor = 0;

    if (weights != NULL)
    {
        vx_uint32 kernel_x            = TENSOR_VIEW_SIZE_INDEX(weights, 0);
        vx_uint32 kernel_y            = TENSOR_VIEW_SIZE_INDEX(weights, 1);
        vx_uint32 ifm                 = TENSOR_VIEW_SIZE_INDEX(weights, 2);
        vx_enum   weightFormat        = TENSOR_DATA_TYPE(weights);
        vx_enum   biasFormat          = biases ? TENSOR_DATA_TYPE(biases) : VX_TYPE_FLOAT64;
        vx_uint32 size                = width * height;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            vx_uint32 convsize        = kernel_x * kernel_y * ifm;
            vx_bool   support_type    = vx_false_e;

            if (biases)
            {
                support_type    = (vx_bool)(
                    (inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_INT8  && weightFormat == VX_TYPE_INT8  && biasFormat == VX_TYPE_INT32 && outputFormat != VX_TYPE_FLOAT32)
                    || (inputFormat == VX_TYPE_INT16  && weightFormat == VX_TYPE_INT16  && biasFormat == VX_TYPE_INT32 && outputFormat == VX_TYPE_INT16)
                    || (inputFormat == VX_TYPE_INT16  && weightFormat == VX_TYPE_INT16  && biasFormat == VX_TYPE_INT64 && outputFormat == VX_TYPE_INT16)
                    || (inputFormat == VX_TYPE_UINT8 && weightFormat == VX_TYPE_UINT8 && biasFormat == VX_TYPE_INT32 && outputFormat != VX_TYPE_FLOAT32)
                    || (inputFormat == VX_TYPE_BFLOAT16 && weightFormat == VX_TYPE_BFLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_BFLOAT16));
            }
            else
            {
                support_type    = (vx_bool)(
                    (inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_INT8  && weightFormat == VX_TYPE_INT8  && outputFormat == VX_TYPE_INT8)
                    || (inputFormat == VX_TYPE_INT16  && weightFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
                    || (inputFormat == VX_TYPE_UINT8 && weightFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                    || (inputFormat == VX_TYPE_BFLOAT16 && weightFormat == VX_TYPE_BFLOAT16 && outputFormat == VX_TYPE_BFLOAT16));
            }
            enable_shader = (vx_bool)(convsize < IMG_MAX_WIDTH && support_type && has_pool == vx_false_e);
            enable_2dTensor = (vx_bool)(size < IMG_MAX_WIDTH && dilation_x == 1 && dilation_y == 1);
        }
        else
        {
            vx_bool   support_type    = vx_false_e;

            if (biases)
            {
                support_type    = (vx_bool)
                    ((inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_FLOAT32 && weightFormat == VX_TYPE_FLOAT32 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                    || (inputFormat == VX_TYPE_INT16  && weightFormat == VX_TYPE_INT16  && biasFormat == VX_TYPE_INT64 && outputFormat == VX_TYPE_INT16)
                    || (inputFormat == VX_TYPE_UINT8 && weightFormat == VX_TYPE_UINT8 && biasFormat == VX_TYPE_INT32 && outputFormat == VX_TYPE_UINT8)
                    || (inputFormat == VX_TYPE_BFLOAT16 && weightFormat == VX_TYPE_BFLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_BFLOAT16));
            }
            else
            {
                support_type    = (vx_bool)
                    ((inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                    || (inputFormat == VX_TYPE_FLOAT32 && weightFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                    || (inputFormat == VX_TYPE_UINT8 && weightFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                    || (inputFormat == VX_TYPE_BFLOAT16 && weightFormat == VX_TYPE_BFLOAT16 && outputFormat == VX_TYPE_BFLOAT16));
            }

            /*enable_2dTensor = (vx_bool)(size < IMG_MAX_WIDTH && dilation_x == 1 && dilation_y == 1);*/
            enable_shader = (vx_bool)(support_type && has_pool == vx_false_e);
        }
    }

    nnSupportFormat = vxnneIsNNSupportFormat(context, inputs, VX_NULL, outputs);
    tpSupportFormat = vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs);

    if (tpSupportFormat &&
        nnSupportFormat &&
        vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
        ((dilation_x > 1) || (dilation_y > 1)))
    {
        mode = gcoNNE_CONV_MODE_NNE_TP2;
        enable_shader = vx_false_e;
        enable_2dTensor = vx_false_e;
    }
    else if (enable_shader && vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER))
    {
        mode = gcoNNE_CONV_MODE_SH;
    }
    else if (weights_biases == VX_NULL)
    {
        vxWarning("NN/TP/SH not support this format, goto cpu path. function %s line %d\n", __FUNCTION__, __LINE__);
        mode = gcoNNE_CONV_MODE_SW;
    }
    else
    {
        vxError("Not support this format. function %s line %d\n", __FUNCTION__, __LINE__);
        status = VX_ERROR_NOT_SUPPORTED;
        goto exit;
    }

    /* if weights is dynamic, nn/tp not support*/
    if (weights != VX_NULL && TENSOR_DATA_LIFETIME(weights) == VX_TENSOR_LIFE_TIME_DYNAMIC)
    {
        if (enable_shader && vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER))
            mode = gcoNNE_CONV_MODE_SH;
        else
            mode = gcoNNE_CONV_MODE_SW;
    }

#if DILATION_SELECT_ENV
    {
        gctSTRING MODE = VX_NULL;

        gcoOS_GetEnv(gcvNULL, "DILATION_MODE", &MODE);
        if (MODE != NULL)
        {
            if (gcoOS_StrStr(MODE, "VIP2", VX_NULL))
                mode = gcoNNE_CONV_MODE_NNE_TP2;
            else if (gcoOS_StrStr(MODE, "SW1", VX_NULL))
                mode = gcoNNE_CONV_MODE_SW1;
            else if (gcoOS_StrStr(MODE, "SW2", VX_NULL))
                mode = gcoNNE_CONV_MODE_SW2;
            else if (gcoOS_StrStr(MODE, "VIP", VX_NULL))
                mode = gcoNNE_CONV_MODE_NNE_TP;
            else if (gcoOS_StrStr(MODE, "SW", VX_NULL))
                mode = gcoNNE_CONV_MODE_SW;
            else
                vxError("Not support mode[%s]!", MODE);
        }
    }
#endif

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_convolution_layer_s), (gctPOINTER*)&convolutionLayer);
    if (!convolutionLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(convolutionLayer, sizeof(vxnne_convolution_layer_s));


    gcoOS_Allocate(gcvNULL, sizeof(vxnne_operation_s) * (dilation_x * dilation_y + vxmOPERATION_COUNT(convolutionLayer)), (gctPOINTER*)&convolutionLayer->dynamic_operations);
    gcoOS_ZeroMemory(convolutionLayer->dynamic_operations, sizeof(vxnne_operation_s) * (dilation_x * dilation_y + vxmOPERATION_COUNT(convolutionLayer)));
    vxnneLayer_Initialize(&convolutionLayer->base,
                            "ConvolutionLayer",
                            node,
                            vxmOPERATION_COUNT(convolutionLayer) + dilation_x * dilation_y,
                            convolutionLayer->dynamic_operations,
                            vxoNNDilationConvolutionLayer_Deinitialize);

    switch(mode)
    {
    case gcoNNE_CONV_MODE_SW:

        gcmASSERT(relu == vx_false_e);
        gcmASSERT((pooling != VX_NN_POOLING_MAX) && (pooling != VX_NN_POOLING_AVG));

        vxnneOperation_Initialize(&convolutionLayer->convolutionSW.base,
                                &convolutionLayer->base,
                                VXNNE_OPERATION_TARGET_SW,
                                VXNNE_OPERATOR_CONVOLUTION,
                                vxnneExecuteSWConvolution,
                                VX_NULL,
                                batchCount,
                                0);

        vxnneLayer_SetOperation(
            &convolutionLayer->base,
            &convolutionLayer->convolutionSW.base,
            idx++);

        convolutionLayer->convolutionSW.inputs                = inputs;
        convolutionLayer->convolutionSW.weights               = weights;
        convolutionLayer->convolutionSW.biases                = biases;
        convolutionLayer->convolutionSW.padX                  = padXLeft;
        convolutionLayer->convolutionSW.padXRight             = padXRight;
        convolutionLayer->convolutionSW.padY                  = padYTop;
        convolutionLayer->convolutionSW.padYBottom            = padYBottom;
        convolutionLayer->convolutionSW.dilationX             = dilationX;
        convolutionLayer->convolutionSW.dilationY             = dilationY;
        convolutionLayer->convolutionSW.downScaleSizeRounding = downScaleSizeRounding;
        convolutionLayer->convolutionSW.outputs               = outputs;

        vxnneOperation_AddReference(&convolutionLayer->convolutionSW.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&convolutionLayer->convolutionSW.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&convolutionLayer->convolutionSW.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&convolutionLayer->convolutionSW.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        break;

    case gcoNNE_CONV_MODE_SW1:
    case gcoNNE_CONV_MODE_SW2:
    case gcoNNE_CONV_MODE_NNE_TP:
    case gcoNNE_CONV_MODE_NNE_TP2:
        {
            vx_bool enable_batch = ((gcoNNE_CONV_MODE_SW2 == mode) || (gcoNNE_CONV_MODE_NNE_TP == mode))?vx_false_e:vx_true_e;
            vx_bool need_reshuffle = ((gcoNNE_CONV_MODE_SW2 == mode) || (gcoNNE_CONV_MODE_NNE_TP == mode))?vx_true_e:vx_false_e;

            vx_type_e output_format = (vx_type_e)(TENSOR_DATA_TYPE(outputs));

            vx_int32 item_size = vxnneGetTypeSize(output_format);

            vx_int32 padding_x = padXLeft->value->n32;
            vx_int32 padding_y = padYTop->value->n32;

            vx_int32 in_h = TENSOR_SIZE_INDEX(inputs, 1);
            vx_int32 in_w = TENSOR_SIZE_INDEX(inputs, 0);
            vx_int32 in_c = TENSOR_SIZE_INDEX(inputs, 2);

            vx_int32 k_w = weights?TENSOR_SIZE_INDEX(weights, 0):WB_ORG_KERNEL_X(weights_biases);
            vx_int32 k_h = weights?TENSOR_SIZE_INDEX(weights, 1):WB_ORG_KERNEL_Y(weights_biases);
            vx_int32 out_c = TENSOR_SIZE_INDEX(outputs, 2);

            vx_int32 pad_x = padding_x/dilation_x, pad_y = padding_y/dilation_y;

            vx_int32 inputs_reshuffle_width = gcmALIGN_NP2(in_w, dilation_x)/dilation_x, inputs_reshuffle_height = gcmALIGN_NP2(in_h, dilation_y)/dilation_y;
            vx_int32 reshuffled_out_w = (inputs_reshuffle_width + 2 * padding_x/dilation_x - k_w) + 1;
            vx_int32 reshuffled_out_h = (inputs_reshuffle_height + 2 * padding_y/dilation_y - k_h) + 1;

            vx_context context = vxGetContext((vx_reference)inputs);
            vx_scalar reshuffled_pad_x = vxCreateScalar(context, VX_TYPE_INT32, &pad_x);
            vx_scalar reshuffled_pad_y = vxCreateScalar(context, VX_TYPE_INT32, &pad_y);

            vx_uint32 size[][4] = {
                 /*
                  * {4, 4, 512, 6 * 6},reshuffled input
                  * {4, 4, 1024, 6 * 6}, reshuffled output
                  * {3, 3, 512 * 6 * 6, 1024 * 6 * 6}, reshuffled weights
                  * {1, 1, 1, 512 * 6 * 6}, reshuffled biases
                  */

                 {inputs_reshuffle_width, inputs_reshuffle_height, in_c, dilation_x * dilation_y},
                 {reshuffled_out_w, reshuffled_out_h, out_c, dilation_x * dilation_y},
                 {k_w, k_h, in_c * dilation_x * dilation_y, dilation_x * dilation_y * out_c},
                 {1, 1, 1, out_c * dilation_x * dilation_y},
             };

            vx_int32 up_sample_tp = 1;
            vx_tensor reshuffled_output = VX_NULL;
            vx_tensor reshuffled_input = VX_NULL;
            vx_tensor reshuffled_weights = VX_NULL;
            vx_op_param_s conv;
            vx_tensor_create_params_t tensor_create_params;
            vx_uint8_ptr reshuffled_input_base = VX_NULL;

            gcmASSERT((dilation_x > 1) || (dilation_y > 1));
            memset(&conv, 0, sizeof(vx_op_param_s));

            if ((mode == gcoNNE_CONV_MODE_SW2) || (mode == gcoNNE_CONV_MODE_NNE_TP))
                gcmASSERT((weights_biases == VX_NULL) && (weights != VX_NULL));

            if (!enable_batch)
            {
                size[gcoNNE_CONV_RESHUFFLED_INPUTS][2] *= size[gcoNNE_CONV_RESHUFFLED_INPUTS][3];
                size[gcoNNE_CONV_RESHUFFLED_INPUTS][3] = 1;

                size[gcoNNE_CONV_RESHUFFLED_OUTPUTS][2] *= size[gcoNNE_CONV_RESHUFFLED_OUTPUTS][3];
                size[gcoNNE_CONV_RESHUFFLED_OUTPUTS][3] = 1;
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 4;
            tensor_create_params.sizes = size[gcoNNE_CONV_RESHUFFLED_INPUTS];
            tensor_create_params.data_format = TENSOR_DATA_TYPE(inputs);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(inputs);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
            }
            reshuffled_input = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 4;
            tensor_create_params.sizes = size[gcoNNE_CONV_RESHUFFLED_OUTPUTS];
            tensor_create_params.data_format = TENSOR_DATA_TYPE(outputs);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(reshuffled_input);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(outputs);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(outputs);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(outputs);
            }
            reshuffled_output = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

            vxoTensor_AllocateMemory(reshuffled_input);
            vxoTensor_AllocateMemory(reshuffled_output);

            QUANT8CHECK(inputs, reshuffled_input);
            QUANT8CHECK(outputs, reshuffled_output);

            if (need_reshuffle)
            {
                vx_type_e weight_format = (vx_type_e)(TENSOR_DATA_TYPE(weights));

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = 4;
                tensor_create_params.sizes = size[gcoNNE_CONV_RESHUFFLED_WEIGHTS];
                tensor_create_params.data_format = weight_format;
                tensor_create_params.quant_format = TENSOR_QUANT_TYPE(weights);
                if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                {
                    tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(weights);
                }
                else
                {
                    tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(weights);
                    tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(weights);
                }
                reshuffled_weights = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);
                status = vxoTensor_AllocateMemory(reshuffled_weights);
                if (status)
                {
                    vxError("Reshuffled Weights Out of Memeory!");
                    goto exit;
                }

                QUANT8CHECK(weights, reshuffled_weights);
            }

            vxoTensor_GetTensorViewMemory(reshuffled_output, (gctPOINTER*)&reshuffled_input_base, VX_NULL);
            memset(reshuffled_input_base, 0, size[1][0] * size[1][1] * size[1][2] * size[1][3] * item_size);

            if (need_reshuffle)
            {
                vx_type_e bias_format   = (vx_type_e)(TENSOR_DATA_TYPE(biases));

                vx_tensor reshuffled_biases;

                vx_weights_biases_parameter_optimizations_t opt = {0};
                opt.zrl = -1;

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = 4;
                tensor_create_params.sizes = size[3];
                tensor_create_params.data_format = bias_format;
                tensor_create_params.quant_format = TENSOR_QUANT_TYPE(biases);
                if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                {
                    tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(biases);
                }
                else
                {
                    tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(biases);
                    tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(biases);
                }
                reshuffled_biases = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

                if (TENSOR_DATA_TYPE(reshuffled_input) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffled_input) == VX_QUANT_AFFINE_SCALE)
                {
                    opt.inputZeroPoint = TENSOR_TF_ZEROPOINT(reshuffled_input);
                    opt.outputFormat = TENSOR_DATA_TYPE(reshuffled_output);
                }

                vxoTensor_AllocateMemory(reshuffled_biases);

                QUANT8CHECK(biases, reshuffled_biases);

                convolutionLayer->convolution_sw1_reshuffle_operation.create_wbp                = vx_true_e;
                convolutionLayer->convolution_sw1_reshuffle_operation.weights                   = (need_reshuffle)?weights:VX_NULL;
                convolutionLayer->convolution_sw1_reshuffle_operation.stride_x                  = dilationX;/*for dilation x*/
                convolutionLayer->convolution_sw1_reshuffle_operation.stride_y                  = dilationY;/*for dilation y*/
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_x_left            = padXLeft;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_x_right           = padXRight;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_y_top             = padYTop;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_y_bottom          = padYBottom;
                convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_inputs         = reshuffled_input;
                convolutionLayer->convolution_sw1_reshuffle_operation.outputs                   = reshuffled_output;
                convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_weights        = reshuffled_weights;
                convolutionLayer->convolution_sw1_reshuffle_operation.bias                      = biases;
                convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_biases         = reshuffled_biases;
                convolutionLayer->convolution_sw1_reshuffle_operation.opt                       = &opt;

                vxnneExecuteSWConv_Reshuffle((struct _vxnne_operation_s *)&convolutionLayer->convolution_sw1_reshuffle_operation);
            }

            if ((mode == gcoNNE_CONV_MODE_NNE_TP) || (gcoNNE_CONV_MODE_NNE_TP2 == mode) || !(vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP)))
            {
                status = vxnneOperation_Initialize(&convolutionLayer->convolution_tp_reshuffle_operation.base,
                                                   &convolutionLayer->base,
                                                   VXNNE_OPERATION_TARGET_TP,
                                                   VXNNE_OPERATOR_DILATION_RESHUFFLE,
                                                   VX_NULL,
                                                   vxnneExecuteSWConv_Reshuffle_DeInilition,
                                                   batchCount,
                                                   0);
                if (status != VX_SUCCESS) goto exit;

                vxnneLayer_SetOperation(
                    &convolutionLayer->base,
                    &convolutionLayer->convolution_tp_reshuffle_operation.base,
                    idx++);

                convolutionLayer->convolution_tp_reshuffle_operation.input            = inputs;
                convolutionLayer->convolution_tp_reshuffle_operation.output           = reshuffled_input;

                vxnneOperation_AddReference(&convolutionLayer->convolution_tp_reshuffle_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_tp_reshuffle_operation.base, (vx_reference)reshuffled_input, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                conv.pad_x_left = 0;
                conv.pad_y_top = 0;
                conv.pool_size_x = 0;
                conv.pool_size_y = 0;
                conv.pool_stride = 1;
                conv.enable_relu = vx_false_e;
                conv.pad_mode = padMode;
                conv.pad_const = padConstant ? padConstant->value->n32 : 0;
                conv.tpType = TP_DILATE_RESHUFFLE;
                conv.other_ref = (vx_reference)dilationX;
                conv.data_buff = gcvNULL;

                memcpy(&convolutionLayer->convolution_tp_reshuffle_operation.base.parameter, &conv, sizeof(vx_op_param_s));
            }
            else
            {
                /* Initialize reshuffle weights operation */
                vxnneOperation_Initialize(&convolutionLayer->convolution_sw1_reshuffle_operation.base,
                                          &convolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_DILATION_RESHUFFLE,
                                          vxnneExecuteSWConv_Reshuffle,
                                          VX_NULL,
                                          batchCount,
                                          0);
                vxnneLayer_SetOperation(
                    &convolutionLayer->base,
                    &convolutionLayer->convolution_sw1_reshuffle_operation.base,
                    idx++);

                convolutionLayer->convolution_sw1_reshuffle_operation.inputs            = inputs;
                convolutionLayer->convolution_sw1_reshuffle_operation.weights           = need_reshuffle?weights:VX_NULL;
                convolutionLayer->convolution_sw1_reshuffle_operation.stride_x          = dilationX;/*for dilation x*/
                convolutionLayer->convolution_sw1_reshuffle_operation.stride_y          = dilationY;/*for dilation y*/
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_x_left    = padXLeft;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_y_top     = padYTop;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_x_right   = padXRight;
                convolutionLayer->convolution_sw1_reshuffle_operation.padding_y_bottom  = padYBottom;
                convolutionLayer->convolution_sw1_reshuffle_operation.outputs           = reshuffled_output;
                convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_inputs = reshuffled_input;
                convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_weights= reshuffled_weights;
                convolutionLayer->convolution_sw1_reshuffle_operation.weights_biaes     = weights_biases;

                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                if (need_reshuffle)
                {
                    vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_weights, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                }
            }

            if (gcoNNE_CONV_MODE_NNE_TP2 == mode)
            {
                vx_weights_biases_parameter_optimizations_t opt = {0};
                opt.zrl = -1;

                if (TENSOR_DATA_TYPE(reshuffled_input) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffled_input) == VX_QUANT_AFFINE_SCALE)
                {
                    opt.inputZeroPoint = TENSOR_TF_ZEROPOINT(reshuffled_input);
                    opt.outputFormat = TENSOR_DATA_TYPE(reshuffled_output);
                }

                {
                    if (weights_biases == VX_NULL)
                        convolutionLayer->convolution_sw1_reshuffle_operation2.create_wbp            = vx_true_e;
                    else
                        convolutionLayer->convolution_sw1_reshuffle_operation2.weights_biaes         = weights_biases;

                    convolutionLayer->convolution_sw1_reshuffle_operation2.inputs                    = VX_NULL;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.weights                   = weights;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.stride_x                  = dilationX;/*for dilation x*/
                    convolutionLayer->convolution_sw1_reshuffle_operation2.stride_y                  = dilationY;/*for dilation y*/
                    convolutionLayer->convolution_sw1_reshuffle_operation2.padding_x_left            = padXLeft;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.padding_x_right           = padXRight;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.padding_y_top             = padYTop;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.padding_y_bottom          = padYBottom;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.reshuffled_inputs         = reshuffled_input;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.outputs                   = reshuffled_output;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.bias                      = biases;
                    convolutionLayer->convolution_sw1_reshuffle_operation2.opt                       = &opt;

                    vxnneExecuteSWConv_Reshuffle((struct _vxnne_operation_s *)&convolutionLayer->convolution_sw1_reshuffle_operation2);
                }

                if (convolutionLayer->convolution_sw1_reshuffle_operation2.weights_biaes)
                {
                    /* force enable batch for internal convolution, disabled while sub convolution finished */
                    vx_uint32 loop = dilation_x * dilation_y;
                    vx_uint32 i;
                    vx_tensor *real_inputs = VX_NULL;
                    vx_tensor *real_outputs = VX_NULL;
                    vx_tensor_view input_view = VX_NULL;
                    vx_tensor_view output_view = VX_NULL;
                    vx_uint32 input_start[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
                    vx_uint32 input_end[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
                    vx_uint32 output_start[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
                    vx_uint32 output_end[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};

                    gcoOS_Allocate(gcvNULL, sizeof(vx_tensor) * loop, (gctPOINTER*)&real_inputs);
                    gcoOS_Allocate(gcvNULL, sizeof(vx_tensor) * loop, (gctPOINTER*)&real_outputs);
                    gcoOS_Allocate(gcvNULL, sizeof(vxnne_convolution_relu_pooling_operation_s) * (dilation_x * dilation_y), (gctPOINTER*)&convolutionLayer->convolution_nn_convolution_dynamic_operation);
                    gcoOS_ZeroMemory(convolutionLayer->convolution_nn_convolution_dynamic_operation, sizeof(vxnne_convolution_relu_pooling_operation_s) * (dilation_x * dilation_y));

                    for (i = 0; i < loop; i++)
                    {
                        /* Initialize covolution operation */
                        status = vxnneOperation_Initialize(&convolutionLayer->convolution_nn_convolution_dynamic_operation[i].base,
                                                           &convolutionLayer->base,
                                                           VXNNE_OPERATION_TARGET_NN,
                                                           VXNNE_OPERATOR_CONVOLUTION,
                                                           VX_NULL,
                                                           NULL,
                                                           batchCount,
                                                           NNE_COMMAND_SIZE);
                        if (status != VX_SUCCESS) goto exit;

                        input_end[0] = TENSOR_VIEW_SIZE_INDEX(reshuffled_input, 0);
                        input_end[1] = TENSOR_VIEW_SIZE_INDEX(reshuffled_input, 1);
                        input_end[2] = TENSOR_VIEW_SIZE_INDEX(reshuffled_input, 2);
                        input_end[3] = i + 1;

                        input_start[0] = 0;
                        input_start[1] = 0;
                        input_start[2] = 0;
                        input_start[3] = i;

                        input_view = vxCreateTensorView(context, input_start, input_end, (vx_uint8)reshuffled_input->dimCount);
                        real_inputs[i] = vxoTensor_CreateTensorFromView(reshuffled_input, input_view);

                        output_end[0] = TENSOR_VIEW_SIZE_INDEX(reshuffled_output, 0);
                        output_end[1] = TENSOR_VIEW_SIZE_INDEX(reshuffled_output, 1);
                        output_end[2] = TENSOR_VIEW_SIZE_INDEX(reshuffled_output, 2);
                        output_end[3] = i + 1;

                        output_start[0] = 0;
                        output_start[1] = 0;
                        output_start[2] = 0;
                        output_start[3] = i;

                        output_view = vxCreateTensorView(context, output_start, output_end, (vx_uint8)reshuffled_output->dimCount);
                        real_outputs[i] = vxoTensor_CreateTensorFromView(reshuffled_output, output_view);

                        convolutionLayer->base.temp_tensors[numTmpTensor++] = real_inputs[i];
                        convolutionLayer->base.temp_tensors[numTmpTensor++] = real_outputs[i];

                        vxReleaseTensorView(&input_view);
                        vxReleaseTensorView(&output_view);

                        convolutionLayer->convolution_nn_convolution_dynamic_operation[i].inputs           = real_inputs[i];
                        convolutionLayer->convolution_nn_convolution_dynamic_operation[i].orig_inputs      = inputs;
                        convolutionLayer->convolution_nn_convolution_dynamic_operation[i].weights_biases   = convolutionLayer->convolution_sw1_reshuffle_operation2.weights_biaes;
                        convolutionLayer->convolution_nn_convolution_dynamic_operation[i].outputs          = real_outputs[i];

                        vxnneOperation_AddReference(&convolutionLayer->convolution_nn_convolution_dynamic_operation[i].base, (vx_reference)real_inputs[i], VXNNE_OPERATION_REFENRENCE_INPUT);
                        vxnneOperation_AddReference(&convolutionLayer->convolution_nn_convolution_dynamic_operation[i].base, (vx_reference)real_outputs[i], VXNNE_OPERATION_REFENRENCE_OUTPUT);

                        {
                            conv.pad_x_left = pad_x;
                            conv.pad_x_right = pad_x;
                            conv.pad_y_top = pad_y;
                            conv.pad_y_bottom = pad_y;
                            conv.pad_mode = padMode;
                            conv.pad_const = padConstant ? padConstant->value->n32 : 0;
                            conv.pool_type = pooling;
                            conv.conv_rounding_type = VX_NN_DS_SIZE_ROUNDING_FLOOR;
                            conv.enable_relu = relu;
                            conv.pool_size_x = poolingx;
                            conv.pool_size_y = poolingy;
                            conv.pool_stride = (poolingx > 0) ? 2 : 1;
                            memcpy(&convolutionLayer->convolution_nn_convolution_dynamic_operation[i].base.parameter, &conv, sizeof(vx_op_param_s));
                        }

                        vxnneLayer_SetOperation(
                            &convolutionLayer->base,
                            &convolutionLayer->convolution_nn_convolution_dynamic_operation[i].base,
                            idx++);
                    }
                }
            }
            else if (gcoNNE_CONV_MODE_NNE_TP == mode)
            {
                if (convolutionLayer->convolution_sw1_reshuffle_operation.weights_biaes)
                {
                    /* Initialize covolution operation */
                    status = vxnneOperation_Initialize(&convolutionLayer->convolution_nn_convolution_operation.base,
                                                       &convolutionLayer->base,
                                                       VXNNE_OPERATION_TARGET_NN,
                                                       VXNNE_OPERATOR_CONVOLUTION,
                                                       VX_NULL,
                                                       NULL,
                                                       batchCount,
                                                       NNE_COMMAND_SIZE);
                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &convolutionLayer->base,
                        &convolutionLayer->convolution_nn_convolution_operation.base,
                        idx++);

                    convolutionLayer->convolution_nn_convolution_operation.inputs           = reshuffled_input;
                    convolutionLayer->convolution_nn_convolution_operation.orig_inputs      = inputs;
                    convolutionLayer->convolution_nn_convolution_operation.weights_biases   = convolutionLayer->convolution_sw1_reshuffle_operation.weights_biaes;
                    convolutionLayer->convolution_nn_convolution_operation.outputs          = reshuffled_output;

                    vxnneOperation_AddReference(&convolutionLayer->convolution_nn_convolution_operation.base, (vx_reference)reshuffled_input, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&convolutionLayer->convolution_nn_convolution_operation.base, (vx_reference)reshuffled_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    {
                        conv.pad_x_left = pad_x;
                        conv.pad_x_right = pad_x;
                        conv.pad_y_top = pad_y;
                        conv.pad_y_bottom = pad_y;
                        conv.pad_mode = padMode;
                        conv.pad_const = padConstant->value->n32;
                        conv.pool_type = pooling;
                        conv.conv_rounding_type = VX_NN_DS_SIZE_ROUNDING_FLOOR;
                        conv.enable_relu = relu;
                        conv.pool_size_x = poolingx;
                        conv.pool_size_y = poolingy;
                        conv.pool_stride = (poolingx > 0) ? 2 : 1;
                        memcpy(&convolutionLayer->convolution_nn_convolution_operation.base.parameter, &conv, sizeof(vx_op_param_s));
                    }
                }
            }
            else
            {
                vx_enum downScaleSizeRounding = VX_NN_DS_SIZE_ROUNDING_FLOOR;
                vx_scalar down_scale_s = vxCreateScalar(context, VX_TYPE_ENUM, &downScaleSizeRounding);

                vxnne_convolution_operation op = (vxnne_convolution_operation)vxAllocateAndZeroMemory(sizeof(vxnne_convolution_operation_s));

                /* Initialize covolution operation */
                vxnneOperation_Initialize(&op->base,
                                          &convolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_CONVOLUTION,
                                          vxnneExecuteSWConvolution,
                                          (gcoNNE_CONV_MODE_SW2 == mode)?vxnneExecuteSWConv_Convolution_DeInilition2:vxnneExecuteSWConv_Convolution_DeInilition,
                                          batchCount,
                                          0);

                vxnneLayer_SetOperation(
                    &convolutionLayer->base,
                    &op->base,
                    idx++);

                op->inputs                  = reshuffled_input;
                op->weights                 = (gcoNNE_CONV_MODE_SW2 == mode)?reshuffled_weights:weights;
                op->biases                  = (gcoNNE_CONV_MODE_SW2 == mode)?convolutionLayer->convolution_sw1_reshuffle_operation.reshuffled_biases:biases;
                op->padX                    = reshuffled_pad_x;
                op->padY                    = reshuffled_pad_y;

                op->downScaleSizeRounding   = down_scale_s;
                op->outputs                 = reshuffled_output;

                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            }

            if ((mode == gcoNNE_CONV_MODE_NNE_TP) || (mode == gcoNNE_CONV_MODE_NNE_TP2) || !(vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP)))
            {
                vx_bool need_correct = ((dilation_x * inputs_reshuffle_width != in_w) || (dilation_y * inputs_reshuffle_height != in_h))?vx_true_e:vx_false_e;
                vx_tensor reshuffled_output2 = VX_NULL;

                if (need_correct)
                {
                    vx_uint32 size2[][4] = {
                         /*
                          * {4*6, 4*6, 1024, 1}, reshuffled output2
                          */

                         {reshuffled_out_w * dilation_x, reshuffled_out_h * dilation_x, out_c, 1},
                     };

                    gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                    tensor_create_params.num_of_dims = 4;
                    tensor_create_params.sizes = size2[0];
                    tensor_create_params.data_format = TENSOR_DATA_TYPE(outputs);
                    tensor_create_params.quant_format = TENSOR_QUANT_TYPE(outputs);
                    if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                    {
                        tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(outputs);
                    }
                    else
                    {
                        tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(outputs);
                        tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(outputs);
                    }

                    reshuffled_output2 = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);

                    vxoTensor_AllocateMemory(reshuffled_output2);

                    QUANT8CHECK(outputs, reshuffled_output2);
                }

                {
                    status = vxnneOperation_Initialize(&convolutionLayer->convolution_tp_upsample_operation.base,
                                                   &convolutionLayer->base,
                                                   VXNNE_OPERATION_TARGET_TP,
                                                   VXNNE_OPERATOR_DILATION_UPSAMPLE,
                                                   VX_NULL,
                                                   vxnneExecuteSWConv_UpSample_DeInilition,
                                                   batchCount,
                                                   0);

                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &convolutionLayer->base,
                        &convolutionLayer->convolution_tp_upsample_operation.base,
                        idx++);

                    convolutionLayer->convolution_tp_upsample_operation.input            = reshuffled_output;
                    convolutionLayer->convolution_tp_upsample_operation.output           = need_correct?reshuffled_output2:outputs;

                    vxnneOperation_AddReference(&convolutionLayer->convolution_tp_upsample_operation.base, (vx_reference)reshuffled_output, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&convolutionLayer->convolution_tp_upsample_operation.base, (vx_reference)(need_correct?reshuffled_output2:outputs), VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    conv.pad_x_left = 0;
                    conv.pad_y_top = 0;
                    conv.pool_size_x = 0;
                    conv.pool_size_y = 0;
                    conv.pool_stride = 1;
                    conv.enable_relu = vx_false_e;
                    conv.pad_mode = padMode;
                    conv.pad_const = padConstant ? padConstant->value->n32 : 0;
                    conv.tpType = TP_DILATE_UPSAMPLE;
                    conv.other_ref = gcvNULL;
                    conv.data_buff = gcvNULL;

                    memcpy(&convolutionLayer->convolution_tp_upsample_operation.base.parameter, &conv, sizeof(vx_op_param_s));

                }


                if (need_correct)
                {

                    up_sample_tp ++;
                    status = vxnneOperation_Initialize(&convolutionLayer->convolution_tp_upsample_operation2.base,
                                                       &convolutionLayer->base,
                                                       VXNNE_OPERATION_TARGET_TP,
                                                       VXNNE_OPERATOR_DILATION_UPSAMPLE2,
                                                       VX_NULL,
                                                       vxnneExecuteSWConv_UpSample_DeInilition,
                                                       batchCount,
                                                       0);
                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &convolutionLayer->base,
                        &convolutionLayer->convolution_tp_upsample_operation2.base,
                        idx++);

                    convolutionLayer->convolution_tp_upsample_operation2.input            = reshuffled_output2;
                    convolutionLayer->convolution_tp_upsample_operation2.output           = outputs;

                    vxnneOperation_AddReference(&convolutionLayer->convolution_tp_upsample_operation2.base, (vx_reference)reshuffled_output2, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&convolutionLayer->convolution_tp_upsample_operation2.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    conv.tpType = TP_DILATE_UPSAMPLE2;
                    conv.other_ref = gcvNULL;
                    conv.data_buff = gcvNULL;

                    memcpy(&convolutionLayer->convolution_tp_upsample_operation2.base.parameter, &conv, sizeof(vx_op_param_s));

                }
            }
            else
            {

                /* Initialize upsample operation */
                vxnneOperation_Initialize(&convolutionLayer->convolution_sw1_upsample_operation.base,
                                          &convolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_CONVOLUTION,
                                          vxnneExecuteSWConv_UpSample,
                                          vxnneExecuteSWConv_UpSample_DeInilition,
                                          batchCount,
                                          0);

                vxnneLayer_SetOperation(
                    &convolutionLayer->base,
                    &convolutionLayer->convolution_sw1_upsample_operation.base,
                    idx++);

                convolutionLayer->convolution_sw1_upsample_operation.inputs           = reshuffled_output;
                convolutionLayer->convolution_sw1_upsample_operation.dilationX        = dilationX;
                convolutionLayer->convolution_sw1_upsample_operation.dilationY        = dilationY;

                convolutionLayer->convolution_sw1_upsample_operation.outputs          = outputs;

                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_upsample_operation.base, (vx_reference)reshuffled_output, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolution_sw1_upsample_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }

        }
        break;
        case gcoNNE_CONV_MODE_SH:
            {
#define CONV2D_ALIGN_SIZE4 (4)
#define CONV2D_ALIGN_SIZE16 (16)
                vx_tensor tensor2Row    = NULL;
                vx_uint32 sizes[]       = {1, 1, 1, 1};
                vx_uint32 dims          = TENSOR_DIM_NUM(inputs);
                vx_enum   downScaleSizeRoundingValue = downScaleSizeRounding->value->e;
                vx_int32  paddingLeft   = padXLeft->value->n32;
                vx_int32  paddingRight  = padXRight->value->n32;
                vx_int32  paddingTop    = padYTop->value->n32;
                vx_int32  paddingBottom = padYBottom->value->n32;
                vx_int32  k_w           = weights ? TENSOR_VIEW_SIZE_INDEX(weights, 0):WB_ORG_KERNEL_X(weights_biases);
                vx_int32  k_h           = weights ? TENSOR_VIEW_SIZE_INDEX(weights, 1):WB_ORG_KERNEL_Y(weights_biases);
                vx_int32  inputWidth    = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
                vx_int32  inputHeight   = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
                vx_int32  inputDepth    = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
                vx_int32  outputWidth   = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
                vx_int32  outputHeight  = TENSOR_VIEW_SIZE_INDEX(outputs, 1);
                vx_int32  outputDepth   = TENSOR_VIEW_SIZE_INDEX(outputs, 2);
                vx_uint32 input_size    = k_w * k_h * inputDepth;
                /* Calculate stride     = (w + padXLeft + padXRight - weight)/(output_w - 1) */
                vx_int32  strideX       = (stridesX != VX_NULL) ? (stridesX->value->n32) : (vxoNNExternsionConvlutionRound((vx_float32)(inputWidth + paddingLeft + paddingRight - k_w) / (outputWidth - 1), downScaleSizeRoundingValue));
                vx_int32  strideY       = (stridesY != VX_NULL) ? (stridesY->value->n32) : (vxoNNExternsionConvlutionRound((vx_float32)(inputHeight + paddingTop + paddingBottom - k_h) / (outputHeight - 1), downScaleSizeRoundingValue));
                vx_bool   enableAlign4  = vx_false_e;
                vxnne_shader_executable shaderExecutable = VX_NULL;
                vx_tensor_create_params_t tensor_create_params;
                vx_tensor    newBiases                      = NULL;
                vx_tensor    weights_new_rs                 = NULL;
                vx_tensor    input_rs                       = NULL;
                vx_tensor    outputs_rs                     = NULL;
                vx_bool      is_static_weights_biases       = vx_false_e;
                vx_bool      enable_adjust_biases           = vx_false_e;
                vx_bool      enable_conv2d_1x1              = vx_false_e;
                vx_bool      enable_cast_gemm               = vx_false_e;

                // step 1, tensor to row
                if (enable_2dTensor)
                {
                    sizes[0] = k_w * k_h * inputDepth;
                    sizes[1] = outputWidth * outputHeight;
                    sizes[2] = 1;
                    sizes[3] = batchCount;
                    dims = gcmMAX(2, TENSOR_DIM_NUM(outputs));
                }
                else
                {
                    sizes[0] = k_w * k_h * inputDepth;
                    sizes[1] = outputWidth;
                    sizes[2] = outputHeight;
                    sizes[3] = batchCount;
                    dims = 4;
                }

                if (node->base.context->evisNoInst.supportEVIS == vx_false_e
                    && ((inputWidth * inputHeight < IMG_MAX_WIDTH) && inputDepth < IMG_MAX_WIDTH)
                    && (k_w == k_h && k_w == 1)
                    && (paddingLeft == paddingRight && paddingLeft == 0)
                    && (paddingTop == paddingBottom && paddingTop == 0)
                    && biases != NULL
                    && (CHECK_LIFETIME_IS_STATIC(weights) || TENSOR_QUANT_TYPE(inputs) != VX_QUANT_AFFINE_SCALE)
                    && ((inputWidth * inputHeight % CONV2D_ALIGN_SIZE4 == 0) || (input_size % CONV2D_ALIGN_SIZE16 != 0) || TENSOR_QUANT_TYPE(inputs) != VX_QUANT_AFFINE_SCALE)
                    )
                {
                    enable_conv2d_1x1 = vx_true_e;
                }

                if (node->base.context->evisNoInst.supportEVIS == vx_false_e
                    && (sizes[0] % CONV2D_ALIGN_SIZE4) != 0 && CHECK_LIFETIME_IS_STATIC(weights))
                {
                    sizes[0] = gcmALIGN(sizes[0], CONV2D_ALIGN_SIZE4);

                    enableAlign4 = vx_true_e;

                    input_size = sizes[0];
                }

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = dims;
                tensor_create_params.sizes = sizes;
                tensor_create_params.data_format = TENSOR_DATA_TYPE(inputs);
                tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
                if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                {
                    tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(inputs);
                }
                else
                {
                    tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
                    tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
                }

                if (enable_conv2d_1x1 == vx_false_e)
                {
                    if (enableAlign4)
                    {
                        tensor2Row = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
                        if (tensor2Row == VX_NULL || vxoTensor_AllocateMemory(tensor2Row) != VX_SUCCESS)
                        {
                            vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }
                    }
                    else
                    {
                        tensor2Row = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
                        if (tensor2Row == VX_NULL)
                        {
                            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }
                    }
                }

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable = vxnneTensor2RowShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR2ROW, &node->kernelAttributes.borderMode,
                                       inputs, k_w, k_h, dilation_x, dilation_y, strideX, strideY, paddingLeft, paddingTop, outputWidth, outputHeight, tensor2Row);
                }
                else
                {
                    if (enable_conv2d_1x1 == vx_false_e)
                    {
                        shaderExecutable = vxnneGPUTensor2RowShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR2ROW, &node->kernelAttributes.borderMode,
                                       inputs, k_w, k_h, dilation_x, dilation_y, strideX, strideY, paddingLeft, paddingTop, outputWidth, outputHeight, tensor2Row);
                    }
                }

                if (enable_conv2d_1x1 == vx_false_e)
                {
                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        return status;
                }

                status = vxnneShaderOperation_Initialize(&convolutionLayer->convolutionTensor2Row_sh_operation,
                    &convolutionLayer->base,
                    VXNNE_OPERATOR_CONVOLUTION,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    return status;

                vxnneLayer_SetOperation(
                        &convolutionLayer->base,
                        &convolutionLayer->convolutionTensor2Row_sh_operation.base,
                        idx++);

                    vxnneOperation_AddReference(&convolutionLayer->convolutionTensor2Row_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&convolutionLayer->convolutionTensor2Row_sh_operation.base, (vx_reference)tensor2Row, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                }

                // step 2, gemm
                {
                    is_static_weights_biases = (vx_bool)(CHECK_LIFETIME_IS_STATIC(weights) && CHECK_LIFETIME_IS_STATIC(biases));
                    enable_adjust_biases     = is_static_weights_biases && TENSOR_QUANT_TYPE(weights) == VX_QUANT_AFFINE_SCALE && TENSOR_QUANT_TYPE(biases);

                    if (enable_adjust_biases)
                    {
                        vx_tensor_create_params_t params;
                        gctPOINTER weightsLogical   = VX_NULL;
                        gctPOINTER biasesLogical    = VX_NULL;
                        gctPOINTER newBiasesLogical = VX_NULL;
                        vx_uint32  i                = 0;
                        vx_uint32  j                = 0;
                        vx_uint32  sizes[4]         = {1};
                        vx_uint32  ofm              = TENSOR_VIEW_SIZE_INDEX(biases, 0);
                        vx_uint32  ifm              = k_w * k_h * inputDepth;

                        sizes[0] = TENSOR_VIEW_SIZE_INDEX(biases, 0);
                        sizes[1] = 1;

                        gcoOS_MemFill(&params, 0, sizeof(vx_tensor_create_params_t));
                        params.num_of_dims = TENSOR_DIM_NUM(biases);
                        params.sizes = sizes;
                        params.data_format = TENSOR_DATA_TYPE(biases);
                        params.quant_format = TENSOR_QUANT_TYPE(biases);
                        if (params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                        {
                            params.quant_data.dfp.fixed_point_pos = TENSOR_POS(biases);
                        }
                        else
                        {
                            params.quant_data.affine.scale = TENSOR_TF_SCALE(biases);
                        }

                        newBiases = vxoTensor_CreateTensor(context, NULL, &params, vx_false_e);
                        convolutionLayer->base.temp_tensors[numTmpTensor++] = newBiases;
                        vxoTensor_AllocateMemory(newBiases);
                        if (newBiases == VX_NULL)
                        {
                            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }

                        TENSOR_DATA_LIFETIME(newBiases) = VX_TENSOR_LIFE_TIME_STATIC;

                        vxoTensor_GetTensorViewMemory(weights, &weightsLogical, VX_NULL);
                        vxoTensor_GetTensorViewMemory(biases, &biasesLogical, VX_NULL);
                        vxoTensor_GetTensorViewMemory(newBiases, &newBiasesLogical, VX_NULL);

                        for (j = 0; j < ofm; j++)
                        {
                            vx_int32 offset = 0;
                            vx_int32 dstVal = 0;

                            for (i = 0; i < ifm; i++)
                            {
                                vx_uint32 idx = j * ifm + i;
                                offset = offset - (((vx_uint8*)weightsLogical)[idx] - TENSOR_TF_ZEROPOINT(weights)) * TENSOR_TF_ZEROPOINT(inputs);
                            }

                            dstVal = ((vx_int32*)biasesLogical)[j] + offset;
                            ((vx_int32*)newBiasesLogical)[j] = dstVal;
                        }

                        convolutionLayer->base.temp_tensors[numTmpTensor++] = newBiases;
                    }
                    else
                    {
                        newBiases = biases;
                    }
                }

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    weights_new_rs = weights;
                    input_rs = tensor2Row;
                    outputs_rs = outputs;
                    if (biases)
                    {
                        shaderExecutable = vxnneGemmShaderExecutable(node->base.context, VXNNE_KERNEL_GEMM, &node->kernelAttributes.borderMode, tensor2Row, weights_new_rs, newBiases, VX_NN_ACTIVATION_NONE, enable_2dTensor, outputs_rs);
                    }
                    else
                    {
                        shaderExecutable = vxnneGemm_noBiasShaderExecutable(node->base.context, VXNNE_KERNEL_GEMM_NOBIAS, &node->kernelAttributes.borderMode, tensor2Row, weights_new_rs, VX_NN_ACTIVATION_NONE, enable_2dTensor, outputs_rs);
                    }
                }
                else
                {
                    vx_bool   enable_tensor_cast  = vx_false_e;
                    vx_tensor weights_rs          = NULL;
                    vx_tensor weights_new         = NULL;

                    sizes[0] = enableAlign4 ? gcmALIGN(k_w * k_h * inputDepth, CONV2D_ALIGN_SIZE4) : (k_w * k_h * inputDepth);
                    sizes[1] = outputWidth;
                    sizes[2] = outputHeight;
                    sizes[3] = batchCount;
                    dims = 4;

                    if (enable_conv2d_1x1
                     && (inputWidth * inputHeight % CONV2D_ALIGN_SIZE4 == 0)
                     && TENSOR_DATA_TYPE(inputs) == VX_TYPE_UINT8
                     && TENSOR_DATA_TYPE(weights) == VX_TYPE_UINT8
                     )
                    {
                        enable_tensor_cast = vx_true_e;
                    }
                    else
                    {
                        if (node->base.context->evisNoInst.supportEVIS == vx_false_e
                            && (input_size % CONV2D_ALIGN_SIZE16 == 0)
                            && (inputWidth * inputHeight < IMG_MAX_WIDTH) && (input_size < IMG_MAX_WIDTH)
                            && biases != NULL
                            && (CHECK_LIFETIME_IS_STATIC(weights) && TENSOR_QUANT_TYPE(inputs) == VX_QUANT_AFFINE_SCALE)
                            && enable_conv2d_1x1 == vx_false_e
                            )
                        {
                            enable_cast_gemm = vx_true_e;
                        }
                    }

                    if (enable_conv2d_1x1)
                    {
                        sizes[0] = inputWidth * inputHeight;
                        sizes[1] = 1;
                        sizes[2] = inputDepth;
                        sizes[3] = batchCount;

                        if (enable_tensor_cast)
                        {
                            vx_tensor t = vxoTensor_ReshapeTensor(inputs, (vx_int32*)sizes, TENSOR_DIM_NUM(inputs));

                            input_rs = vxoTensor_ReformatTensor(t, VX_TYPE_UINT32);

                            convolutionLayer->base.temp_tensors[numTmpTensor++] = t;
                            convolutionLayer->base.temp_tensors[numTmpTensor++] = input_rs;
                        }
                        else
                            input_rs = inputs;
                    }
                    else
                    {
                        vx_tensor t = vxoTensor_ReshapeTensor(tensor2Row, (vx_int32*)sizes, dims);
                        if (enable_cast_gemm)
                        {
                            input_rs = vxoTensor_ReformatTensor(t, VX_TYPE_UINT32);
                            convolutionLayer->base.temp_tensors[numTmpTensor++] = input_rs;
                        }
                        else
                            input_rs = t;
                        convolutionLayer->base.temp_tensors[numTmpTensor++] = t;
                    }

                    if (enableAlign4)
                    {
                        vx_uint32 ifm = k_w * k_h * TENSOR_VIEW_SIZE_INDEX(weights, 2);
                        vx_uint32 ofm = TENSOR_VIEW_SIZE_INDEX(weights, 3);

                        sizes[0] = ifm;
                        sizes[1] = ofm;
                        dims     = 2;
                        weights_rs = vxoTensor_ReshapeTensor(weights, (vx_int32*)sizes, dims);
                        convolutionLayer->base.temp_tensors[numTmpTensor++] = weights_rs;

                        sizes[0] = gcmALIGN(ifm, CONV2D_ALIGN_SIZE4);
                        sizes[1] = ofm;
                        dims     = 2;
                        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                        tensor_create_params.num_of_dims = dims;
                        tensor_create_params.sizes = sizes;
                        tensor_create_params.data_format = TENSOR_DATA_TYPE(weights);
                        tensor_create_params.quant_format = TENSOR_QUANT_TYPE(weights);
                        if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                        {
                            tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(weights);
                        }
                        else
                        {
                            tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(weights);
                            tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(weights);
                        }

                        weights_new = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
                        convolutionLayer->base.temp_tensors[numTmpTensor++] = weights_new;
                        vxoTensor_AllocateMemory(weights_new);
                        if (weights_new == VX_NULL)
                        {
                            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }

                        TENSOR_DATA_LIFETIME(weights_new) = VX_TENSOR_LIFE_TIME_STATIC;

                        vxnneTensorConstPad(weights_rs, weights_new, 0, gcmALIGN(ifm, CONV2D_ALIGN_SIZE4) - ifm, 0, 0, TENSOR_TF_ZEROPOINT(weights));

                        sizes[0] = gcmALIGN(ifm, CONV2D_ALIGN_SIZE4);
                        sizes[1] = 1;
                        sizes[2] = 1;
                        sizes[3] = ofm;
                        dims     = 4;

                        if (enable_cast_gemm)
                        {
                            vx_tensor t = vxoTensor_ReshapeTensor(weights_new, (vx_int32*)sizes, dims);
                            weights_new_rs = vxoTensor_ReformatTensor(t, VX_TYPE_UINT32);

                            convolutionLayer->base.temp_tensors[numTmpTensor++] = t;
                        }
                        else
                        {
                            weights_new_rs = vxoTensor_ReshapeTensor(weights_new, (vx_int32*)sizes, dims);
                        }

                        convolutionLayer->base.temp_tensors[numTmpTensor++] = weights_new_rs;
                    }
                    else
                    {
                        if (enable_cast_gemm)
                        {
                            vx_tensor t = NULL;

                            sizes[0] = k_w * k_h * TENSOR_VIEW_SIZE_INDEX(weights, 2);
                            sizes[1] = 1;
                            sizes[2] = 1;
                            sizes[3] = TENSOR_VIEW_SIZE_INDEX(weights, 3);
                            dims     = 4;

                            t = vxoTensor_ReshapeTensor(weights, (vx_int32*)sizes, dims);
                            convolutionLayer->base.temp_tensors[numTmpTensor++] = t;

                            weights_new_rs = vxoTensor_ReformatTensor(t, VX_TYPE_UINT32);
                            convolutionLayer->base.temp_tensors[numTmpTensor++] = weights_new_rs;
                        }
                        else
                            weights_new_rs = weights;
                    }

                    if (biases)
                    {
                        if (enable_conv2d_1x1)
                        {
                            if (enable_tensor_cast)
                            {
                                vx_tensor t = NULL;

                                sizes[0] = inputWidth * inputHeight;
                                sizes[1] = 1;
                                sizes[2] = outputDepth;
                                sizes[3] = batchCount;

                                t = vxoTensor_ReshapeTensor(outputs, (vx_int32*)sizes, TENSOR_DIM_NUM(outputs));
                                convolutionLayer->base.temp_tensors[numTmpTensor++] = t;

                                outputs_rs = vxoTensor_ReformatTensor(t, VX_TYPE_UINT32);
                                convolutionLayer->base.temp_tensors[numTmpTensor++] = outputs_rs;
                            }
                            else
                                outputs_rs = outputs;

                            shaderExecutable = vxnneGPUConv2D_1x1ShaderExecutable(node->base.context, VXNNE_KERNEL_CONVOLUTION_1X1,
                                &node->kernelAttributes.borderMode, enable_tensor_cast, input_rs, weights_new_rs, newBiases, outputs_rs);
                        }
                        else
                        {
                            outputs_rs = outputs;
                            shaderExecutable = vxnneGPUGemmShaderExecutable(node->base.context, VXNNE_KERNEL_GEMM,
                                &node->kernelAttributes.borderMode, enable_cast_gemm, input_rs, weights_new_rs, newBiases, outputs_rs);
                        }
                    }
                    else
                    {
                        outputs_rs = outputs;
                        shaderExecutable = vxnneGPUGemm_noBiasShaderExecutable(node->base.context, VXNNE_KERNEL_GEMM_NOBIAS, &node->kernelAttributes.borderMode, input_rs, weights_new_rs, outputs_rs);
                    }
                }

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    return status;
                }

                status = vxnneShaderOperation_Initialize(&convolutionLayer->convolutionGemm_sh_operation,
                    &convolutionLayer->base,
                    VXNNE_OPERATOR_CONVOLUTION,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    return status;

                if (batchCount > 1)
                {
                    vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
                    vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 2, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
                }

                vxnneLayer_SetOperation(
                    &convolutionLayer->base,
                    &convolutionLayer->convolutionGemm_sh_operation.base,
                    idx++);

                vxnneOperation_AddReference(&convolutionLayer->convolutionGemm_sh_operation.base, (vx_reference)input_rs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolutionGemm_sh_operation.base, (vx_reference)weights_new_rs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolutionGemm_sh_operation.base, (vx_reference)newBiases, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&convolutionLayer->convolutionGemm_sh_operation.base, (vx_reference)outputs_rs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                convolutionLayer->base.temp_tensors[numTmpTensor++] = tensor2Row;

                break;
            }
        default:
            vxError("ERROR MODE: %d\n", mode);
            break;
    }

    convolutionLayer->base.num_temp_tensors = numTmpTensor;
    node->layer = &convolutionLayer->base;

exit:
    return status;
}


VX_PRIVATE_API vx_status vxoNNSWDepthwiseConvolution(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_depthwise_convolution_operation dcOperation = (vxnne_depthwise_convolution_operation)operation;


    vx_tensor inputs  = dcOperation->inputs;
    vx_tensor outputs = dcOperation->outputs;
    vx_tensor weights = dcOperation->weights;
    vx_tensor biases  = dcOperation->biases;
    vx_scalar padXLeft = dcOperation->padXLeft;
    vx_scalar padXRight = dcOperation->padXRight;
    vx_scalar padYTop = dcOperation->padYTop;
    /* vx_scalar padYBottom = dcOperation->padYBottom; */
    vx_scalar downScaleSizeRounding = dcOperation->downScaleSizeRounding;

    vx_int32 channel_multiplier;

    vx_int32 input_width = TENSOR_SIZE_INDEX(inputs, 0);    /* W */
    vx_int32 input_height = TENSOR_SIZE_INDEX(inputs, 1);   /* H */
    vx_int32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);    /* D */
    /*vx_int32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);*/    /* N */

    vx_int32 output_width = TENSOR_SIZE_INDEX(outputs, 0);   /* W */
    vx_int32 output_height = TENSOR_SIZE_INDEX(outputs, 1);  /* H */
    vx_int32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);   /* D */
    vx_int32 output_batch = TENSOR_SIZE_INDEX(outputs, 3);   /* N */

    vx_int32 kernel_width = TENSOR_SIZE_INDEX(weights, 0);      /* W */
    vx_int32 kernel_height = TENSOR_SIZE_INDEX(weights, 1);     /* H */
    /*vx_int32 kernel_in_depth = TENSOR_SIZE_INDEX(weights, 2);*/   /* D */
    /*vx_int32 kernel_out_depth = TENSOR_SIZE_INDEX(weights, 3);*/  /* N */
    vx_int32 pad_left = padXLeft->value->n32, pad_right = padXRight->value->n32;
    vx_int32 pad_top = padYTop->value->n32/*, pad_bottom = padYBottom->value->n32*/;
    vx_float64 sum = .0f, in = .0f, weight = .0f;
    vx_int32 stride = 1;
    vx_int32 b = 0, d = 0, h = 0, w = 0, dm = 0, m = 0, n = 0, kstart_x = 0, kstart_y = 0;

    if ((input_width == 1 && input_height == 1) || (output_width == 1))
        stride = 1;
    else
    {
        /* Calculate stride = (w + padXLeft + padXRight - weight)/(output_w - 1) */
        stride = vxoNNExternsionConvlutionRound((vx_float32)(input_width + pad_left + pad_right - kernel_width) / (output_width - 1), downScaleSizeRounding->value->e);
    }
    gcmASSERT(stride > 0);

    channel_multiplier = (dcOperation->depth_multiplier != VX_NULL) ? dcOperation->depth_multiplier->value->n32 : 1;

    for (b = 0; b < output_batch; b ++)
    {
        for (d = 0; d < output_depth; d ++)
        {
            vx_int32 output_base_offset = d * output_width * output_height + b * output_depth * output_width * output_height;
            vx_int32 input_slice_index = d / channel_multiplier;
            vx_int32 input_base_offset;

            if (input_slice_index < input_depth)
            {
                input_base_offset = input_slice_index * input_width * input_height + b * input_depth * input_width * input_height;
            }
            else
            {
                input_base_offset = -1;
            }

            for (h = 0; h < output_height; h ++)
            {
                vx_int32 y_start = h * stride - pad_top;
                vx_int32 y_end = gcmMIN((y_start + kernel_height), input_height);

                kstart_y = (y_start < 0) ? -y_start : 0;
                y_start = gcmMAX(0, y_start);

                for (w = 0; w < output_width; w ++)
                {
                    vx_int32 x = 0, y = 0;
                    vx_int32 x_start = w * stride - pad_left;
                    vx_int32 x_end = gcmMIN((x_start + kernel_width), input_width);


                    kstart_x = (x_start < 0)?-x_start:0;
                    x_start = gcmMAX(0, x_start);

                    sum = 0.f;

                    if (input_base_offset != -1)
                    {
                        for (y = y_start, n = kstart_y; y < y_end; y++, n ++)
                        {
                            for (x = x_start, m = kstart_x; x < x_end; x++, m ++)
                            {
                                in = VX_GET_DATA_FROM_TENSOR(inputs, y * input_width + x + input_base_offset);
                                weight = VX_GET_DATA_FROM_TENSOR(weights, n * kernel_width + m + (d * channel_multiplier + dm) * kernel_width * kernel_height);

                                sum += in * weight;
                            }
                        }
                    }

                    if (biases)
                    {
                        sum += VX_GET_DATA_FROM_TENSOR(biases, d * channel_multiplier + dm);
                    }

                    VX_SAVE_DATA_TO_TENSOR(outputs, sum, h * output_width + w + output_base_offset);
                }
            }
        }
    }

    return status;

}

VX_PRIVATE_API vx_status vxoNNSWDepthwiseConv_DilationWeight(vx_tensor weights, vx_tensor weight_dilation, vx_int32 channels_multiplier)
{
    vx_status status = VX_SUCCESS;

    vx_int32 weights_width = TENSOR_SIZE_INDEX(weight_dilation, 0), weights_height = TENSOR_SIZE_INDEX(weight_dilation, 1), weights_input_depth = TENSOR_SIZE_INDEX(weight_dilation, 2), weights_output_depth = TENSOR_SIZE_INDEX(weight_dilation, 3);
    vx_int32 d = 0, b = 0, item_size = TENSOR_DATA_SIZE(weights), slice = weights_width * weights_height, multiplier = weights_output_depth/weights_input_depth;
    vx_uint8_ptr input_base = TENSOR_LOGICAL_ADDR(weights);
    vx_uint8_ptr output_base = VX_NULL;

    gcmASSERT(channels_multiplier == multiplier);

    if (TENSOR_LOGICAL_ADDR(weight_dilation) == VX_NULL && vxoTensor_AllocateMemory(weight_dilation) != VX_SUCCESS)
        return VX_ERROR_NO_MEMORY;

    output_base = TENSOR_LOGICAL_ADDR(weight_dilation);

    for (b = 0; b < weights_output_depth; b ++)
    {
        vx_int32 output_offset = b * weights_input_depth * slice;
        for (d = 0; d < weights_input_depth; d ++)
        {
            if (TENSOR_DATA_TYPE(weights) == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(weights) == VX_QUANT_AFFINE_SCALE && (TENSOR_DATA_TYPE(weights) == (TENSOR_DATA_TYPE(weight_dilation))))
            {
                vx_int32 s = 0;
                for (s = 0; s < slice; s++)
                {
                    if (d == (b/multiplier))
                        output_base[output_offset + d * slice + s] = input_base[b * slice + s];
                    else
                        output_base[output_offset + d * slice + s] = (vx_uint8)TENSOR_TF_ZEROPOINT(weights);
                }
            }
            else
            {
                if (d == (b/multiplier))
                    memcpy(output_base + output_offset * item_size + d * slice * item_size, input_base + b * slice * item_size, slice * item_size);
                else
                    memset(output_base + output_offset * item_size + d * slice * item_size, 0, slice * item_size);
            }
        }
    }

    return status;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDepthwiseConvolutionLayerInitializer(vx_node node,
        vx_tensor inputs,
        vx_weights_biases_parameter weights_biases,
        vx_tensor weights,
        vx_tensor biases,
        vx_scalar padXLeft,
        vx_scalar padXRight,
        vx_scalar padYTop,
        vx_scalar padYBottom,
        vx_enum   padMode,
        vx_scalar padConstant,
        vx_scalar dilationX,
        vx_scalar dilationY,
        vx_scalar strideX,
        vx_scalar strideY,
        vx_scalar depth_multiplier,
        vx_scalar relu_s,
        vx_scalar pooling_s,
        vx_scalar poolingX,
        vx_scalar poolingY,
        vx_scalar downScaleSizeRounding,
        vx_tensor outputs)
{

    vx_status status = VX_SUCCESS;

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);

    vxnne_depthwise_convolution_layer  depthwiseConvolutionLayer         = VX_NULL;

    vx_bool enable_nne = vx_false_e;
    vx_context context = vxGetContext((vx_reference)inputs);
    vx_uint32  operation_idx = 0;
    vx_uint32  numTmpTensor = 0;
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_depthwise_convolution_layer_s), (gctPOINTER*)&depthwiseConvolutionLayer);
    if (!depthwiseConvolutionLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(depthwiseConvolutionLayer, sizeof(vxnne_depthwise_convolution_layer_s));

    vxnneLayer_Initialize(&depthwiseConvolutionLayer->base,
                          "DepthwiseConvolutionLayer",
                          node,
                          vxmOPERATION_COUNT(depthwiseConvolutionLayer),
                          depthwiseConvolutionLayer->operations,
                          VX_NULL);

    if ((TENSOR_DATA_LIFETIME(weights) == VX_TENSOR_LIFE_TIME_STATIC) &&
        (biases == VX_NULL) &&
        vxnneIsNNSupportFormat(context, inputs, VX_NULL, outputs))
    {
        enable_nne = vx_true_e;
    }
    else if ((TENSOR_DATA_LIFETIME(weights) == VX_TENSOR_LIFE_TIME_STATIC) &&
        (TENSOR_DATA_LIFETIME(biases) == VX_TENSOR_LIFE_TIME_STATIC) &&
        vxnneIsNNSupportFormat(context, inputs, VX_NULL, outputs))
    {
        enable_nne = vx_true_e;
    }

    /* Check if nn support fp16 */
    if (node->base.context->nnConfig.fixedFeature.nnCoreCountFloat16 == 0 &&
        (TENSOR_DATA_TYPE(inputs) == VX_TYPE_FLOAT16 || TENSOR_DATA_TYPE(weights) == VX_TYPE_FLOAT16 || TENSOR_DATA_TYPE(outputs) == VX_TYPE_FLOAT16))
        enable_nne = vx_false_e;

    if (enable_nne)
    {
        {
            vx_int32 channels_multiplier = depth_multiplier->value->n32;
            vx_uint32 size[][4] = {
                {TENSOR_SIZE_INDEX(weights, 0), TENSOR_SIZE_INDEX(weights, 1), TENSOR_SIZE_INDEX(inputs, 2), TENSOR_SIZE_INDEX(weights, 2)},
            };

            vx_bool sw_convolution = vx_false_e;

            vx_tensor_create_params_t tensor_create_params;
            vx_tensor weight_dilation;

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = TENSOR_DIM_NUM(weights);
            tensor_create_params.sizes = size[0];
            tensor_create_params.data_format = TENSOR_DATA_TYPE(weights);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(weights);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(weights);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(weights);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(weights);
            }

            weight_dilation = vxoTensor_CreateTensor(vxGetContext((vx_reference)node), node->graph, &tensor_create_params, vx_false_e);

            vxoNNSWDepthwiseConv_DilationWeight(weights, weight_dilation, channels_multiplier);

            if (sw_convolution)
            {
                vx_enum downScaleSizeRounding = VX_NN_DS_SIZE_ROUNDING_FLOOR;
                vx_scalar down_scale_s = vxCreateScalar(vxGetContext((vx_reference)node), VX_TYPE_ENUM, &downScaleSizeRounding);

                /* Initialize covolution operation */
                vxnneOperation_Initialize(&depthwiseConvolutionLayer->convolution_nn_convolution_sw.base,
                                          &depthwiseConvolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_CONVOLUTION,
                                          vxnneExecuteSWConvolution,
                                          vxnneExecuteSW_Depthwise_Convolution_DeInilition,
                                          batchCount,
                                          0);

                vxnneLayer_SetOperation(
                    &depthwiseConvolutionLayer->base,
                    &depthwiseConvolutionLayer->convolution_nn_convolution_sw.base,
                    0);

                depthwiseConvolutionLayer->convolution_nn_convolution_sw.inputs                  = inputs;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.weights                 = weight_dilation;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.biases                  = biases;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.padX                    = padXLeft;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.padY                    = padYTop;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.downScaleSizeRounding   = down_scale_s;
                depthwiseConvolutionLayer->convolution_nn_convolution_sw.outputs                 = outputs;

                vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_nn_convolution_sw.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_nn_convolution_sw.base, (vx_reference)weight_dilation, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_nn_convolution_sw.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_nn_convolution_sw.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            }
            else
            {
                vx_weights_biases_parameter_optimizations_t opt = { -1, TENSOR_DATA_TYPE(outputs), TENSOR_TF_ZEROPOINT(inputs)};
                vx_weights_biases_parameter weights_biases = _createWeightsBiasesParameterFromTensors(
                                    vxGetContext((vx_reference)node),
                                    VX_NN_CONVOLUTION_LAYER,
                                    inputs->dims,/*inputs_dims,*/
                                    inputs->dimCount,
                                    inputs->dimCount,
                                    padXLeft->value->n32,
                                    padXRight->value->n32,
                                    padYTop->value->n32,
                                    padYBottom->value->n32,
                                    0,/*pooling_size_x,*/
                                    0,/*pooling_size_y,*/
                                    strideX->value->n32,
                                    strideY->value->n32,
                                    VX_NN_DS_SIZE_ROUNDING_FLOOR,
                                    outputs->dims,/*convolution_outputs_dims,*/
                                    outputs->dims,/*pool_outputs_dims,*/
                                    &opt, /*optimizations,*/
                                    TENSOR_DATA_TYPE(weight_dilation),
                                    0,
                                    VX_TENSOR_RANK_WHCN,
                                    weight_dilation,
                                    biases,
                                    VX_NULL,
                                    vx_false_e,
                                    vx_false_e
                                    );

                status = vxnneConvolutionReluPoolingInitializer(node,
                    "ConvolutionReluLayer",
                    inputs,
                    weights_biases,
                    dilationX->value->n32,
                    dilationY->value->n32,
                    padXLeft->value->n32,
                    padXRight->value->n32,
                    padYTop->value->n32,
                    padYBottom->value->n32,
                    VX_NN_DS_SIZE_ROUNDING_FLOOR,
                    vx_false_e,
                    vx_false_e,
                    VIV_NN_POOLING_NON,
                    0,
                    0,
                    VX_PAD_CONSTANT,
                    VX_NULL,
                    outputs);

                goto exit;
            }
        }
    }
    else
    {
        vx_enum  inputFormat                       = TENSOR_DATA_TYPE(inputs);
        vx_enum  weightFormat                      = TENSOR_DATA_TYPE(weights);
        vx_enum  biasFormat                        = VX_TYPE_INVALID;
        vx_enum  outputFormat                      = TENSOR_DATA_TYPE(outputs);
        vx_bool  dataformat_flag[4]                = {vx_false_e};
        vx_bool  depthwiseConv_shader_flag         = vx_false_e;

        if (biases != VX_NULL)
            biasFormat = TENSOR_DATA_TYPE(biases);

        if(context->evisNoInst.supportEVIS)
            dataformat_flag[0]        = (vx_bool)(inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16);
        else
            dataformat_flag[0]        = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16) ||
                                                  (inputFormat == VX_TYPE_FLOAT16 && weightFormat == VX_TYPE_FLOAT16 && biasFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                                  (inputFormat == VX_TYPE_FLOAT32 && weightFormat == VX_TYPE_FLOAT32 && biasFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32));
        dataformat_flag[1]        = (vx_bool)(inputFormat == VX_TYPE_UINT8 && weightFormat == VX_TYPE_UINT8 && biasFormat == VX_TYPE_INT32 && outputFormat == VX_TYPE_UINT8);
        dataformat_flag[2]        = (vx_bool)(inputFormat == VX_TYPE_INT8 && weightFormat == VX_TYPE_INT8 && biasFormat == VX_TYPE_INT32 && outputFormat == VX_TYPE_INT8);
        dataformat_flag[3]        = (vx_bool)(inputFormat == VX_TYPE_INT16 && weightFormat == VX_TYPE_INT16 && biasFormat == VX_TYPE_INT32 && outputFormat == VX_TYPE_INT16);
        depthwiseConv_shader_flag = (vx_bool) ((dataformat_flag[0] || dataformat_flag[1] || dataformat_flag[2] || dataformat_flag[3]) && biases);

        if (depthwiseConv_shader_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = NULL;
            vx_tensor               newBiases        = NULL;
            vx_tensor               tensorCopy       = NULL;

            if(node->base.context->evisNoInst.supportEVIS)
            {
                newBiases  = biases;
                tensorCopy = inputs;
                shaderExecutable = vxnneGetDepthwiseConvShaderExecutable(node->base.context,
                                                                         VXNNE_KERNEL_DEPTHWISE_CONV,
                                                                         &node->kernelAttributes.borderMode,
                                                                         tensorCopy,
                                                                         weights,
                                                                         newBiases,
                                                                         padXLeft,
                                                                         padXRight,
                                                                         padYTop,
                                                                         padYBottom,
                                                                         padMode,
                                                                         padConstant,
                                                                         dilationX,
                                                                         dilationY,
                                                                         depth_multiplier,
                                                                         relu_s,
                                                                         pooling_s,
                                                                         poolingX,
                                                                         poolingY,
                                                                         downScaleSizeRounding,
                                                                         outputs);
            }
            else
            {
            #define DWCONV_ALIGN_SIZE4 (4)
                vx_bool       enable_adjust_biases           = vx_false_e;
                vx_int32      kernel_width                   = TENSOR_VIEW_SIZE_INDEX(weights, 0);
                vx_int32      kernel_height                  = TENSOR_VIEW_SIZE_INDEX(weights, 1);
                vx_bool       is_static_weights_biases       = vx_false_e;
                vx_int32      padLeftv                       = padXLeft->value->n32;
                vx_int32      padRightv                      = padXRight->value->n32;
                vx_int32      padTopv                        = padYTop->value->n32;
                vx_int32      padBottomv                     = padYBottom->value->n32;
                vx_bool       is_copy_tensor                 = vx_false_e;
                vx_scalar     padLeft                        = VX_NULL;
                vx_scalar     padRight                       = VX_NULL;
                vx_scalar     padTop                         = VX_NULL;
                vx_scalar     padBottom                      = VX_NULL;
                vx_int32      strideXvalue                   = 0;
                vx_int32      strideYvalue                   = 0;
                vx_uint32     input_width                    = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
                vx_uint32     input_height                   = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
                vx_uint32     output_width                   = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
                vx_uint32     output_height                  = TENSOR_VIEW_SIZE_INDEX(outputs, 1);

                if ((input_width == 1 && input_height ==1) || (output_width == 1 && input_width == (vx_uint32)kernel_width && padLeftv == 0))
                {
                    strideXvalue = strideYvalue = 1;
                }
                else
                {
                    strideXvalue = vxoNNExternsionConvlutionRound((vx_float32)(input_width + padLeftv + padRightv - kernel_width) / (output_width - 1), downScaleSizeRounding->value->e);
                    strideYvalue = vxoNNExternsionConvlutionRound((vx_float32)(input_height + padTopv + padBottomv - kernel_height) / (output_height - 1), downScaleSizeRounding->value->e);
                }
                is_copy_tensor = ((inputFormat == VX_TYPE_UINT8) && (3 == kernel_width) && (3 == kernel_height) && (strideXvalue == 1) && (output_width % 4 == 0)
                                 && (padLeftv == 1 || padRightv == 1 || padTopv == 1 || padBottomv == 1));
                if (is_copy_tensor)
                {
                    vx_tensor_create_params_t tensor_create_params;
                    vx_uint32 sizes[]       = {1, 1, 1, 1};
                    vx_uint32 inputWidth   = 0;
                    vx_uint32 copy_dims     = TENSOR_DIM_NUM(inputs);
                    gctPOINTER inputLogical = VX_NULL;
                    vx_uint8   inputZP      = (vx_uint8)TENSOR_TF_ZEROPOINT(inputs);
                    vx_uint32  copy_size     = 0;


                    sizes[0] = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
                    sizes[1] = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
                    sizes[2] = copy_dims > 2 ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
                    sizes[3] = copy_dims > 3 ? TENSOR_VIEW_SIZE_INDEX(inputs, 3) : 1;
                    sizes[0] = sizes[0] + padLeftv + padRightv;
                    inputWidth = sizes[0];
                    sizes[1] = sizes[1] + padTopv + padBottomv;
                    if (sizes[0] % DWCONV_ALIGN_SIZE4 != 0)
                    {
                         sizes[0] = gcmALIGN(sizes[0], DWCONV_ALIGN_SIZE4);
                    }
                    gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                    tensor_create_params.num_of_dims = copy_dims;
                    tensor_create_params.sizes = sizes;
                    tensor_create_params.data_format = TENSOR_DATA_TYPE(inputs);
                    tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);
                    if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                    {
                        tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(inputs);
                    }
                    else
                    {
                        tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(inputs);
                        tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(inputs);
                    }
                    tensorCopy = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_false_e);
                    if (tensorCopy == VX_NULL || vxoTensor_AllocateMemory(tensorCopy) != VX_SUCCESS)
                    {
                        vxError("vxoTensor_AllocateMemory fail at function %s, line %d", __FUNCTION__, __LINE__);
                        status = VX_ERROR_NO_MEMORY;
                        goto exit;
                    }
                    vxoTensor_GetTensorViewMemory(tensorCopy, &inputLogical, VX_NULL);
                    copy_size = sizes[0] * sizes[1] * sizes[2] * sizes[3];
                    gcoOS_MemFill(inputLogical, inputZP, copy_size);
                    padLeftv   = 0;
                    padRightv  = 0;
                    padTopv    = 0;
                    padBottomv = 0;
                    padLeft   = vxCreateScalar(context, VX_TYPE_INT32, &padLeftv);
                    padRight  = vxCreateScalar(context, VX_TYPE_INT32, &padRightv);
                    padTop    = vxCreateScalar(context, VX_TYPE_INT32, &padTopv);
                    padBottom = vxCreateScalar(context, VX_TYPE_INT32, &padBottomv);

                    shaderExecutable = vxnneGPUTensorCopyROIShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPYROI, &node->kernelAttributes.borderMode,
                                                         padLeft, padTop, padXLeft, padYTop, inputs, tensorCopy);
                    if (!shaderExecutable)
                    {
                        status = VX_FAILURE;
                        return status;
                    }

                    status = vxnneShaderOperation_Initialize(&depthwiseConvolutionLayer->depthwise_tensorcopy_sh_operation,
                        &depthwiseConvolutionLayer->base,
                        VXNNE_OPERATOR_DEPTHWISE_CONV,
                        batchCount,
                        shaderExecutable);

                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &depthwiseConvolutionLayer->base,
                        &depthwiseConvolutionLayer->depthwise_tensorcopy_sh_operation.base,
                        operation_idx++);

                    vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_tensorcopy_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_tensorcopy_sh_operation.base, (vx_reference)tensorCopy, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    depthwiseConvolutionLayer->base.temp_tensors[numTmpTensor++] = tensorCopy;
                    depthwiseConvolutionLayer->base.num_temp_tensors = numTmpTensor;
                }
                else
                {
                    tensorCopy = inputs;
                    padLeft   = vxCreateScalar(context, VX_TYPE_INT32, &padLeftv);
                    padRight  = vxCreateScalar(context, VX_TYPE_INT32, &padRightv);
                    padTop    = vxCreateScalar(context, VX_TYPE_INT32, &padTopv);
                    padBottom = vxCreateScalar(context, VX_TYPE_INT32, &padBottomv);
                }

                if (biases)
                {
                    is_static_weights_biases = (vx_bool)(CHECK_LIFETIME_IS_STATIC(weights) && CHECK_LIFETIME_IS_STATIC(biases));
                }
                else
                {
                    is_static_weights_biases = (vx_bool)(CHECK_LIFETIME_IS_STATIC(weights));
                }
                enable_adjust_biases = (is_static_weights_biases && (inputFormat == VX_TYPE_UINT8) && (3 == kernel_width) && (3 == kernel_height));
                if (enable_adjust_biases)
                {
                    vx_tensor_create_params_t params;
                    gctPOINTER weightsLogical   = VX_NULL;
                    gctPOINTER biasesLogical    = VX_NULL;
                    gctPOINTER newBiasesLogical = VX_NULL;
                    vx_uint32  i                = 0;
                    vx_uint32  j                = 0;
                    vx_uint32  sizes[4]         = {1, 1, 1, 1};
                    vx_uint32  ifm              = kernel_width * kernel_height;
                    if (biases != VX_NULL)
                    {
                        vx_uint32  ofm              = TENSOR_VIEW_SIZE_INDEX(biases, 0);
                        sizes[0] = TENSOR_VIEW_SIZE_INDEX(biases, 0);
                        sizes[1] = 1;
                        gcoOS_MemFill(&params, 0, sizeof(vx_tensor_create_params_t));
                        params.num_of_dims = TENSOR_DIM_NUM(biases);
                        params.sizes = sizes;
                        params.data_format = TENSOR_DATA_TYPE(biases);
                        params.quant_format = TENSOR_QUANT_TYPE(biases);
                        if (params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                        {
                            params.quant_data.dfp.fixed_point_pos = TENSOR_POS(biases);
                        }
                        else
                        {
                            params.quant_data.affine.scale = TENSOR_TF_SCALE(biases);
                        }
                        newBiases = vxoTensor_CreateTensor(context, NULL, &params, vx_false_e);
                        if (newBiases == VX_NULL|| vxoTensor_AllocateMemory(newBiases) != VX_SUCCESS)
                        {
                            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }
                        TENSOR_DATA_LIFETIME(newBiases) = VX_TENSOR_LIFE_TIME_STATIC;
                        vxoTensor_GetTensorViewMemory(weights, &weightsLogical, VX_NULL);
                        vxoTensor_GetTensorViewMemory(biases, &biasesLogical, VX_NULL);
                        vxoTensor_GetTensorViewMemory(newBiases, &newBiasesLogical, VX_NULL);
                        for (j = 0; j < ofm; j++)
                        {
                            vx_int32 offset = 0;
                            vx_int32 dstVal = 0;
                            for (i = 0; i < ifm; i++)
                            {
                                vx_uint32 idx = j * ifm + i;
                                offset = offset - (((vx_uint8*)weightsLogical)[idx] - TENSOR_TF_ZEROPOINT(weights)) * TENSOR_TF_ZEROPOINT(inputs);
                            }
                            dstVal = ((vx_int32*)biasesLogical)[j] + offset;
                            ((vx_int32*)newBiasesLogical)[j] = dstVal;
                        }
                        depthwiseConvolutionLayer->base.temp_tensors[numTmpTensor++] = newBiases;
                        depthwiseConvolutionLayer->base.num_temp_tensors = numTmpTensor;
                    }
                    else
                    {
                        vx_uint32  ofm              = TENSOR_VIEW_SIZE_INDEX(weights, 2);
                        sizes[0] = TENSOR_VIEW_SIZE_INDEX(weights, 2);
                        sizes[1] = 1;
                        gcoOS_MemFill(&params, 0, sizeof(vx_tensor_create_params_t));
                        params.num_of_dims = 2;
                        params.sizes = sizes;
                        params.data_format = VX_TYPE_INT32;
                        params.quant_format = VX_QUANT_AFFINE_SCALE;
                        params.quant_data.affine.scale = TENSOR_TF_SCALE(weights) * TENSOR_TF_SCALE(inputs);
                        params.quant_data.affine.zeroPoint = 0;
                        newBiases = vxoTensor_CreateTensor(context, NULL, &params, vx_false_e);
                        if (newBiases == VX_NULL|| vxoTensor_AllocateMemory(newBiases) != VX_SUCCESS)
                        {
                            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                            status = VX_ERROR_NO_MEMORY;
                            goto exit;
                        }
                        TENSOR_DATA_LIFETIME(newBiases) = VX_TENSOR_LIFE_TIME_STATIC;
                        vxoTensor_GetTensorViewMemory(weights, &weightsLogical, VX_NULL);
                        vxoTensor_GetTensorViewMemory(newBiases, &newBiasesLogical, VX_NULL);
                        for (j = 0; j < ofm; j++)
                        {
                            vx_int32 offset = 0;
                            for (i = 0; i < ifm; i++)
                            {
                                vx_uint32 idx = j * ifm + i;
                                offset = offset - (((vx_uint8*)weightsLogical)[idx] - TENSOR_TF_ZEROPOINT(weights)) * TENSOR_TF_ZEROPOINT(inputs);
                            }
                            ((vx_int32*)newBiasesLogical)[j] = offset;
                        }
                        depthwiseConvolutionLayer->base.temp_tensors[numTmpTensor++] = newBiases;
                        depthwiseConvolutionLayer->base.num_temp_tensors = numTmpTensor;
                    }
                }
                else
                {
                    newBiases = biases;
                }
                shaderExecutable = vxnneGetGPUDepthwiseConvShaderExecutable(node->base.context,
                                                                             VXNNE_KERNEL_DEPTHWISE_CONV,
                                                                             &node->kernelAttributes.borderMode,
                                                                             tensorCopy,
                                                                             weights,
                                                                             newBiases,
                                                                             padLeft,
                                                                             padRight,
                                                                             padTop,
                                                                             padBottom,
                                                                             dilationX,
                                                                             dilationY,
                                                                             depth_multiplier,
                                                                             downScaleSizeRounding,
                                                                             strideXvalue,
                                                                             strideYvalue,
                                                                             outputs);

                if(padLeft)    vxReleaseScalar(&padLeft);
                if(padRight)   vxReleaseScalar(&padRight);
                if(padTop)     vxReleaseScalar(&padTop);
                if(padBottom)  vxReleaseScalar(&padBottom);
            #undef DWCONV_ALIGN_SIZE4
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&depthwiseConvolutionLayer->depthwise_convolution_sh_operation,
                                                     &depthwiseConvolutionLayer->base,
                                                     VXNNE_OPERATOR_DEPTHWISE_CONV,
                                                     batchCount,
                                                     shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            if (batchCount > 1)
            {
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 1, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
                vxnneShaderExecutable_SetParametersAttribute(shaderExecutable, 2, VXNNE_SHADER_PARAMETERS_ATTRIBUTE_NO_BATCH_BIT);
            }

            vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_convolution_sh_operation.base, (vx_reference)tensorCopy, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_convolution_sh_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_convolution_sh_operation.base, (vx_reference)newBiases, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->depthwise_convolution_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &depthwiseConvolutionLayer->base,
                &depthwiseConvolutionLayer->depthwise_convolution_sh_operation.base,
                operation_idx++);
        }
        else
        {
            vxnneOperation_Initialize(&depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base,
                                      &depthwiseConvolutionLayer->base,
                                      VXNNE_OPERATION_TARGET_SW,
                                      VXNNE_OPERATOR_DEPTHWISE_CONV,
                                      vxoNNSWDepthwiseConvolution,
                                      VX_NULL,
                                      batchCount,
                                      0);

            vxnneLayer_SetOperation(
                &depthwiseConvolutionLayer->base,
                &depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base,
                0);

            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.inputs                = inputs;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.weights               = weights;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.biases                = biases;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.padXLeft              = padXLeft;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.padXRight             = padXRight;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.padYTop               = padYTop;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.padYBottom            = padYBottom;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.dilationX             = dilationX;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.dilationY             = dilationY;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.depth_multiplier      = depth_multiplier;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.downScaleSizeRounding = downScaleSizeRounding;
            depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.outputs               = outputs;

            vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base, (vx_reference)biases, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&depthwiseConvolutionLayer->convolution_sw1_depthwise_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
    }

    node->layer = &depthwiseConvolutionLayer->base;
    return status;

exit:
    if (depthwiseConvolutionLayer != NULL) gcoOS_Free(NULL, depthwiseConvolutionLayer);
    return status;
}


VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_tensor inputs                = (vx_tensor)parameters[0];
    vx_tensor weights               = (vx_tensor)parameters[1];
    vx_tensor biases                = (vx_tensor)parameters[2];
    vx_scalar padX                  = (vx_scalar)parameters[3];
    vx_scalar padXRight             = (vx_scalar)parameters[4];
    vx_scalar padY                  = (vx_scalar)parameters[5];
    vx_scalar padYBottom            = (vx_scalar)parameters[6];
    vx_scalar dilationX             = (vx_scalar)parameters[9];
    vx_scalar dilationY             = (vx_scalar)parameters[10];
    vx_scalar strideX               = (vx_scalar)parameters[11];
    vx_scalar strideY               = (vx_scalar)parameters[12];
    vx_scalar depth_multiplier      = (vx_scalar)parameters[13];
    vx_scalar downScaleSizeRounding = (vx_scalar)parameters[14];
    vx_tensor outputs               = (vx_tensor)parameters[17];

    if ((depth_multiplier != NULL) && (depth_multiplier->value->n32 > 0))
    {
        return vxoNNDepthwiseConvolutionLayerInitializer(node,
            inputs,
            VX_NULL, weights, biases,
            padX, padXRight, padY, padYBottom, VX_PAD_CONSTANT, VX_NULL,
            dilationX, dilationY, strideX, strideY, depth_multiplier,
            VX_NULL,
            VX_NULL, VX_NULL, VX_NULL,
            downScaleSizeRounding,
            outputs);

    }
    else
    {
        return vxoNNDilationConvolutionLayerInitializer(node,
            inputs,
            VX_NULL, weights, biases,
            padX, padXRight, padY, padYBottom, VX_PAD_CONSTANT, VX_NULL,
            dilationX, dilationY,
            strideX, strideY,
            VX_NULL,
            VX_NULL, VX_NULL, VX_NULL,
            downScaleSizeRounding,
            outputs);
    }
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConvolutionLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Reorg
 ***************************************************************************************************************************/

vx_status vxnneExecuteSWReorg(struct _vxnne_operation_s *operation)
{
    vxnne_reorg_operation reorgOperation   = (vxnne_reorg_operation)operation;

    vx_tensor  inputs           = (vx_tensor)reorgOperation->inputs;
    vx_scalar  strides          = (vx_scalar)reorgOperation->stride;
    vx_tensor  outputs          = (vx_tensor)reorgOperation->outputs;

    vx_uint32  stride           = strides->value->u32;
    vx_uint32  input_width      = TENSOR_SIZE_INDEX(inputs, 0);
    vx_uint32  input_height     = TENSOR_SIZE_INDEX(inputs, 1);
    vx_uint32  input_depth      = TENSOR_SIZE_INDEX(inputs, 2);
    vx_uint32  input_batch      = TENSOR_SIZE_INDEX(inputs, 3);
    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);
    vx_int8   inputFixedPoint   = TENSOR_POS(inputs);
    vx_int8   outputFixedPoint  = TENSOR_POS(outputs);

    vx_uint32  out_c            = input_depth / (stride * stride);
    vx_uint32  i,j,k,b;
    vx_float32 data = 0.0f;
    gctPOINTER inputBase;
    gctPOINTER outputBase;

    vx_status status = VX_SUCCESS;

    if (input_batch == 0)
    {
        input_batch = 1;
    }

    vxoTensor_GetTensorViewMemory(inputs, &inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, &outputBase, VX_NULL);

    if ((TENSOR_DATA_TYPE(inputs) != VX_TYPE_BFLOAT16 && TENSOR_DATA_TYPE(inputs) != VX_TYPE_FLOAT16 && TENSOR_DATA_TYPE(inputs) != VX_TYPE_FLOAT32 && TENSOR_DATA_TYPE(inputs) != VX_TYPE_INT8)
        || (TENSOR_DATA_TYPE(outputs) != VX_TYPE_BFLOAT16 && TENSOR_DATA_TYPE(outputs) != VX_TYPE_FLOAT16 && TENSOR_DATA_TYPE(outputs) != VX_TYPE_FLOAT32 && TENSOR_DATA_TYPE(outputs) != VX_TYPE_INT8))
    {
        vxError("input or outputs format is not support");
        status = VX_ERROR_NOT_SUPPORTED;
        return status;
    }
    for(b = 0; b < input_batch; ++b)
    {
        for(k = 0; k < input_depth; ++k)
        {
            for(j = 0; j < input_height; ++j)
            {
                for(i = 0; i < input_width; ++i)
                {
                    vx_int32 in_index  = i + input_width * (j + input_height * (k + input_depth * b));
                    vx_int32 c2 = k % out_c;
                    vx_int32 offset = k / out_c;
                    vx_int32 w2 = i * stride + offset % stride;
                    vx_int32 h2 = j * stride + offset / stride;
                    vx_int32 out_index = w2 + input_width * stride * (h2 + input_height * stride * (c2 + out_c * b));

                    data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), out_index, (vx_uint8_ptr)inputBase, inputFixedPoint, TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                    vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), in_index, data, (vx_uint8_ptr)outputBase, outputFixedPoint, TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                }
            }
        }
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNReorgLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReorgLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReorgLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReorgLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  stride_s                   = (vx_scalar)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];
    vx_uint32  stride                     = stride_s->value->u32;
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_uint32  input_depth                = TENSOR_SIZE_INDEX(inputs, 2);
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_context context                    = vxGetContext((vx_reference)node);

    vxnne_reorg_layer  reorgLayer         = VX_NULL;
    vx_bool            dataFormat_flag    = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 || inputFormat == VX_TYPE_INT8) && (outputFormat == VX_TYPE_FLOAT16 || outputFormat == VX_TYPE_INT8))
                                                    || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                                    || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16));
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_reorg_layer_s), (gctPOINTER*)&reorgLayer);
    if (!reorgLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(reorgLayer, sizeof(vxnne_reorg_layer_s));

    vxnneLayer_Initialize(&reorgLayer->base,
                          "ReorgLayer",
                          node,
                          vxmOPERATION_COUNT(reorgLayer),
                          reorgLayer->operations,
                          VX_NULL);

    if (vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_REORG))
    {
        vx_op_param_s conv = {0};

        status = vxnneOperation_Initialize(&reorgLayer->reorg_tp_operation.base,
                                           &reorgLayer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_REORG,
                                           VX_NULL,
                                           vxnneOperation_TP_Deinitialize,
                                           batchCount,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_REORG;
        conv.other_ref = gcvNULL;
        conv.data_buff = gcvNULL;
        conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        conv.tp_value->u32[0] = stride;

        vxMemCopy(&reorgLayer->reorg_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneLayer_SetOperation(
            &reorgLayer->base,
            &reorgLayer->reorg_tp_operation.base,
            0);
        reorgLayer->reorg_tp_operation.input  = inputs;
        reorgLayer->reorg_tp_operation.output = outputs;

        vxnneOperation_AddReference(&reorgLayer->reorg_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reorgLayer->reorg_tp_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        if (stride == 2 && dataFormat_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vx_scalar outc_s   = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &input_depth);

            vxnne_shader_executable shaderExecutable = VX_NULL;

            shaderExecutable = vxnneGetReorgShaderExecutable(node->base.context, VXNNE_KERNEL_REORG, &node->kernelAttributes.borderMode,
                                                                 inputs, stride_s, outc_s, outputs);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                if (!outc_s) vxReleaseScalar(&outc_s);
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&reorgLayer->reorg_sh_operation,
                                            &reorgLayer->base,
                                            VXNNE_OPERATOR_REORG,
                                            batchCount,
                                            shaderExecutable);

            if (status != VX_SUCCESS)
            {
                if (!outc_s) vxReleaseScalar(&outc_s);
                goto exit;
            }

            vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &reorgLayer->base,
                &reorgLayer->reorg_sh_operation.base,
                0);
        }
        else
        {
            vxnneOperation_Initialize(&reorgLayer->reorg_sw_operation.base,
                                      &reorgLayer->base,
                                      VXNNE_OPERATION_TARGET_SW,
                                      VXNNE_OPERATOR_REORG,
                                      vxnneExecuteSWReorg,
                                      VX_NULL,
                                      batchCount,
                                      0);

            vxnneLayer_SetOperation(
                &reorgLayer->base,
                &reorgLayer->reorg_sw_operation.base,
                0);

            reorgLayer->reorg_sw_operation.inputs           = inputs;
            reorgLayer->reorg_sw_operation.stride           = (vx_reference)stride_s;
            reorgLayer->reorg_sw_operation.outputs          = outputs;

            vxnneOperation_AddReference(&reorgLayer->reorg_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&reorgLayer->reorg_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
    }

    node->layer = &reorgLayer->base;
    return status;

exit:
    if (reorgLayer) gcoOS_Free(gcvNULL, (gctPOINTER)reorgLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReorgLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}


/***************************************************************************************************************************
 *                                                 DeConvolution
 ***************************************************************************************************************************/
VX_PRIVATE_API void vxnneLayerSW_gemm_nn(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                         vx_int8 A_fixedPointPos, vx_int8 B_fixedPointPos, vx_int8 C_fixedPointPos,
                                         vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                         vx_uint8_ptr A, vx_int32 lda,
                                         vx_uint8_ptr B, vx_int32 ldb,
                                         vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(k = 0; k < K; ++k){
            /*register float A_PART = ALPHA*A[i*lda+k];*/
            register float A_PART = ALPHA*vxnneGetData(A_format, i*lda+k, A, A_fixedPointPos);
            for(j = 0; j < N; ++j){
                /*C[i*ldc+j] += A_PART*B[k*ldb+j];*/
                vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + A_PART*vxnneGetData(B_format, k*ldb+j, B, B_fixedPointPos), C, C_fixedPointPos, roundMode);
            }
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_nn_u8(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                            vx_int32 A_zeroPoint, vx_float32 A_scale,
                                            vx_int32 B_zeroPoint, vx_float32 B_scale,
                                            vx_int8 C_fixedPointPos,
                                            vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                            vx_uint8_ptr A, vx_int32 lda,
                                            vx_uint8_ptr B, vx_int32 ldb,
                                            vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(k = 0; k < K; ++k){
            /*register float A_PART = ALPHA*A[i*lda+k];*/
            register float A_PART = ALPHA*vxnneGetDataQuant(A_format, i*lda+k, A, A_zeroPoint, A_scale);
            for(j = 0; j < N; ++j){
                /*C[i*ldc+j] += A_PART*B[k*ldb+j];*/
                vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + A_PART*vxnneGetDataQuant(B_format, k*ldb+j, B, B_zeroPoint, B_scale), C, C_fixedPointPos, roundMode);
            }
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_nt(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                         vx_int8 A_fixedPointPos, vx_int8 B_fixedPointPos, vx_int8 C_fixedPointPos,
        vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
        vx_uint8_ptr A, vx_int32 lda,
        vx_uint8_ptr B, vx_int32 ldb,
        vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            register float sum = 0;
            for(k = 0; k < K; ++k){
                /*sum += ALPHA*A[i*lda+k]*B[j*ldb + k];*/
                sum += ALPHA * vxnneGetData(A_format, i*lda+k, A, A_fixedPointPos) * vxnneGetData(B_format, j*ldb + k, B, B_fixedPointPos);
            }
            /*C[i*ldc+j] += sum;*/
            vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + sum, C, C_fixedPointPos, roundMode);
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_nt_u8(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                            vx_int32 A_zeroPoint, vx_float32 A_scale,
                                            vx_int32 B_zeroPoint, vx_float32 B_scale,
                                            vx_int8 C_fixedPointPos,
                                            vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                            vx_uint8_ptr A, vx_int32 lda,
                                            vx_uint8_ptr B, vx_int32 ldb,
                                            vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            register float sum = 0;
            for(k = 0; k < K; ++k){
                /*sum += ALPHA*A[i*lda+k]*B[j*ldb + k];*/
                sum += ALPHA * vxnneGetDataQuant(A_format, i*lda+k, A, A_zeroPoint, A_scale) * vxnneGetDataQuant(B_format, j*ldb + k, B, B_zeroPoint, B_scale);
            }
            /*C[i*ldc+j] += sum;*/
            vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + sum, C, C_fixedPointPos, roundMode);
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_tn(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                         vx_int8 A_fixedPointPos, vx_int8 B_fixedPointPos, vx_int8 C_fixedPointPos,
                                         vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                         vx_uint8_ptr A, vx_int32 lda,
                                         vx_uint8_ptr B, vx_int32 ldb,
                                         vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(k = 0; k < K; ++k){
            /*register float A_PART = ALPHA*A[k*lda+i];*/
            register float A_PART = ALPHA*vxnneGetData(A_format, k*lda+i, A, A_fixedPointPos);
            for(j = 0; j < N; ++j){
                /*C[i*ldc+j] += A_PART*B[k*ldb+j];*/
                vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + A_PART*vxnneGetData(B_format, k*ldb+j, B, B_fixedPointPos), C, C_fixedPointPos, roundMode);
            }
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_tn_u8(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                            vx_int32 A_zeroPoint, vx_float32 A_scale,
                                            vx_int32 B_zeroPoint, vx_float32 B_scale,
                                            vx_int8 C_fixedPointPos,
                                            vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                            vx_uint8_ptr A, vx_int32 lda,
                                            vx_uint8_ptr B, vx_int32 ldb,
                                            vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(k = 0; k < K; ++k){
            /*register float A_PART = ALPHA*A[k*lda+i];*/
            register float A_PART = ALPHA*vxnneGetDataQuant(A_format, k*lda+i, A, A_zeroPoint, A_scale);
            for(j = 0; j < N; ++j){
                /*C[i*ldc+j] += A_PART*B[k*ldb+j];*/
                vxnneSaveData(C_format, i*ldc+j, vxnneGetData(C_format, i*ldc+j, C, C_fixedPointPos) + A_PART*vxnneGetDataQuant(B_format, k*ldb+j, B, B_zeroPoint, B_scale), C, C_fixedPointPos, roundMode);
            }
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_tt(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                         vx_int8 A_fixedPointPos, vx_int8 B_fixedPointPos, vx_int8 C_fixedPointPos,
                                         vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                         vx_uint8_ptr A, vx_int32 lda,
                                         vx_uint8_ptr B, vx_int32 ldb,
                                         vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            register float sum = 0;
            for(k = 0; k < K; ++k){
                /*sum += ALPHA*A[i+k*lda]*B[k+j*ldb];*/
                sum += ALPHA * vxnneGetData(A_format, i+k*lda, A, A_fixedPointPos) * vxnneGetData(B_format, k+j*ldb, B, B_fixedPointPos);
            }
            /*C[i*ldc+j] += sum;*/
            vxnneSaveData(C_format, i*ldc+j, sum, C, C_fixedPointPos, roundMode);
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_gemm_tt_u8(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                            vx_int32 A_zeroPoint, vx_float32 A_scale,
                                            vx_int32 B_zeroPoint, vx_float32 B_scale,
                                            vx_int8 C_fixedPointPos,
                                            vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                            vx_uint8_ptr A, vx_int32 lda,
                                            vx_uint8_ptr B, vx_int32 ldb,
                                            vx_uint8_ptr C, vx_int32 ldc)
{
    vx_int32 i,j,k;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            register float sum = 0;
            for(k = 0; k < K; ++k){
                /*sum += ALPHA*A[i+k*lda]*B[k+j*ldb];*/
                sum += ALPHA * vxnneGetDataQuant(A_format, i+k*lda, A, A_zeroPoint, A_scale) * vxnneGetDataQuant(B_format, k+j*ldb, B, B_zeroPoint, B_scale);
            }
            /*C[i*ldc+j] += sum;*/
            vxnneSaveData(C_format, i*ldc+j, sum, C, C_fixedPointPos, roundMode);
        }
    }
}


VX_PRIVATE_API void vxnneLayerSW_gemm(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                      vx_int8 A_fixedPointPos, vx_int8 B_fixedPointPos, vx_int8 C_fixedPointPos,
                                      vx_int32 TA, vx_int32 TB, vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                      vx_uint8_ptr A, vx_int32 lda,
                                      vx_uint8_ptr B, vx_int32 ldb,
                                      vx_float32 BETA,
                                      vx_uint8_ptr C, vx_int32 ldc)
{
    /*vxInfo("cpu: %d %d %d %d %d %f %d %d %f %d\n",TA, TB, M, N, K, ALPHA, lda, ldb, BETA, ldc);*/
    int i, j;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            /*C[i*ldc + j] *= BETA;*/
            vxnneSaveData(C_format, i*ldc + j, BETA * vxnneGetData(C_format, i*ldc + j, C, C_fixedPointPos), C, C_fixedPointPos, roundMode);
        }
    }
    if(!TA && !TB)
        vxnneLayerSW_gemm_nn(A_format, B_format, C_format, roundMode, A_fixedPointPos, B_fixedPointPos, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else if(TA && !TB)
        vxnneLayerSW_gemm_tn(A_format, B_format, C_format, roundMode, A_fixedPointPos, B_fixedPointPos, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else if(!TA && TB)
        vxnneLayerSW_gemm_nt(A_format, B_format, C_format, roundMode, A_fixedPointPos, B_fixedPointPos, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else
        vxnneLayerSW_gemm_tt(A_format, B_format, C_format, roundMode, A_fixedPointPos, B_fixedPointPos, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
}

VX_PRIVATE_API void vxnneLayerSW_gemm_u8(vx_type_e A_format, vx_type_e B_format, vx_type_e C_format, vx_int32 roundMode,
                                         vx_int32 A_zeroPoint, vx_float32 A_scale,
                                         vx_int32 B_zeroPoint, vx_float32 B_scale,
                                         vx_int8 C_fixedPointPos,
                                         vx_int32 TA, vx_int32 TB, vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                                         vx_uint8_ptr A, vx_int32 lda,
                                         vx_uint8_ptr B, vx_int32 ldb,
                                         vx_float32 BETA,
                                         vx_uint8_ptr C, vx_int32 ldc)
{
    /*vxInfo("cpu: %d %d %d %d %d %f %d %d %f %d\n",TA, TB, M, N, K, ALPHA, lda, ldb, BETA, ldc);*/
    int i, j;
    for(i = 0; i < M; ++i){
        for(j = 0; j < N; ++j){
            /*C[i*ldc + j] *= BETA;*/
            vxnneSaveData(C_format, i*ldc + j, BETA * vxnneGetData(C_format, i*ldc + j, C, C_fixedPointPos), C, C_fixedPointPos, roundMode);
        }
    }
    if(!TA && !TB)
        vxnneLayerSW_gemm_nn_u8(A_format, B_format, C_format, roundMode, A_zeroPoint, A_scale, B_zeroPoint, B_scale, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else if(TA && !TB)
        vxnneLayerSW_gemm_tn_u8(A_format, B_format, C_format, roundMode, A_zeroPoint, A_scale, B_zeroPoint, B_scale, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else if(!TA && TB)
        vxnneLayerSW_gemm_nt_u8(A_format, B_format, C_format, roundMode, A_zeroPoint, A_scale, B_zeroPoint, B_scale, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
    else
        vxnneLayerSW_gemm_tt_u8(A_format, B_format, C_format, roundMode, A_zeroPoint, A_scale, B_zeroPoint, B_scale, C_fixedPointPos, M, N, K, ALPHA,A,lda, B, ldb, C,ldc);
}

VX_PRIVATE_API vx_bool vxnneLayerSW_is_a_ge_zero_and_a_lt_b(vx_int32 a, vx_int32 b) {
  return (((vx_uint32)a) < (vx_uint32)(b))?vx_true_e:vx_false_e;
}

VX_PRIVATE_API void vxnneLayerSW_col2im(vx_type_e col_format, vx_type_e im_format, vx_int32 roundMode,
                                        vx_int8 col_fixedPointPos, vx_int8 im_fixedPointPos,
    vx_uint8_ptr data_col, const vx_int32 channels,
    const vx_int32 height, const vx_int32 width, const vx_int32 kernel_h, const vx_int32 kernel_w,
    const vx_int32 pad_h, const vx_int32 pad_w,
    const vx_int32 stride_h, const vx_int32 stride_w,
    const vx_int32 dilation_h, const vx_int32 dilation_w,
    vx_uint8_ptr data_im) {
    vx_int32 channel = 0, kernel_row = 0, kernel_col = 0, input_row = 0, output_rows = 0, input_col = 0;
    vx_int32 output_col = 0;

    const vx_int32 output_h = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
    const vx_int32 output_w = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
    const vx_int32 channel_size = height * width;
    for (channel = channels; channel--; data_im += vxnneGetTypeSize(im_format) *channel_size) {
        for (kernel_row = 0; kernel_row < kernel_h; kernel_row++) {
            for (kernel_col = 0; kernel_col < kernel_w; kernel_col++) {
                input_row = -pad_h + kernel_row * dilation_h;
                for (output_rows = output_h; output_rows; output_rows--) {
                    if (!vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_row, height)) {
                        /*data_col += output_w;*/
                        data_col += vxnneGetTypeSize(col_format) * output_w;
                    } else {
                          input_col = -pad_w + kernel_col * dilation_w;
                          for (output_col = output_w; output_col; output_col--) {
                              if (vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_col, width)) {
                                  /*data_im[input_row * width + input_col] += *data_col;*/
                                  vx_int32 idx = input_row * width + input_col;
                                  vx_float64 val1 = vxnneGetData(col_format, 0, data_col, col_fixedPointPos);
                                  vx_float64 val2 =  vxnneGetData(im_format, input_row * width + input_col, data_im, im_fixedPointPos);
                                  vx_float64 val = val1 + val2;
                                  vxnneSaveData(im_format, idx, val, data_im, im_fixedPointPos, roundMode);
                              }
                              /*data_col++;*/
                              data_col += vxnneGetTypeSize(col_format);

                              input_col += stride_w;
                          }
                    }
                    input_row += stride_h;
                }
            }
        }
    }
}
VX_PRIVATE_API void vxnneLayerSW_col2im_add_bias(vx_type_e col_format, vx_type_e im_format,vx_type_e bias_format, vx_int32 roundMode,
                                        vx_int8 col_fixedPointPos, vx_int8 im_fixedPointPos, vx_int8 bias_fixedPointPos,
    vx_uint8_ptr data_col,vx_uint8_ptr biases,
    const vx_int32 channels, const vx_int32 height, const vx_int32 width,
    const vx_int32 kernel_h, const vx_int32 kernel_w,
    const vx_int32 pad_h, const vx_int32 pad_w,
    const vx_int32 pad_h_b, const vx_int32 pad_w_r,
    const vx_int32 stride_h, const vx_int32 stride_w,
    const vx_int32 dilation_h, const vx_int32 dilation_w,
    vx_uint8_ptr data_im, vx_float32_ptr tmp_ptr) {
    vx_int32 channel = 0, kernel_row = 0, kernel_col = 0, input_row = 0, output_rows = 0, input_col = 0;
    vx_int32 output_col = 0;

    const vx_int32 output_h = (height + pad_h_b + pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
    const vx_int32 output_w = (width + pad_w_r + pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
    const vx_int32 channel_size = height * width;
    vx_int32 i=0;

    for (channel = channels; channel--; data_im += vxnneGetTypeSize(im_format) *channel_size) {
        gcoOS_MemFill(tmp_ptr, 0, sizeof(vx_float32) * channel_size);

        for (kernel_row = 0; kernel_row < kernel_h; kernel_row++) {
            for (kernel_col = 0; kernel_col < kernel_w; kernel_col++) {
                input_row = -pad_h + kernel_row * dilation_h;
                for (output_rows = output_h; output_rows; output_rows--) {
                    if (!vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_row, height)) {
                        /*data_col += output_w;*/
                        data_col += vxnneGetTypeSize(col_format) * output_w;
                    } else {
                          input_col = -pad_w + kernel_col * dilation_w;
                          for (output_col = output_w; output_col; output_col--) {
                              if (vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_col, width)) {
                                  /*data_im[input_row * width + input_col] += *data_col;*/
                                  vx_int32 idx = input_row * width + input_col;
                                  vx_float64 val1 = vxnneGetData(col_format, 0, data_col, col_fixedPointPos);
                                  tmp_ptr[idx] += (vx_float32)val1;
                              }
                              /*data_col++;*/
                              data_col += vxnneGetTypeSize(col_format);

                              input_col += stride_w;
                          }
                    }
                    input_row += stride_h;
                }
            }
        }

        if(biases)
        {
            vx_float32 fbias = vxnneGetData(bias_format, channels-channel-1, biases, bias_fixedPointPos);
            for(i=0;i<channel_size;i++)
                vxnneSaveData(im_format, i, tmp_ptr[i]+fbias, data_im, im_fixedPointPos, roundMode);
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_col2im_add_bias_u8(vx_type_e col_format, vx_type_e im_format,vx_type_e bias_format, vx_int32 roundMode,
    vx_int8 col_fixedPointPos, vx_int32 im_zeroPoint, vx_float32 im_scale, vx_int32 bias_zeroPoint, vx_float32 bias_scale,
    vx_uint8_ptr data_col,vx_uint8_ptr biases,
    const vx_int32 channels, const vx_int32 height, const vx_int32 width,
    const vx_int32 kernel_h, const vx_int32 kernel_w,
    const vx_int32 pad_h, const vx_int32 pad_w,
    const vx_int32 pad_h_b, const vx_int32 pad_w_r,
    const vx_int32 stride_h, const vx_int32 stride_w,
    const vx_int32 dilation_h, const vx_int32 dilation_w,
    vx_uint8_ptr data_im, vx_float32_ptr tmp_ptr) {
    vx_int32 channel = 0, kernel_row = 0, kernel_col = 0, input_row = 0, output_rows = 0, input_col = 0;
    vx_int32 output_col = 0;

    const vx_int32 output_h = (height + 2 * gcmMAX(pad_h, pad_h_b) - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
    const vx_int32 output_w = (width + 2 * gcmMAX(pad_w, pad_w_r) - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
    const vx_int32 channel_size = height * width;
    vx_int32 i=0;

    for (channel = channels; channel--; data_im += vxnneGetTypeSize(im_format) *channel_size) {
        gcoOS_MemFill(tmp_ptr, 0, sizeof(vx_float32) * channel_size);

        for (kernel_row = 0; kernel_row < kernel_h; kernel_row++) {
            for (kernel_col = 0; kernel_col < kernel_w; kernel_col++) {
                input_row = -pad_h + kernel_row * dilation_h;
                for (output_rows = output_h; output_rows; output_rows--) {
                    if (!vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_row, height)) {
                        /*data_col += output_w;*/
                        data_col += vxnneGetTypeSize(col_format) * output_w;
                    } else {
                          input_col = -pad_w + kernel_col * dilation_w;
                          for (output_col = output_w; output_col; output_col--) {
                              if (vxnneLayerSW_is_a_ge_zero_and_a_lt_b(input_col, width)) {
                                  /*data_im[input_row * width + input_col] += *data_col;*/
                                  vx_int32 idx = input_row * width + input_col;
                                  vx_float64 val1 = vxnneGetData(col_format, 0, data_col, col_fixedPointPos);
                                  tmp_ptr[idx] += (vx_float32)val1;
                              }
                              /*data_col++;*/
                              data_col += vxnneGetTypeSize(col_format);

                              input_col += stride_w;
                          }
                    }
                    input_row += stride_h;
                }
            }
        }

        if(biases)
        {
            vx_float32 fbias = vxnneGetDataQuant(bias_format, channels-channel-1, biases, bias_zeroPoint, bias_scale);
            for(i=0;i<channel_size;i++)
                vxnneSaveDataQuant(im_format, i, tmp_ptr[i]+fbias, data_im, im_zeroPoint, im_scale, roundMode);
        }
    }
}

VX_PRIVATE_API void vxnneLayerSW_add_bias(vx_type_e output_format, vx_type_e bias_format, vx_int32 roundMode,
                                          vx_int8 output_fixedPointPos, vx_int8 bias_fixedPointPos,
                vx_uint8_ptr output, vx_uint8_ptr biases, vx_int32 batch, vx_int32 n, vx_int32 size)
{
    vx_int32 i,j,b;
    for(b = 0; b < batch; ++b){
        for(i = 0; i < n; ++i){
            for(j = 0; j < size; ++j){
                /*output[(b*n + i)*size + j] += biases[i];*/
                vxnneSaveData(output_format, (b*n + i)*size + j, vxnneGetData(bias_format, i, biases, bias_fixedPointPos) + vxnneGetData(output_format, (b*n + i)*size + j, output, output_fixedPointPos), output, output_fixedPointPos, roundMode);
            }
        }
    }
}

#define INDEX_WIDTH 0
#define INDEX_HEIGHT 1
#define INDEX_DEPTH 2
#define INDEX_BATCH 3

VX_PRIVATE_API vx_status vxnneGetTensorMemeory(vx_tensor tensor, vx_ptr_ptr ptr, vx_bool stage, vx_bool zero)
{
    vx_status status = VX_SUCCESS;
    vx_uint32 tensor_size = 0;

    gcmASSERT(tensor != VX_NULL);

    vxoTensor_GetTensorSize(tensor, &tensor_size);

    if (stage)
    {
        vx_ptr data = VX_NULL;

        vxoTensor_GetTensorViewMemory(tensor, &data, VX_NULL);
        *ptr = vxAllocate(tensor_size);
        gcoOS_MemCopy(*ptr, data, tensor_size);
    }
    else
        vxoTensor_GetTensorViewMemory(tensor, (vx_ptr_ptr)ptr, VX_NULL);

    if (zero)
        gcoOS_MemFill(*ptr, 0, tensor_size);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConv_ReshuffleWeights(struct _vxnne_operation_s *operation)
{
    vxnne_deconvolution_reshuffle_operation deconvOperation   = (vxnne_deconvolution_reshuffle_operation)operation;

    vx_tensor weights               = deconvOperation->weights;
    vx_tensor reshuffle_weights     = deconvOperation->reshuffled_weights;
    vx_tensor biases                = deconvOperation->bias;
    vx_tensor reshuffled_biases     = deconvOperation->reshuffled_biases;
    vx_int32 stride_w               = deconvOperation->stride_x->value->n32;
    vx_int32 stride_h               = deconvOperation->stride_y->value->n32;
    vx_int32 group                  = deconvOperation->group->value->n32;
    vx_tensor inputs                = deconvOperation->inputs;

    vx_type_e weight_format = (vx_type_e)(TENSOR_DATA_TYPE(weights));
    vx_type_e reshuffle_weight_format = (vx_type_e)(TENSOR_DATA_TYPE(reshuffle_weights));

    vx_int32 kernel_size_x = TENSOR_SIZE_INDEX(weights, 0);
    vx_int32 kernel_size_y = TENSOR_SIZE_INDEX(weights, 1);
    vx_int32 kernel_size_c = TENSOR_SIZE_INDEX(weights, 3);

    vx_uint8_ptr reshuffled_weights = VX_NULL;

    vx_int32 reshuffle_width = gcmALIGN_NP2(kernel_size_x, stride_w)/stride_w, reshuffle_height = gcmALIGN_NP2(kernel_size_y, stride_h)/stride_h;
    vx_int32 slice_size = kernel_size_x * kernel_size_y, reshuffled_slice_size = reshuffle_width * reshuffle_height;

    vx_int32 kernel_reshuffle_pad_x_left = ((TENSOR_VIEW_SIZE_INDEX(deconvOperation->outputs, 0) - 1) + reshuffle_width - TENSOR_VIEW_SIZE_INDEX(deconvOperation->inputs, 0)) / 2;
    vx_int32 kernel_reshuffle_pad_x_right = ((TENSOR_VIEW_SIZE_INDEX(deconvOperation->outputs, 0) - 1) + reshuffle_width - TENSOR_VIEW_SIZE_INDEX(deconvOperation->inputs, 0)) - kernel_reshuffle_pad_x_left;
    vx_int32 kernel_reshuffle_pad_y_top = ((TENSOR_VIEW_SIZE_INDEX(deconvOperation->outputs, 1) - 1) + reshuffle_height - TENSOR_VIEW_SIZE_INDEX(deconvOperation->inputs, 1)) / 2;
    vx_int32 kernel_reshuffle_pad_y_bottom = ((TENSOR_VIEW_SIZE_INDEX(deconvOperation->outputs, 1) - 1) + reshuffle_height - TENSOR_VIEW_SIZE_INDEX(deconvOperation->inputs, 1)) - kernel_reshuffle_pad_y_top;

    vx_int32 w = 0, h = 0, sx = 0, sy = 0, c = 0, b = 0, batch = TENSOR_SIZE_INDEX(weights, 2);/*kernel_size_c;*/
    vx_uint8_ptr data = reshuffled_weights, buffer = VX_NULL;
    vx_int32 item_size = vxnneGetTypeSize(weight_format);
    vx_uint8_ptr weights_ptr = weights->tensorBuffer->memory.logicals[0];

    vx_int8 weightsFixedPointPos = TENSOR_POS(weights);
    vx_bool depthwise = (group != kernel_size_c) ? vx_false_e:vx_true_e;

    reshuffled_weights = reshuffle_weights->tensorBuffer->memory.logicals[0];

    if (deconvOperation->reshuffled)
        memcpy(reshuffled_weights, weights_ptr, item_size * slice_size * kernel_size_c * batch);
    else
    {
        buffer = (vx_uint8_ptr)vxAllocateAndZeroMemory(item_size * slice_size * kernel_size_c * batch);
        data = reshuffled_weights;
        if (group != kernel_size_c)
        {


        /* transpose */
        for (b = 0; b < batch; b++)
        {
            for (c = 0; c < kernel_size_c; c++)
            {
                memcpy(buffer + kernel_size_x * kernel_size_y * (b * kernel_size_c + c) * item_size, weights_ptr + kernel_size_x * kernel_size_y * (c * batch + b) * item_size, item_size * slice_size);
            }
        }
        }
        else
            memcpy(buffer, weights_ptr, item_size * slice_size * kernel_size_c * batch);

        if (depthwise)
        {
            /* reshuffle */
            for (b = 0; b < kernel_size_c; b++)
            {
                for (sy = 0; sy < stride_h; sy++)
                {
                    for (sx = 0; sx < stride_w; sx++)
                    {
                        for (c = 0; c < kernel_size_c; c++)
                        {
                            vx_uint8_ptr weight_output = reshuffled_weights + (b * reshuffled_slice_size * stride_w * stride_h * kernel_size_c + reshuffled_slice_size * kernel_size_c * (sy * stride_w + sx) + reshuffled_slice_size * c) * item_size;

                            data = buffer + b * slice_size * item_size;

                            for (h = 0; h < reshuffle_height; h++)
                            {
                                for (w = 0; w < reshuffle_width; w++)
                                {

                                    /*     ___________
                                    *    |     |     |
                                    *    |  3  |  2  |
                                    *    |_____|_____|
                                    *    |     |     |
                                    *    |  1  |  0  |
                                    *    |_____|_____|
                                    *
                                    */
                                    vx_uint8_ptr reshuffled_output = weight_output + (h * reshuffle_width + w) * item_size;
                                    vx_int32 input_index = ((reshuffle_height - 1 - h) * stride_w + sy) * kernel_size_x + ((reshuffle_width - 1 - w) * stride_h + sx);
                                    vx_int32 delat_x = reshuffle_width * stride_w - kernel_size_x, delat_y = reshuffle_height * stride_h - kernel_size_y;

                                    /**reshuffled_output = data[input_index];*/

                                    vx_int32 input_x = (reshuffle_width - 1 - w) * stride_w + sx - delat_x;
                                    vx_int32 input_y = (reshuffle_height - 1 - h) * stride_h + sy - delat_y;
                                    input_index = input_y * kernel_size_x + input_x;

                                    if ((input_x >= kernel_size_x) || (input_y >= kernel_size_y)
                                        || (input_x < 0) || (input_y < 0))
                                    {
                                        if (weight_format == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffle_weights) == VX_QUANT_AFFINE_SCALE)
                                            memset(reshuffled_output, TENSOR_TF_ZEROPOINT(weights), item_size);
                                        else
                                            memset(reshuffled_output, 0, item_size);
                                    }
                                    else if (reshuffle_weight_format == weight_format)
                                    {
                                        if (c == b)
                                            memcpy(reshuffled_output, data + input_index * item_size, item_size);
                                        else
                                        {
                                            if (weight_format == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffle_weights) == VX_QUANT_AFFINE_SCALE)
                                                memset(reshuffled_output, TENSOR_TF_ZEROPOINT(weights), item_size);
                                            else
                                                memset(reshuffled_output, 0, item_size);
                                        }
                                    }
                                    else
                                    {
                                        vxnneSaveDataExt(weight_format, TENSOR_QUANT_TYPE(reshuffle_weights), 0, vxnneGetDataExt(weight_format, TENSOR_QUANT_TYPE(reshuffle_weights), input_index, data, weightsFixedPointPos, TENSOR_TF_ZEROPOINT(reshuffle_weights), TENSOR_TF_SCALE(reshuffle_weights)), reshuffled_output, weightsFixedPointPos, TENSOR_TF_ZEROPOINT(reshuffle_weights), TENSOR_TF_SCALE(reshuffle_weights), 0);

                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        else
        {

            /* reshuffle */
            for (b = 0; b < batch; b++)
            {
                for (sy = 0; sy < stride_h; sy++)
                {
                    for (sx = 0; sx < stride_w; sx++)
                    {
                        for (c = 0; c < kernel_size_c; c++)
                        {
                            vx_uint8_ptr weight_output = reshuffled_weights + (b * reshuffled_slice_size * stride_w * stride_h * kernel_size_c + reshuffled_slice_size * kernel_size_c * (sy * stride_w + sx) + reshuffled_slice_size * c) * item_size;

                            data = buffer + (b * slice_size * kernel_size_c + slice_size * c) * item_size;

                            for (h = 0; h < reshuffle_height; h++)
                            {
                                for (w = 0; w < reshuffle_width; w++)
                                {

                                    /*     ___________
                                     *    |     |     |
                                     *    |  3  |  2  |
                                     *    |_____|_____|
                                     *    |     |     |
                                     *    |  1  |  0  |
                                     *    |_____|_____|
                                     *
                                     */
                                    vx_uint8_ptr reshuffled_output = weight_output + (h * reshuffle_width + w) * item_size;
                                    vx_int32 input_index = ((reshuffle_height - 1 - h) * stride_w + sy) * kernel_size_x + ((reshuffle_width - 1 - w) * stride_h + sx);
                                    vx_int32 delat_x = reshuffle_width * stride_w - kernel_size_x, delat_y = reshuffle_height * stride_h - kernel_size_y;

                                    /**reshuffled_output = data[input_index];*/

                                    vx_int32 input_x = (reshuffle_width - 1 - w) * stride_w + sx - delat_x;
                                    vx_int32 input_y = (reshuffle_height - 1 - h) * stride_h + sy - delat_y;
                                    input_index = input_y * kernel_size_x + input_x;

                                    if ((input_x >=  kernel_size_x) || (input_y >=  kernel_size_y)
                                        || (input_x < 0) || (input_y < 0))
                                    {
                                        if (weight_format == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffle_weights) == VX_QUANT_AFFINE_SCALE)
                                            memset(reshuffled_output, TENSOR_TF_ZEROPOINT(weights), item_size);
                                        else
                                            memset(reshuffled_output, 0, item_size);
                                    }
                                    else if (reshuffle_weight_format == weight_format)
                                    {
                                        memcpy(reshuffled_output, data + input_index * item_size, item_size);
                                    }
                                    else
                                    {
                                        vxnneSaveDataExt(weight_format, TENSOR_QUANT_TYPE(reshuffle_weights), 0, vxnneGetDataExt(weight_format, TENSOR_QUANT_TYPE(reshuffle_weights), input_index, data, weightsFixedPointPos, TENSOR_TF_ZEROPOINT(reshuffle_weights), TENSOR_TF_SCALE(reshuffle_weights)), reshuffled_output, weightsFixedPointPos, TENSOR_TF_ZEROPOINT(reshuffle_weights), TENSOR_TF_SCALE(reshuffle_weights), 0);

                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        vxFree(buffer);

        if (biases && reshuffled_biases)
        {
            vx_int32 i = 0;

            vx_int32 bias_c = TENSOR_SIZE_INDEX(biases, 3);
            vx_int32 r_bias_c = TENSOR_SIZE_INDEX(reshuffled_biases, 3);
            vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(biases));
            vx_uint8_ptr bias_ptr = TENSOR_LOGICAL_ADDR(biases);
            vx_uint8_ptr reshuffled_bias_ptr = TENSOR_LOGICAL_ADDR(reshuffled_biases);

            if (bias_c != r_bias_c)
            {

                for (i = 0; i < r_bias_c/(stride_w * stride_h); i ++)
                {
                    vx_int32 j = 0;
                    for (j = 0; j < (stride_w * stride_h); j ++)
                        memcpy(reshuffled_bias_ptr + (i  * (stride_w * stride_h) + j)* item_size, bias_ptr + i * item_size, item_size);

                }

            }
        }
    }

    if (deconvOperation->create_wbp)
    {
        if (weight_format == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(reshuffle_weights) == VX_QUANT_AFFINE_SCALE)
        {
            vx_weights_biases_parameter_optimizations_t opt;

            opt.inputZeroPoint = TENSOR_TF_ZEROPOINT(inputs);
            opt.zrl = -1;
            opt.outputFormat = VX_TYPE_UINT8;

            deconvOperation->weights_biaes = _createWeightsBiasesParameterFromTensors(
                                vxGetContext((vx_reference)deconvOperation->weights),
                                VX_NN_CONVOLUTION_LAYER,
                                deconvOperation->inputs->dims,/*inputs_dims,*/
                                deconvOperation->inputs->dimCount,
                                deconvOperation->inputs->dimCount,
                                kernel_reshuffle_pad_x_left,
                                kernel_reshuffle_pad_x_right,
                                kernel_reshuffle_pad_y_top,
                                kernel_reshuffle_pad_y_bottom,
                                0,/*pooling_size_x,*/
                                0,/*pooling_size_y,*/
                                0,
                                0,
                                VX_NN_DS_SIZE_ROUNDING_FLOOR,
                                deconvOperation->outputs->dims,/*convolution_outputs_dims,*/
                                deconvOperation->outputs->dims,/*pool_outputs_dims,*/
                                &opt, /*optimizations,*/
                                TENSOR_DATA_TYPE(weights),
                                0,
                                VX_TENSOR_RANK_WHCN,
                                deconvOperation->reshuffled_weights,
                                deconvOperation->reshuffled_biases,
                                VX_NULL,
                                vx_false_e,
                                vx_false_e
                                );
        }
        else
        {
            deconvOperation->weights_biaes = _createWeightsBiasesParameterFromTensors(
                                vxGetContext((vx_reference)deconvOperation->weights),
                                VX_NN_CONVOLUTION_LAYER,
                                deconvOperation->inputs->dims,/*inputs_dims,*/
                                deconvOperation->inputs->dimCount,
                                deconvOperation->inputs->dimCount,
                                kernel_reshuffle_pad_x_left,
                                kernel_reshuffle_pad_x_right,
                                kernel_reshuffle_pad_y_top,
                                kernel_reshuffle_pad_y_bottom,
                                0,/*pooling_size_x,*/
                                0,/*pooling_size_y,*/
                                0,
                                0,
                                VX_NN_DS_SIZE_ROUNDING_FLOOR,
                                deconvOperation->outputs->dims,/*convolution_outputs_dims,*/
                                deconvOperation->outputs->dims,/*pool_outputs_dims,*/
                                NULL, /*optimizations,*/
                                TENSOR_DATA_TYPE(weights),
                                0,
                                VX_TENSOR_RANK_WHCN,
                                deconvOperation->reshuffled_weights,
                                deconvOperation->reshuffled_biases,
                                VX_NULL,
                                vx_false_e,
                                vx_false_e
                                );
        }
    }

    return deconvOperation->weights_biaes == VX_NULL ? VX_FAILURE : VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConv_UpSample(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_deconvolution_operation deconvOperation   = (vxnne_deconvolution_operation)operation;

    vx_tensor inputs        = deconvOperation->inputs;
    vx_tensor outputs       = deconvOperation->outputs;

    vx_type_e input_format = (vx_type_e)(TENSOR_DATA_TYPE(inputs));
    vx_type_e output_format = (vx_type_e)(TENSOR_DATA_TYPE(outputs));

    vx_int32 in_h = TENSOR_SIZE_INDEX(inputs, 1);
    vx_int32 in_w = TENSOR_SIZE_INDEX(inputs, 0);
    vx_int32 out_h = TENSOR_SIZE_INDEX(outputs, 1);
    vx_int32 out_w = TENSOR_SIZE_INDEX(outputs, 0);
    vx_int32 stride_w = (vx_int32)vxnneRound(out_w * 1.0f / in_w, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);
    vx_int32 stride_h = (vx_int32)vxnneRound(out_h * 1.0f / in_h, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);

    vx_int32 padding_x = deconvOperation->padding_x_left->value->n32;
    vx_int32 padding_y = deconvOperation->padding_y_top->value->n32;

    vx_int32 conv_out_channels = TENSOR_SIZE_INDEX(outputs, 2);

    vx_uint8_ptr input_ptr;
    vx_uint8_ptr output_ptr;

    vx_int32 i = 0, j = 0, b = 0;
    vx_int32 input_item_size = vxnneGetTypeSize(input_format);
    vx_int32 output_item_size = vxnneGetTypeSize(output_format);
    vx_int32 s_w = 0, s_h = 0;

    vxoTensor_GetTensorBatchArrayViewMemory(inputs, 0, (gctPOINTER*)&input_ptr, VX_NULL);
    vxoTensor_GetTensorBatchArrayViewMemory(outputs, 0, (gctPOINTER*)&output_ptr, VX_NULL);

    for (b = 0; b < conv_out_channels; b++)
    {
        vx_uint8_ptr output_base = output_ptr + b * out_w * out_h * output_item_size;
        vx_uint8_ptr input_base = input_ptr + b * in_w * in_h * stride_w * stride_h * input_item_size;

        for (s_h = 0; s_h < stride_h; s_h++)
        {
            for (s_w = 0; s_w < stride_w; s_w++)
            {
                vx_int32 slice_in_offset = (s_h * stride_w + s_w) * in_w * in_h;

                for (j = 0; j < in_h; j++)
                {
                    for (i = 0; i < in_w; i++)
                    {
                        vx_int32 out_x = (i * stride_w - padding_x + s_w), out_y = (j * stride_h - padding_y + s_h);
                        vx_int32 output_index = out_x  + out_y * out_w;
                        vx_int32 input_index = slice_in_offset + j * in_w + i;

                        if (out_x < 0 || out_y < 0 || out_x >= out_w || out_y >= out_h)
                            continue;

                        if (input_format == output_format)
                        {
                            memcpy(output_base + output_index * input_item_size, input_base + input_index * input_item_size, input_item_size);
                        }
                        else
                        {
                            /* output_base[output_index] = input_base[input_index];*/
                            vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(outputs), output_index, vxnneGetDataExt(input_format, TENSOR_QUANT_TYPE(inputs), input_index, input_base, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs)), output_base, 0, TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_POS(outputs));

                        }
                    }
                }
            }
        }
    }

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConv_Reshuffle_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_deconvolution_reshuffle_operation reshuffle_operation   = (vxnne_deconvolution_reshuffle_operation)operation;

    if (reshuffle_operation->stride_x)
        vxReleaseScalar(&reshuffle_operation->stride_x);

    if (reshuffle_operation->stride_y)
        vxReleaseScalar(&reshuffle_operation->stride_y);

    if (reshuffle_operation->padding_x_left)
        vxReleaseScalar(&reshuffle_operation->padding_x_left);

    if (reshuffle_operation->padding_x_right)
        vxReleaseScalar(&reshuffle_operation->padding_x_right);

    if (reshuffle_operation->padding_y_top)
        vxReleaseScalar(&reshuffle_operation->padding_y_top);

    if (reshuffle_operation->padding_y_bottom)
        vxReleaseScalar(&reshuffle_operation->padding_y_bottom);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConv_Conv_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_convolution_operation conv_operation   = (vxnne_convolution_operation)operation;

    if (conv_operation->downScaleSizeRounding)
        vxReleaseScalar(&conv_operation->downScaleSizeRounding);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConv_UpSample_DeInilition(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_deconvolution_operation deconv_operation   = (vxnne_deconvolution_operation)operation;

    if (deconv_operation->inputs)
        vxoTensor_ReleaseTensor(&deconv_operation->inputs);

    return status;
}

VX_PRIVATE_API vx_status vxnneExecuteSWDeConvolution(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_deconvolution_operation deconvOperation   = (vxnne_deconvolution_operation)operation;

    vx_tensor inputs        = deconvOperation->inputs;
    vx_tensor weights       = deconvOperation->weights;
    vx_tensor bias          = (deconvOperation->biases != VX_NULL)?deconvOperation->biases:VX_NULL;
    vx_tensor outputs       = deconvOperation->outputs;
    vx_int32 padding_x      = deconvOperation->padding_x_left->value->n32;
    vx_int32 padding_x_right= deconvOperation->padding_x_right->value->n32;
    vx_int32 padding_y      = deconvOperation->padding_y_top->value->n32;
    vx_int32 padding_y_bottom = deconvOperation->padding_y_bottom->value->n32;
    vx_enum rounding_policy = VX_ROUND_POLICY_TO_NEAREST_EVEN;
    vx_int32 a_x            = deconvOperation->a_x->value->n32;
    vx_int32 a_y            = deconvOperation->a_y->value->n32;

    vx_type_e input_format  = (vx_type_e)(TENSOR_DATA_TYPE(inputs));
    vx_type_e output_format = (vx_type_e)(TENSOR_DATA_TYPE(outputs));
    vx_type_e weight_format = (vx_type_e)(TENSOR_DATA_TYPE(weights));
    vx_int8   input_fixPointPos = TENSOR_POS(inputs);
    vx_int8   output_fixPointPos = TENSOR_POS(outputs);
    vx_int8   weights_fixPointPos = TENSOR_POS(weights);
    vx_float32   input_scale = TENSOR_TF_SCALE(inputs);
    vx_float32   output_scale = TENSOR_TF_SCALE(outputs);
    vx_float32   weights_scale = TENSOR_TF_SCALE(weights);
    vx_int32   input_zp = TENSOR_TF_ZEROPOINT(inputs);
    vx_int32   output_zp = TENSOR_TF_ZEROPOINT(outputs);
    vx_int32   weights_zp = TENSOR_TF_ZEROPOINT(weights);
    vx_int8   bias_fixPointPos = (bias != VX_NULL) ? TENSOR_POS(bias) : 0;
    vx_float32   bias_scale = (bias != VX_NULL) ? TENSOR_TF_SCALE(bias) : 0;
    vx_int32   bias_zp = (bias != VX_NULL) ? TENSOR_TF_ZEROPOINT(bias) : 0;

    vx_int32 group = (deconvOperation->group->value->n32 > 0)?deconvOperation->group->value->n32:1;
    vx_int32 g = 0;
    vx_int32 kernel_size_x = weights->viewRegion.viewEnds[INDEX_WIDTH] - weights->viewRegion.viewStarts[INDEX_WIDTH];
    vx_int32 kernel_size_y = weights->viewRegion.viewEnds[INDEX_HEIGHT] - weights->viewRegion.viewStarts[INDEX_HEIGHT];
    vx_int32 kernel_size_c = weights->viewRegion.viewEnds[INDEX_DEPTH] - weights->viewRegion.viewStarts[INDEX_DEPTH];
    vx_int32 in_h = inputs->viewRegion.viewEnds[INDEX_HEIGHT] - inputs->viewRegion.viewStarts[INDEX_HEIGHT];
    vx_int32 in_w = inputs->viewRegion.viewEnds[INDEX_WIDTH] - inputs->viewRegion.viewStarts[INDEX_WIDTH];
    vx_int32 out_h = outputs->viewRegion.viewEnds[INDEX_HEIGHT] - outputs->viewRegion.viewStarts[INDEX_HEIGHT];
    vx_int32 out_w = outputs->viewRegion.viewEnds[INDEX_WIDTH] - outputs->viewRegion.viewStarts[INDEX_WIDTH];

    vx_int32 stride_w = (vx_int32)vxnneRound(out_w * 1.0f/ in_w, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);
    vx_int32 stride_h = (vx_int32)vxnneRound(out_h * 1.0f / in_h, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);

    vx_int32 conv_out_channels = inputs->viewRegion.viewEnds[INDEX_DEPTH] - inputs->viewRegion.viewStarts[INDEX_DEPTH];
    vx_int32 conv_in_channels = outputs->viewRegion.viewEnds[INDEX_DEPTH] - outputs->viewRegion.viewStarts[INDEX_DEPTH];
    vx_int32 kernel_dim = kernel_size_x * kernel_size_y * kernel_size_c;
    vx_int32 conv_in_spatial_dim = in_h * in_w;

    vx_int32 conv_out_spatial_dim = in_h * in_w;
    vx_int32 input_offset = conv_out_channels * conv_in_spatial_dim / group;
    vx_int32 weight_offset = conv_out_channels * kernel_dim / group;
    vx_int32 bias_offset = conv_out_channels / group;
    vx_uint8_ptr col_ptr = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(vx_float32) * kernel_dim * conv_out_spatial_dim * group);
    vx_float32_ptr tmp_ptr = (vx_float32_ptr)vxAllocateAndZeroMemory(sizeof(vx_float32) * out_w * out_h);

    vx_uint8_ptr output_ptr = VX_NULL;
    vx_uint8_ptr weight_ptr = VX_NULL;
    vx_uint8_ptr input_ptr = VX_NULL/*(conv_out_channels/group)*height*width*/;

    vx_bool input_stage = vx_false_e, output_stage = vx_false_e;
    vx_uint8_ptr bias_ptr = VX_NULL;
    vx_type_e bias_format  = VX_TYPE_INVALID;
    if(bias)
    {
        bias_format = (vx_type_e)(TENSOR_DATA_TYPE(bias));
        vxnneGetTensorMemeory(bias, (vx_ptr_ptr)&bias_ptr, input_stage, vx_false_e);
    }

    vxnneGetTensorMemeory(inputs, (vx_ptr_ptr)&input_ptr, input_stage, vx_false_e);
    vxnneGetTensorMemeory(weights, (vx_ptr_ptr)&weight_ptr, input_stage, vx_false_e);
    vxnneGetTensorMemeory(outputs, (vx_ptr_ptr)&output_ptr, output_stage, vx_true_e);
    gcoOS_MemFill(col_ptr, 0, sizeof(vx_float32) * kernel_dim * conv_out_spatial_dim * group);

    for(g = 0; g < group; ++g){
        vx_uint8_ptr g_col_ptr = sizeof(vx_float32) * kernel_dim * conv_out_spatial_dim * g + col_ptr;
        vx_uint8_ptr g_weight_ptr = weight_offset * vxnneGetTypeSize(weight_format) * g + weight_ptr;
        vx_uint8_ptr g_bias_ptr = bias_offset * vxnneGetTypeSize(bias_format) * g + bias_ptr;
        vx_uint8_ptr g_input_ptr =  input_offset * vxnneGetTypeSize(input_format) * g + input_ptr;
        vx_uint8_ptr g_output_ptr = conv_out_channels * out_h * out_w / group * vxnneGetTypeSize(output_format) * g + output_ptr;
#define CblasTrans 1
#define CblasNoTrans 0

        /* vxnneLayerSW_gemm(
                vx_int32 TA, vx_int32 TB,
                vx_int32 M, vx_int32 N, vx_int32 K, vx_float32 ALPHA,
                vx_uint8_ptr A, vx_int32 lda,
                vx_uint8_ptr B, vx_int32 ldb,
                vx_float32 BETA,
                vx_uint8_ptr C, vx_int32 ldc);

            CblasTrans  : 1;
            CblasNoTrans: 0;
            int lda = (TransA == CblasNoTrans) ? K : M;
            int ldb = (TransB == CblasNoTrans) ? N : K;


            vxnneLayerSW_col2im(
                vx_uint8_ptr data_col, const vx_int32 channels,
                const vx_int32 height, const vx_int32 width, const vx_int32 kernel_h, const vx_int32 kernel_w,
                const vx_int32 pad_h, const vx_int32 pad_w,
                const vx_int32 stride_h, const vx_int32 stride_w,
                const vx_int32 dilation_h, const vx_int32 dilation_w,
                vx_uint8_ptr data_im);

        */
        if(input_format == VX_TYPE_UINT8 && weight_format == VX_TYPE_UINT8)
        {
            vxnneLayerSW_gemm_u8(weight_format, input_format, VX_TYPE_FLOAT32, rounding_policy, weights_zp, weights_scale, input_zp, input_scale, 0,
                    CblasTrans, CblasNoTrans,
                    kernel_dim, conv_in_spatial_dim, (conv_out_channels/group), 1,
                    g_weight_ptr, kernel_dim,
                    g_input_ptr, conv_in_spatial_dim,
                    0,
                    g_col_ptr, conv_in_spatial_dim);
        }
        else
        {
            vxnneLayerSW_gemm(weight_format, input_format, VX_TYPE_FLOAT32, rounding_policy, weights_fixPointPos, input_fixPointPos, 0,
                CblasTrans, CblasNoTrans,
                kernel_dim, conv_in_spatial_dim, (conv_out_channels/group), 1,
                g_weight_ptr, kernel_dim,
                g_input_ptr, conv_in_spatial_dim,
                0,
                g_col_ptr, conv_in_spatial_dim);
        }

        if(output_format == VX_TYPE_UINT8)
        {
            vxnneLayerSW_col2im_add_bias_u8(VX_TYPE_FLOAT32, output_format, bias_format,rounding_policy, 0, output_zp, output_scale, bias_zp, bias_scale,
                    g_col_ptr, bias_ptr ? g_bias_ptr:NULL,conv_in_channels/group,
                    out_h, out_w, kernel_size_y, kernel_size_x,
                    padding_y, padding_x,
                    padding_y_bottom, padding_x_right,
                    stride_h, stride_w,
                    a_x, a_y,
                    g_output_ptr,tmp_ptr);
        }
        else
        {
            vxnneLayerSW_col2im_add_bias(VX_TYPE_FLOAT32, output_format, bias_format,rounding_policy, 0, output_fixPointPos,bias_fixPointPos,
                g_col_ptr, bias_ptr ? g_bias_ptr:NULL,conv_in_channels/group,
                out_h, out_w, kernel_size_y, kernel_size_x,
                padding_y, padding_x,
                padding_y_bottom, padding_x_right,
                stride_h, stride_w,
                a_x, a_y,
                g_output_ptr,tmp_ptr);
        }
    }

    vxFree(col_ptr);
    vxFree(tmp_ptr);

    if (input_stage)
    {
        vxFree(input_ptr);
        vxFree(weight_ptr);
    }

    if (output_stage)
    {
        vx_uint32 tensor_size = 0;
        vx_ptr outputs_tensor_logical = VX_NULL;
        vxoTensor_GetTensorSize(outputs, &tensor_size);
        vxoTensor_GetTensorViewMemory(outputs, &outputs_tensor_logical, VX_NULL);

        gcoOS_MemCopy(outputs_tensor_logical, output_ptr, tensor_size);

        vxFree(output_ptr);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNDeConvolutionLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDeConvolutionLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDeConvolutionLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

typedef enum _gcoNNDeConv_Mode
{
    gcoNNE_DECONV_MODE_SW,
    gcoNNE_DECONV_MODE_SW1,
    gcoNNE_DECONV_MODE_SHADER,
    gcoNNE_DECONV_MODE_SHADER1,
    gcoNNE_DECONV_MODE_NNE_TP,
}
gcoNNDeConv_Mode;

VX_PRIVATE_API vx_status vxoNNDeConvolution_GetPad(vx_int32 in, vx_int32 out, vx_int32 stride, vx_int32 kernel_reshuffle, vx_int32 pad_head, vx_int32 pad_tail,
                                    vx_int32_ptr decov_output, vx_int32_ptr kernel_resuffle_pad_head, vx_int32_ptr kernel_resuffle_pad_tail,
                                    vx_int32_ptr upsample_pad, vx_uint32_ptr sample_output, vx_uint32_ptr upsample_output)
{

            if ((out < in * stride))
            {
                *decov_output = in;

                *kernel_resuffle_pad_head = ((*decov_output - 1) + kernel_reshuffle - in) / 2;
                *kernel_resuffle_pad_tail = ((*decov_output - 1) + kernel_reshuffle - in) - *kernel_resuffle_pad_head;

                *sample_output = *decov_output;

                *upsample_output = *decov_output * stride;

                *upsample_pad = (pad_tail > pad_head)?(vx_int32)ceilf((*decov_output * stride - out)/2.0f):(*decov_output * stride - out)/2;
            }
            else if ((out > in * stride))
            {
                *decov_output = gcmALIGN_NP2(out, stride) / stride;

                *kernel_resuffle_pad_head = ((*decov_output - 1) + kernel_reshuffle - in) / 2;
                *kernel_resuffle_pad_tail = ((*decov_output - 1) + kernel_reshuffle - in) - *kernel_resuffle_pad_head;

                *sample_output = *decov_output;

                *upsample_output = *decov_output * stride;

                *upsample_pad = (vx_int32)ceilf((*decov_output * stride - out)/2.0f);
            }
            else
            {
                *upsample_pad = *kernel_resuffle_pad_head * (stride - 1);
            }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDeConvolutionLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  weights                    = (vx_tensor)parameters[1];
    vx_tensor  bias                       = (vx_tensor)parameters[2];
    vx_scalar  padding_x                  = (vx_scalar)parameters[3];
    vx_scalar  padding_x_right            = (vx_scalar)parameters[4];
    vx_scalar  padding_y                  = (vx_scalar)parameters[5];
    vx_scalar  padding_y_bottom           = (vx_scalar)parameters[6];
    vx_scalar  overflow_policy            = (vx_scalar)parameters[7];
    vx_scalar  rounding_policy            = (vx_scalar)parameters[8];
    vx_scalar  a_x                        = (vx_scalar)parameters[9];
    vx_scalar  a_y                        = (vx_scalar)parameters[10];
    vx_scalar  group                      = (vx_scalar)parameters[11];
    vx_scalar  stride_w_s                 = (vx_scalar)parameters[12];
    vx_scalar  stride_h_s                 = (vx_scalar)parameters[13];
    vx_tensor  outputs                    = (vx_tensor)parameters[num - 1];
    gcoNNDeConv_Mode deconvolution_mode   = /* gcoNNE_DECONV_MODE_SW; gcoNNE_DECONV_MODE_SW1; */ gcoNNE_DECONV_MODE_NNE_TP;
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_bool depthwise = vx_false_e;

    vxnne_deconvolution_layer  deconvolutionLayer = VX_NULL;
    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_int32 in_h = TENSOR_SIZE_INDEX(inputs, 1);
    vx_int32 in_w = TENSOR_SIZE_INDEX(inputs, 0);
    vx_int32 out_h = TENSOR_SIZE_INDEX(outputs, 1);
    vx_int32 out_w = TENSOR_SIZE_INDEX(outputs, 0);
    vx_int32 pad_x      = padding_x->value->n32;
    vx_int32 pad_y      = padding_y->value->n32;
    vx_int32 pad_x_right= padding_x_right->value->n32;
    vx_int32 pad_y_bottom = padding_y_bottom->value->n32;
    vx_int32 kernel_size_x = TENSOR_SIZE_INDEX(weights, 0);
    vx_int32 kernel_size_y = TENSOR_SIZE_INDEX(weights, 1);
    /* de-covolution*/
    vx_int32 stride_w = (stride_w_s != VX_NULL) ? (stride_w_s->value->n32) : (vx_int32)vxnneRound((out_w - kernel_size_x - a_x->value->n32 + pad_x + pad_x_right) * 1.0f / (in_w - 1), VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);
    vx_int32 stride_h = (stride_h_s != VX_NULL) ? (stride_h_s->value->n32) : (vx_int32)vxnneRound((out_h - kernel_size_y - a_y->value->n32 + pad_y + pad_y_bottom) * 1.0f / (in_h - 1), VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING);

    vx_int32 kernel_reshuffle_width = gcmALIGN_NP2(kernel_size_x, stride_w)/stride_w, kernel_reshuffle_height = gcmALIGN_NP2(kernel_size_y, stride_h)/stride_h;
    vx_int32 kernel_reshuffle_pad_x_right = (in_w - 1) * stride_w - (out_w + gcmMAX(pad_x, pad_x_right) - kernel_reshuffle_width * stride_w);
    vx_int32 kernel_reshuffle_pad_x_left = gcmMAX(pad_x, pad_x_right);
    vx_int32 kernel_reshuffle_pad_y_bottom = (in_h - 1) * stride_h - (out_h + gcmMAX(pad_y, pad_y_bottom) - kernel_reshuffle_height * stride_h);
    vx_int32 kernel_reshuffle_pad_y_top = gcmMAX(pad_y, pad_y_bottom);
    vx_int32 decov_output_w = in_w + kernel_reshuffle_pad_x_left + kernel_reshuffle_pad_x_right - kernel_reshuffle_width + 1;
    vx_int32 decov_output_h = in_h + kernel_reshuffle_pad_y_top + kernel_reshuffle_pad_y_bottom - kernel_reshuffle_height + 1;

    vx_bool need_upsample = (in_w == out_w && in_h == out_h) ? vx_false_e : vx_true_e;
    need_upsample = ((decov_output_w != out_w) || (decov_output_h != out_h))?vx_true_e:vx_false_e;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_deconvolution_layer_s), (gctPOINTER*)&deconvolutionLayer);
    if (!deconvolutionLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(deconvolutionLayer, sizeof(vxnne_deconvolution_layer_s));

    vxnneLayer_Initialize(&deconvolutionLayer->base,
                          "DeConvolutionLayer",
                          node,
                          vxmOPERATION_COUNT(deconvolutionLayer),
                          deconvolutionLayer->operations,
                          VX_NULL);

    {
        vx_uint32 group_size     = 0;
        vx_bool   dataTypeFlg    = vx_false_e;
        vx_uint32 kx             = TENSOR_VIEW_SIZE_INDEX(weights, 0);
        vx_uint32 ky             = TENSOR_VIEW_SIZE_INDEX(weights, 1);
        vx_uint32 src_width      = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
        vx_uint32 src_height     = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
        vx_uint32 src_depth      = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
        vx_uint32 dst_width      = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
        vx_uint32 dst_height     = TENSOR_VIEW_SIZE_INDEX(outputs, 1);

        if((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
            || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8 && (kx == 2 && ky ==2))
            || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16 && (kx == 2 && ky ==2)))
            dataTypeFlg = vx_true_e;

        group_size = group->value->u32;
        depthwise = (group_size == src_depth && group_size != 1) ? vx_true_e : vx_false_e;

        if (((kx == 2 && ky == 2) || (kx == 4 && ky == 4))
            && src_width * 2 == dst_width && src_height * 2 == dst_height
            && group_size == src_depth && dataTypeFlg && bias != NULL)
            deconvolution_mode = gcoNNE_DECONV_MODE_SHADER;
    }
    /* check NN input and output format according to need_upsample in gcoNNE_DECONV_MODE_NNE_TP*/
    if (((need_upsample && !vxnneIsNNSupportFormat(vxGetContext((vx_reference)inputs), inputs, VX_NULL, VX_NULL)) ||
        (!need_upsample && !vxnneIsNNSupportFormat(vxGetContext((vx_reference)inputs), inputs, VX_NULL, outputs))) &&
        (deconvolution_mode != gcoNNE_DECONV_MODE_SHADER))
    {
        vxError("Shander not support this format, goto cpu path temporarily. function %s line %d\n", __FUNCTION__, __LINE__);
        deconvolution_mode = gcoNNE_DECONV_MODE_SW;
    }

    {
        gctSTRING envctrl = gcvNULL;
        if (gcmIS_SUCCESS(gcoOS_GetEnv(gcvNULL, "USE_DECONV_MODE_SW", &envctrl)) && envctrl)
        {
            vx_int32 flag = atoi(envctrl);
            if(flag)
                deconvolution_mode = gcoNNE_DECONV_MODE_SW;
        }
    }

    switch (deconvolution_mode)
    {
    case gcoNNE_DECONV_MODE_SHADER:
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            shaderExecutable = vxnneDeConvolutionShaderExecutable(node->base.context, VXNNE_KERNEL_DECONVOLUTION, &node->kernelAttributes.borderMode,
                inputs,
                weights,
                bias,
                padding_x,
                padding_y,
                overflow_policy,
                rounding_policy,
                a_x,
                a_y,
                group,
                outputs);
            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&deconvolutionLayer->deconvolution_sh_operation,
                                          &deconvolutionLayer->base,
                                          VXNNE_OPERATOR_DECONVOLUTION,
                                          batchCount,
                                          shaderExecutable);

            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sh_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sh_operation.base, (vx_reference)bias, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &deconvolutionLayer->base,
                &deconvolutionLayer->deconvolution_sh_operation.base,
                0);

            break;
        }

    case gcoNNE_DECONV_MODE_SW:
        {
            vxnneOperation_Initialize(&deconvolutionLayer->deconvolution_sw_operation.base,
                                      &deconvolutionLayer->base,
                                      VXNNE_OPERATION_TARGET_SW,
                                      VXNNE_OPERATOR_DECONVOLUTION,
                                      vxnneExecuteSWDeConvolution,
                                      VX_NULL,
                                      batchCount,
                                      0);

            vxnneLayer_SetOperation(
                &deconvolutionLayer->base,
                &deconvolutionLayer->deconvolution_sw_operation.base,
                0);

            deconvolutionLayer->deconvolution_sw_operation.inputs           = inputs;
            deconvolutionLayer->deconvolution_sw_operation.weights          = weights;
            deconvolutionLayer->deconvolution_sw_operation.biases           = bias;
            deconvolutionLayer->deconvolution_sw_operation.padding_x_left   = padding_x;
            deconvolutionLayer->deconvolution_sw_operation.padding_x_right  = padding_x_right;
            deconvolutionLayer->deconvolution_sw_operation.padding_y_top    = padding_y;
            deconvolutionLayer->deconvolution_sw_operation.padding_y_bottom = padding_y_bottom;
            deconvolutionLayer->deconvolution_sw_operation.overflow_policy  = overflow_policy;
            deconvolutionLayer->deconvolution_sw_operation.rounding_policy  = rounding_policy;
            deconvolutionLayer->deconvolution_sw_operation.a_x              = a_x;
            deconvolutionLayer->deconvolution_sw_operation.a_y              = a_y;
            deconvolutionLayer->deconvolution_sw_operation.group            = group;
            deconvolutionLayer->deconvolution_sw_operation.outputs          = outputs;

            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw_operation.base, (vx_reference)bias, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            break;
        }

    case gcoNNE_DECONV_MODE_SW1:
    case gcoNNE_DECONV_MODE_NNE_TP:
        {
            vx_int32 index = 0, idx = 0;

            vx_type_e output_format = (vx_type_e)(TENSOR_DATA_TYPE(outputs));
            vx_type_e weight_format = (vx_type_e)(TENSOR_DATA_TYPE(weights));

            vx_int32 kernel_channel = TENSOR_SIZE_INDEX(weights, 2);
            vx_int32 kernel_batch = TENSOR_SIZE_INDEX(weights, 3);

            vx_bool tp_upsample = (deconvolution_mode == gcoNNE_DECONV_MODE_NNE_TP)? vx_true_e : vx_true_e;
            vx_int32 upsample_pad_x = 0, upsample_pad_y = 0;
            vx_bool need_clip = vx_false_e;
            vx_int32 s = 1;

            vx_context context = vxGetContext((vx_reference)inputs);
            vx_tensor sample_output = VX_NULL, sampled_output = VX_NULL, reshuffled_weights = VX_NULL, reshuffled_bias = VX_NULL;

            vx_uint32 size[][4] = {
                 /*
                  * {2, 2, 21, 84},
                  * {17, 17, 84, 1},
                  */

                 {kernel_reshuffle_width, kernel_reshuffle_height, kernel_batch, stride_w * stride_h * kernel_channel}, /* reshuffled_weights */
                 {decov_output_w, decov_output_h, stride_w * stride_h * kernel_channel, 1}, /* sample_output */
                 {1, 1, 1, kernel_batch * stride_w * stride_h}, /* reshuffled_bias */
                 { decov_output_w * stride_w, decov_output_h * stride_h, kernel_channel, 1 }, /* upsampled_output */
            };
            vx_tensor_create_params_t tensor_create_params;

            vx_int8 weight_fixed_point_pos = TENSOR_POS(weights);
            vx_int8 output_fixed_point_pos = TENSOR_POS(outputs);

            vx_bool tfQuant = vx_false_e;

            if (depthwise)
            {
                size[0][3] = stride_w * stride_h * kernel_batch;
                size[1][2] *= kernel_batch;
                size[3][2] = kernel_batch;
            }

            vxoNNDeConvolution_GetPad(in_w, out_w, stride_w, kernel_reshuffle_width, pad_x, pad_x_right,
                &decov_output_w, &kernel_reshuffle_pad_x_left, &kernel_reshuffle_pad_x_right,
                &upsample_pad_x, &size[1][0], &size[3][0]);

            vxoNNDeConvolution_GetPad(in_h, out_h, stride_h, kernel_reshuffle_height, pad_y, pad_y_bottom,
                &decov_output_h, &kernel_reshuffle_pad_y_top, &kernel_reshuffle_pad_y_bottom,
                &upsample_pad_y, &size[1][1], &size[3][1]);
            need_clip = ((upsample_pad_x > 0) || (upsample_pad_y > 0) || (out_w < in_w * stride_w) || (out_h < in_h * stride_h)) ? vx_true_e : vx_false_e;

            if (weight_format == VX_TYPE_UINT8 && output_format == VX_TYPE_UINT8 && TENSOR_QUANT_TYPE(weights) == VX_QUANT_AFFINE_SCALE && TENSOR_QUANT_TYPE(outputs) == VX_QUANT_AFFINE_SCALE)
            {
                tfQuant = vx_true_e;
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 4;
            tensor_create_params.sizes = size[1];
            tensor_create_params.data_format = tfQuant ? VX_TYPE_UINT8: output_format;
            tensor_create_params.quant_format = tfQuant ? VX_QUANT_AFFINE_SCALE : TENSOR_QUANT_TYPE(outputs);;
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = output_fixed_point_pos;
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(outputs);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(outputs);
            }

            if (need_upsample)
            {

                sample_output = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_true_e);
                if (sample_output == VX_NULL)
                {
                    vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                    status = VX_ERROR_NO_MEMORY;
                    goto exit;
                }

                if (need_clip)
                {
                    tensor_create_params.num_of_dims = 3;
                    tensor_create_params.sizes = size[3];
                    sampled_output = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_true_e);
                    if (sampled_output == VX_NULL)
                    {
                        vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                        status = VX_ERROR_NO_MEMORY;
                        goto exit;
                    }

                }
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = 4;
            tensor_create_params.sizes = size[0];
            tensor_create_params.data_format = tfQuant ? VX_TYPE_UINT8: weight_format;
            tensor_create_params.quant_format = tfQuant ? VX_QUANT_AFFINE_SCALE : TENSOR_QUANT_TYPE(weights);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = weight_fixed_point_pos;
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(weights);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(weights);
            }

            reshuffled_weights = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);
            if (reshuffled_weights == VX_NULL)
            {
                vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                status = VX_ERROR_NO_MEMORY;
                goto exit;
            }

            if (bias != NULL)
            {
                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = 4;
                tensor_create_params.sizes = size[2];
                tensor_create_params.data_format = TENSOR_DATA_TYPE(bias);;
                tensor_create_params.quant_format = tfQuant ? VX_QUANT_AFFINE_SCALE : TENSOR_QUANT_TYPE(bias);
                if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
                {
                    tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(bias);
                }
                else
                {
                    tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(bias);
                    tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(bias);
                }

                reshuffled_bias = vxoTensor_CreateTensor(context, node->graph, &tensor_create_params, vx_false_e);
                if (reshuffled_bias == VX_NULL)
                {
                    vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
                    status = VX_ERROR_NO_MEMORY;
                    goto exit;
                }

                vxoTensor_AllocateMemory(reshuffled_bias);
            }
            if (deconvolution_mode == gcoNNE_DECONV_MODE_SW1)
            {
                vx_enum downScaleSizeRounding = VX_NN_DS_SIZE_ROUNDING_FLOOR;

                /* Initialize reshuffle weights operation */
                vxnneOperation_Initialize(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base,
                                          &deconvolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_DECONVOLUTION,
                                          vxnneExecuteSWDeConv_ReshuffleWeights,
                                          vxnneExecuteSWDeConv_Reshuffle_DeInilition,
                                          batchCount,
                                          0);
                vxnneLayer_SetOperation(
                    &deconvolutionLayer->base,
                    &deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base,
                    index ++);

                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.inputs           = inputs;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.weights          = weights;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.bias             = bias;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.stride_x         = vxCreateScalar(context, VX_TYPE_INT32, &stride_w);
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.stride_y         = vxCreateScalar(context, VX_TYPE_INT32, &stride_h);
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.a_x              = a_x;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.a_y              = a_y;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.group            = group;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.outputs          = need_upsample ? sample_output : outputs;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_weights= reshuffled_weights;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_biases = reshuffled_bias;

                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base, (vx_reference)weights, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base, (vx_reference)bias, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_weights, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base, (vx_reference)reshuffled_bias, VXNNE_OPERATION_REFENRENCE_OUTPUT);


                /* Initialize covolution operation */
                vxnneOperation_Initialize(&deconvolutionLayer->deconvolution_sw1_convolution_operation.base,
                                          &deconvolutionLayer->base,
                                          VXNNE_OPERATION_TARGET_SW,
                                          VXNNE_OPERATOR_DECONVOLUTION,
                                          vxnneExecuteSWConvolution,
                                          vxnneExecuteSWDeConv_Conv_DeInilition,
                                          batchCount,
                                          0);

                vxnneLayer_SetOperation(
                    &deconvolutionLayer->base,
                    &deconvolutionLayer->deconvolution_sw1_convolution_operation.base,
                    index++);

                deconvolutionLayer->deconvolution_sw1_convolution_operation.inputs                  = inputs;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.weights                 = reshuffled_weights;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.biases                  = reshuffled_bias;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.padX                    = vxCreateScalar(context, VX_TYPE_INT32, &kernel_reshuffle_pad_x_left);
                deconvolutionLayer->deconvolution_sw1_convolution_operation.padY                    = vxCreateScalar(context, VX_TYPE_INT32, &kernel_reshuffle_pad_y_top);
                deconvolutionLayer->deconvolution_sw1_convolution_operation.strideX                 = vxCreateScalar(context, VX_TYPE_INT32, &s);
                deconvolutionLayer->deconvolution_sw1_convolution_operation.strideY                 = vxCreateScalar(context, VX_TYPE_INT32, &s);
                deconvolutionLayer->deconvolution_sw1_convolution_operation.overflowPolicy          = overflow_policy;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.roundingPolicy          = rounding_policy;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.downScaleSizeRounding   = vxCreateScalar(context, VX_TYPE_ENUM, &downScaleSizeRounding);
                deconvolutionLayer->deconvolution_sw1_convolution_operation.outputs                 = need_upsample ? sample_output : outputs;

                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_convolution_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_convolution_operation.base, (vx_reference)reshuffled_weights, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_convolution_operation.base, (vx_reference)(need_upsample ? sample_output : outputs), VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
            else
            {
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.create_wbp                = vx_true_e;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.inputs                    = inputs;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.weights                   = weights;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.bias                      = bias;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_biases         = reshuffled_bias;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.stride_x                  = vxCreateScalar(context, VX_TYPE_INT32, &stride_w);
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.stride_y                  = vxCreateScalar(context, VX_TYPE_INT32, &stride_h);
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.group                     = group;
                deconvolutionLayer->deconvolution_sw1_convolution_operation.outputs                 = need_upsample ? sample_output : outputs;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_weights        = reshuffled_weights;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_biases         = reshuffled_bias;
                deconvolutionLayer->deconvolution_sw1_reshuffle_operation.outputs = need_upsample ? sample_output : outputs;

                vxoTensor_AllocateMemory(deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_weights);
                if (bias != VX_NULL)
                {
                    vxoTensor_AllocateMemory(deconvolutionLayer->deconvolution_sw1_reshuffle_operation.reshuffled_biases);
                }

                status = vxnneExecuteSWDeConv_ReshuffleWeights(&deconvolutionLayer->deconvolution_sw1_reshuffle_operation.base);
                if (status != VX_SUCCESS) goto exit;

                if (deconvolutionLayer->deconvolution_sw1_reshuffle_operation.weights_biaes)
                {
                    vx_weights_biases_parameter weights_biases = deconvolutionLayer->deconvolution_sw1_reshuffle_operation.weights_biaes;

                    /* Initialize covolution operation */
                    status = vxnneOperation_Initialize(&deconvolutionLayer->convolution_operation.base,
                                                       &deconvolutionLayer->base,
                                                       VXNNE_OPERATION_TARGET_NN,
                                                       VXNNE_OPERATOR_CONVOLUTION,
                                                       VX_NULL,
                                                       NULL,
                                                       batchCount,
                                                       NNE_COMMAND_SIZE);
                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &deconvolutionLayer->base,
                        &deconvolutionLayer->convolution_operation.base,
                        index++);

                    deconvolutionLayer->convolution_operation.orig_inputs      = inputs;
                    deconvolutionLayer->convolution_operation.inputs           = inputs;
                    deconvolutionLayer->convolution_operation.weights_biases   = weights_biases;
                    deconvolutionLayer->convolution_operation.outputs          = need_upsample?sample_output: outputs;

                    vxnneOperation_AddReference(&deconvolutionLayer->convolution_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&deconvolutionLayer->convolution_operation.base, (vx_reference)(need_upsample ? sample_output : outputs), VXNNE_OPERATION_REFENRENCE_OUTPUT);
                    {
                        vx_op_param_s conv;
                        memset(&conv, 0, sizeof(vx_op_param_s));
                        conv.pad_x_left = kernel_reshuffle_pad_x_left;
                        conv.pad_x_right = kernel_reshuffle_pad_x_right;
                        conv.pad_y_top = kernel_reshuffle_pad_y_top;
                        conv.pad_y_bottom = kernel_reshuffle_pad_y_bottom;
                        conv.pad_mode = VX_PAD_CONSTANT;
                        conv.pad_const = TENSOR_TF_ZEROPOINT(inputs);
                        conv.pool_type = VX_NN_POOLING_MAX-1;
                        conv.conv_rounding_type = VX_NN_DS_SIZE_ROUNDING_FLOOR;
                        conv.enable_relu = vx_false_e;
                        conv.pool_size_x = conv.pool_size_y = 0;
                        memcpy(&deconvolutionLayer->convolution_operation.base.parameter, &conv, sizeof(vx_op_param_s));
                    }
                }
            }

            if (need_upsample)
            {
                if (tp_upsample && (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_UPSAMPLE)) &&
                    (need_clip || (!need_clip && vxnneIsTPSupportFormat(context, VX_NULL, VX_NULL, outputs))))
                {
                    vx_op_param_s conv = { 0 };
                    status = vxnneOperation_Initialize(&deconvolutionLayer->upsample_tp_operation.base,
                        &deconvolutionLayer->base,
                        VXNNE_OPERATION_TARGET_TP,
                        VXNNE_OPERATOR_UPSAMPLE,
                        VX_NULL,
                        vxnneExecuteSWDeConv_UpSample_DeInilition,
                        batchCount,
                        0);
                    if (status != VX_SUCCESS) goto exit;

                    vxnneLayer_SetOperation(
                        &deconvolutionLayer->base,
                        &deconvolutionLayer->upsample_tp_operation.base,
                        index++);

                    deconvolutionLayer->upsample_tp_operation.input = sample_output;
                    deconvolutionLayer->upsample_tp_operation.output = need_clip ? sampled_output : outputs;

                    vxnneOperation_AddReference(&deconvolutionLayer->upsample_tp_operation.base, (vx_reference)sample_output, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&deconvolutionLayer->upsample_tp_operation.base, (vx_reference)(need_clip ? sampled_output : outputs), VXNNE_OPERATION_REFENRENCE_OUTPUT);

                    conv.pad_x_left = 0;
                    conv.pad_y_top = 0;
                    conv.pool_size_x = 0;
                    conv.pool_size_y = 0;
                    conv.pool_stride = 1;
                    conv.enable_relu = vx_false_e;
                    conv.pad_mode = VX_PAD_CONSTANT;
                    conv.pad_const = 0;
                    conv.tpType = TP_UPSAMPLE;
                    conv.other_ref = gcvNULL;
                    conv.data_buff = gcvNULL;

                    memcpy(&deconvolutionLayer->upsample_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

                    /* clip the output */
                    if (need_clip)
                    {

                        status = vxnneOperation_Initialize(&deconvolutionLayer->upsample_tp_operation_clip.base,
                            &deconvolutionLayer->base,
                            VXNNE_OPERATION_TARGET_TP,
                            VXNNE_OPERATOR_UPSAMPLE,
                            VX_NULL,
                            vxnneExecuteSWDeConv_UpSample_DeInilition,
                            batchCount,
                            0);
                        if (status != VX_SUCCESS) goto exit;

                        vxnneLayer_SetOperation(
                            &deconvolutionLayer->base,
                            &deconvolutionLayer->upsample_tp_operation_clip.base,
                            index++);

                        deconvolutionLayer->upsample_tp_operation_clip.input = sampled_output;
                        deconvolutionLayer->upsample_tp_operation_clip.output = outputs;

                        vxnneOperation_AddReference(&deconvolutionLayer->upsample_tp_operation_clip.base, (vx_reference)sampled_output, VXNNE_OPERATION_REFENRENCE_INPUT);
                        vxnneOperation_AddReference(&deconvolutionLayer->upsample_tp_operation_clip.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                        conv.pad_x_left = upsample_pad_x;
                        conv.pad_y_top = upsample_pad_y;
                        conv.pool_size_x = 0;
                        conv.pool_size_y = 0;
                        conv.pool_stride = 1;
                        conv.enable_relu = vx_false_e;
                        conv.pad_mode = VX_PAD_CONSTANT;
                        conv.pad_const = 0;
                        conv.tpType = TP_UPSAMPLE_CLIP;
                        conv.other_ref = gcvNULL;
                        conv.data_buff = gcvNULL;

                        memcpy(&deconvolutionLayer->upsample_tp_operation_clip.base.parameter, &conv, sizeof(vx_op_param_s));

                        deconvolutionLayer->base.temp_tensors[idx++] = sampled_output;
                    }
                }
                else
                {
                    /* Initialize upsample operation */
                    vxnneOperation_Initialize(&deconvolutionLayer->deconvolution_sw1_upsample_operation.base,
                                              &deconvolutionLayer->base,
                                              VXNNE_OPERATION_TARGET_SW,
                                              VXNNE_OPERATOR_DECONVOLUTION,
                                              vxnneExecuteSWDeConv_UpSample,
                                              vxnneExecuteSWDeConv_UpSample_DeInilition,
                                              batchCount,
                                              0);

                    vxnneLayer_SetOperation(
                        &deconvolutionLayer->base,
                        &deconvolutionLayer->deconvolution_sw1_upsample_operation.base,
                        index++);

                    deconvolutionLayer->deconvolution_sw1_upsample_operation.inputs           = sample_output;

                    deconvolutionLayer->deconvolution_sw1_upsample_operation.overflow_policy  = overflow_policy;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.rounding_policy  = rounding_policy;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.a_x              = a_x;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.a_y              = a_y;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.group            = group;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.outputs          = outputs;
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.padding_x_left   = vxCreateScalar(context, VX_TYPE_INT32, &upsample_pad_x);
                    deconvolutionLayer->deconvolution_sw1_upsample_operation.padding_y_top    = vxCreateScalar(context, VX_TYPE_INT32, &upsample_pad_y);

                    vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_upsample_operation.base, (vx_reference)sample_output, VXNNE_OPERATION_REFENRENCE_INPUT);
                    vxnneOperation_AddReference(&deconvolutionLayer->deconvolution_sw1_upsample_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
                }

                deconvolutionLayer->base.temp_tensors[idx++]   = sample_output;
            }
            deconvolutionLayer->base.temp_tensors[idx ++]      = reshuffled_weights;
            deconvolutionLayer->base.temp_tensors[idx ++]      = reshuffled_bias;
            deconvolutionLayer->base.num_temp_tensors          = idx;

            break;
        }

    default:
        vxError("Not Support[DECONVOLUTION]!");
        break;

    }

    node->layer = &deconvolutionLayer->base;
     return status;
exit:
    if (deconvolutionLayer != NULL)
        gcoOS_Free(NULL, deconvolutionLayer);
     return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNDeConvolutionLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 L2Normalize
 ***************************************************************************************************************************/

vx_status vxnneExecuteSWL2Normalize(struct _vxnne_operation_s *operation)
{
    vxnne_l2normalize_operation l2normalizeOperation   = (vxnne_l2normalize_operation)operation;

    vx_tensor  inputs           = (vx_tensor)l2normalizeOperation->inputs;
    vx_tensor  outputs          = (vx_tensor)l2normalizeOperation->outputs;

    vx_uint32  dims             = TENSOR_VIEW_DIM_NUM(inputs);
    vx_uint32  input_width      = TENSOR_SIZE_INDEX(inputs, 0);
    vx_uint32  input_height     = TENSOR_SIZE_INDEX(inputs, 1);
    vx_uint32  input_depth      = TENSOR_SIZE_INDEX(inputs, 2);
    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);
    vx_int8   inputFixedPoint   = TENSOR_POS(inputs);
    vx_int8   outputFixedPoint  = TENSOR_POS(outputs);

    vx_uint32  i,j,c;
    vx_float32 epsilon = (vx_float32)1e-12;
    vx_float32 rsqrt = 0.0f;
    vx_float32 data = 0.0f;
    gctPOINTER inputBase;
    gctPOINTER outputBase;

    vx_status status = VX_SUCCESS;

    switch(dims)
    {
    case 1:
        input_depth   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
        input_width   = 1;
        input_height  = 1;
        break;
    case 2:
        input_width   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
        input_depth   = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
        input_height  = 1;
        break;
    case 3:
    case 4:
        input_width   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
        input_height  = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
        input_depth   = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
        break;
    default:
        vxError("Input tensor error dimension[%u]\n", dims);
    }

    vxoTensor_GetTensorBatchArrayViewMemory(inputs, operation->currBatchIndex, (gctPOINTER *)&inputBase, VX_NULL);
    vxoTensor_GetTensorBatchArrayViewMemory(outputs, operation->currBatchIndex, (gctPOINTER *)&outputBase, VX_NULL);

    if ((inputFormat != VX_TYPE_FLOAT16 && inputFormat != VX_TYPE_FLOAT32 && inputFormat != VX_TYPE_INT8 && inputFormat != VX_TYPE_INT16 && inputFormat != VX_TYPE_UINT8)
        || (outputFormat != VX_TYPE_FLOAT16 && outputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_INT8 && outputFormat != VX_TYPE_INT16 && outputFormat != VX_TYPE_UINT8))
    {
        vxError("input or outputs format is not support");
        status = VX_ERROR_NOT_SUPPORTED;
        return status;
    }

    for(j = 0; j < input_height; ++j)
    {
        for(i = 0; i < input_width; ++i)
        {
            vx_float32 sum = 0.0f;
            for (c = 0; c < input_depth; c++)
            {
                vx_int32 in_index = i + input_width * (j + input_height * c);
                data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), in_index, (vx_uint8_ptr)inputBase, inputFixedPoint, TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                sum += data * data;
            }
            rsqrt = gcoMATH_ReciprocalSquareRoot(gcoMATH_MAX(sum,epsilon));
            for (c = 0; c < input_depth; c++)
            {
                vx_int32 index = i + input_width * (j + input_height * c);
                data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), index, (vx_uint8_ptr)inputBase, inputFixedPoint, TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                data *= rsqrt;
                vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), index, data, (vx_uint8_ptr)outputBase, outputFixedPoint, TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
            }
        }
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNL2NormalizeLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNL2NormalizeLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNL2NormalizeLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNL2NormalizeLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor  input                      = (vx_tensor)parameters[0];
    vx_tensor  output                     = (vx_tensor)parameters[1];
    vx_bool    enableShader;
    vx_type_e  inputFormat                = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e  outputFormat               = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_uint32  dims                       = TENSOR_VIEW_DIM_NUM(input);
    vx_uint32  width                      = TENSOR_VIEW_SIZE_INDEX(input, 0);
    vx_uint32  height                     = (dims > 1) ? TENSOR_VIEW_SIZE_INDEX(input, 1) : 1;
    vx_uint32  depth                      = (dims > 2) ? TENSOR_VIEW_SIZE_INDEX(input, 2) : 1;
    vx_uint32  batch                      = (dims > 3) ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;
    vx_uint32  sizes[4]                   = {0, 0, 0, 0};
    vx_tensor  src                        = NULL;
    vx_tensor  dst                        = NULL;

    vxnne_l2normalize_layer l2normalizeLayer = VX_NULL;
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_l2normalize_layer_s), (gctPOINTER*)&l2normalizeLayer);
    if (!l2normalizeLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    switch(dims)
    {
    case 1:
        depth   = TENSOR_VIEW_SIZE_INDEX(input, 0);
        batch   = 1;
        width   = 1;
        height  = 1;
        dims    = 3;
        break;
    case 2:
        depth   = TENSOR_VIEW_SIZE_INDEX(input, 0);
        batch   = TENSOR_VIEW_SIZE_INDEX(input, 1);
        width   = 1;
        height  = 1;
        dims    = 4;
        break;
    case 3:
        width   = TENSOR_VIEW_SIZE_INDEX(input, 0);
        height  = TENSOR_VIEW_SIZE_INDEX(input, 1);
        depth   = TENSOR_VIEW_SIZE_INDEX(input, 2);
        batch   = 1;
        dims    = 3;
        break;
    case 4:
        width   = TENSOR_VIEW_SIZE_INDEX(input, 0);
        height  = TENSOR_VIEW_SIZE_INDEX(input, 1);
        depth   = TENSOR_VIEW_SIZE_INDEX(input, 2);
        batch   = TENSOR_VIEW_SIZE_INDEX(input, 3);
        break;
    default:
        vxError("Input tensor error dimension[%u]\n", dims);
    }

    sizes[0] = width;
    sizes[1] = height;
    sizes[2] = depth;
    sizes[3] = batch;

    src      = vxoTensor_ReshapeTensor(input, (vx_int32*)sizes, dims);
    dst      = vxoTensor_ReshapeTensor(output, (vx_int32*)sizes, dims);

    gcoOS_ZeroMemory(l2normalizeLayer, sizeof(vxnne_l2normalize_layer_s));

    vxnneLayer_Initialize(&l2normalizeLayer->base,
                          "L2NormalizeLayer",
                          node,
                          vxmOPERATION_COUNT(l2normalizeLayer),
                          l2normalizeLayer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enableShader = (vx_bool)((inputFormat != VX_TYPE_FLOAT32) && (outputFormat != VX_TYPE_FLOAT32));
        enableShader  = enableShader && (width * height < IMG_MAX_WIDTH);
    }
    else
        enableShader = vx_true_e;

    if (enableShader && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_uint32 dim                  = 4;
        vx_uint32 tmp_sizes[4]         = {1, 1, 1, 1};
        vx_tensor_create_params_t      tensor_create_params;
        vx_tensor sumTmp = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            tmp_sizes[0] = gcmALIGN(width * height, 16);
            tmp_sizes[3] = batch;
        }
        else
        {
            tmp_sizes[0] = width;
            tmp_sizes[1] = height;
            tmp_sizes[3] = batch;
        }

        gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
        tensor_create_params.num_of_dims = dim;
        tensor_create_params.sizes = tmp_sizes;
        tensor_create_params.data_format = VX_TYPE_FLOAT32;
        tensor_create_params.quant_format = 0;

        sumTmp = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
        if (sumTmp == VX_NULL)
        {
            vxError("vxoTensor_CreateTensor fail at function %s, line %d", __FUNCTION__, __LINE__);
            status = VX_ERROR_NO_MEMORY;
            goto exit;
        }

        //node 1
        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneL2NormSumSqrtShaderExecutable(node->base.context, VXNNE_KERNEL_L2NORM_SUMSQRT, &node->kernelAttributes.borderMode, src, sumTmp);
        }
        else
        {
            shaderExecutable = vxnneGPUL2NormSumSqrtShaderExecutable(node->base.context, VXNNE_KERNEL_L2NORM_SUMSQRT, &node->kernelAttributes.borderMode, src, sumTmp);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }
        status = vxnneShaderOperation_Initialize(&l2normalizeLayer->l2normalize_SumSqrt_sh_operation,
                                        &l2normalizeLayer->base,
                                        VXNNE_OPERATOR_L2NORMALIZE_SUMSQRT,
                                        batch,
                                        shaderExecutable);
        if (status != VX_SUCCESS)
            goto exit;

        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_SumSqrt_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_SumSqrt_sh_operation.base, (vx_reference)sumTmp, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        //node 2
        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneL2NormSumScaleShaderExecutable(node->base.context, VXNNE_KERNEL_L2NORM_SUMSCALE, &node->kernelAttributes.borderMode, src, sumTmp, dst);
        }
        else
        {
            shaderExecutable = vxnneGPUL2NormSumScaleShaderExecutable(node->base.context, VXNNE_KERNEL_L2NORM_SUMSCALE, &node->kernelAttributes.borderMode, src, sumTmp, dst);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }
        status = vxnneShaderOperation_Initialize(&l2normalizeLayer->l2normalize_sumScale_sh_operation,
                                        &l2normalizeLayer->base,
                                        VXNNE_OPERATOR_L2NORMALIZE_SUMSCALE,
                                        batch,
                                        shaderExecutable);
        if (status != VX_SUCCESS)
            goto exit;

        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_sumScale_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_sumScale_sh_operation.base, (vx_reference)sumTmp, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_sumScale_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &l2normalizeLayer->base,
            &l2normalizeLayer->l2normalize_SumSqrt_sh_operation.base,
            0);
        vxnneLayer_SetOperation(
            &l2normalizeLayer->base,
            &l2normalizeLayer->l2normalize_sumScale_sh_operation.base,
            1);

        l2normalizeLayer->base.temp_tensors[0] = sumTmp;
        l2normalizeLayer->base.temp_tensors[1] = src;
        l2normalizeLayer->base.temp_tensors[2] = dst;
        l2normalizeLayer->base.num_temp_tensors = 3;

        node->layer = &l2normalizeLayer->base;
    }
    else
    {
        vxnneOperation_Initialize(&l2normalizeLayer->l2normalize_sw_operation.base,
                                  &l2normalizeLayer->base,
                                  VXNNE_OPERATION_TARGET_SW,
                                  VXNNE_OPERATOR_L2NORMALIZE,
                                  vxnneExecuteSWL2Normalize,
                                  VX_NULL,
                                  batch,
                                  0);

        vxnneLayer_SetOperation(
            &l2normalizeLayer->base,
            &l2normalizeLayer->l2normalize_sw_operation.base,
            0);

        l2normalizeLayer->l2normalize_sw_operation.inputs           = input;
        l2normalizeLayer->l2normalize_sw_operation.outputs          = output;

        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&l2normalizeLayer->l2normalize_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        node->layer = &l2normalizeLayer->base;
    }

    return status;

exit:
    if (src) vxoTensor_ReleaseTensor(&src);
    if (dst) vxoTensor_ReleaseTensor(&dst);
    if (l2normalizeLayer)
        gcoOS_Free(NULL, (gctPOINTER)l2normalizeLayer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNL2NormalizeLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

vx_status vxnneExecutionLayer_Create(vx_graph graph, vxnne_execution_layer* executionLayer)
{
    vxnne_execution_layer layer = VX_NULL;

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_execution_layer_s), (gctPOINTER*)&layer);
    if (!layer)
    {
        return VX_ERROR_NO_MEMORY;
    }
    gcoOS_ZeroMemory(layer, sizeof(vxnne_execution_layer_s));

    vxnneLayer_Initialize(&layer->base, "execution_layer", VX_NULL, 0, VX_NULL, vxnneExecutionLayer_Deinitialize);

    layer->base.execute = vxnneExecutionLayer_Execute;
    layer->graph = graph;

    *executionLayer = layer;

    return VX_SUCCESS;
}

vx_status vxnneExecutionLayer_GenerateCommands(vx_context context, vxnne_layer layer)
{
    vx_uint32               i;
    vx_status               status = VX_SUCCESS;
    vxnne_execution_layer   executionLayer = (vxnne_execution_layer)layer;

    for (i = 0; i < executionLayer->opIndicesNum; i++)
    {
        status = vxnneOperationCommand_GenerateCommands(context, &executionLayer->opIndices[i]);
        if (status != VX_SUCCESS)
        {
            goto OnError;
        }
    }

        /*used by swap handel*/
    vxo_insertHandel(executionLayer);

    /* nn and tp have saved done, this is for saving SH reset current offset */
    if (executionLayer->graph->binarySave)
    {
        executionLayer->graph->binarySave->lastOperation0ffset = executionLayer->graph->binarySave->currOperationOffset;
        executionLayer->graph->binarySave->currOperationOffset = 0;
    }

OnError:
    return status;
}

VX_PRIVATE_API vx_status vxnneOperation_ExecuteYUVScalerCommand(vx_node node, vxnne_operation operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_yuv2rgb_scale_operation scaleOp = (vxnne_yuv2rgb_scale_operation)operation;
    vx_uint32 imageInputWidth, imageInputHeight, i, splitCount;
    vx_uint32 outputWidth = TENSOR_VIEW_SIZE_INDEX(scaleOp->outputs, 0); /* x-axis*/
    vx_uint32 outputHeight = TENSOR_VIEW_SIZE_INDEX(scaleOp->outputs, 1); /* y-axis*/
    vx_rectangle_t rect;
    vx_uint32 inputStarts[gcdMAX_3DGPU_COUNT], inputSizes[gcdMAX_3DGPU_COUNT];
    vx_uint32 outputStarts[gcdMAX_3DGPU_COUNT], outputSizes[gcdMAX_3DGPU_COUNT];
    vx_uint16 inputInitErrors[gcdMAX_3DGPU_COUNT], inputInitIntErrors[gcdMAX_3DGPU_COUNT];
    vxnne_yuv2rgb_scale_operation_s SCOperation;

    vxmASSERT(VXNNE_OPERATOR_YUV2RGB_SCALE == operation->operatorType);

    if (node->kernel->signature.directionTable[0] == VX_INPUT &&
        node->kernel->signature.dataTypeTable[0] == VX_TYPE_IMAGE)
    {
        /* Get input image from node directly */
        scaleOp->inputs = (vx_image)node->paramTable[0];
    }

    if (node->kernel->signature.directionTable[1] == VX_INPUT &&
        node->kernel->signature.dataTypeTable[1] == VX_TYPE_ARRAY)
    {
        /* Get input rectangle from node directly */
        vx_array rects = (vx_array)node->paramTable[1];
        rect.start_x = *((vx_uint32_ptr)rects->memory.logicals[0] + 0);
        rect.start_y = *((vx_uint32_ptr)rects->memory.logicals[0] + 1);
        rect.end_x   = *((vx_uint32_ptr)rects->memory.logicals[0] + 2);
        rect.end_y   = *((vx_uint32_ptr)rects->memory.logicals[0] + 3);
    }
    else
    {
        rect = scaleOp->rect;
    }
    if (!rect.end_x || !rect.end_y || rect.end_x < rect.start_x || rect.end_y < rect.start_y)
    {
        vx_image image = scaleOp->inputs;
        if (image->region.start_x > image->region.end_x)
        {
            rect.start_x = 0;
            rect.end_x = image->memory.dims[0][VX_DIM_X];
        }
        else
        {
            rect.start_x = image->region.start_x;
            rect.end_x = image->region.end_x;
        }
        if (image->region.start_y > image->region.end_y)
        {
            rect.start_y = 0;
            rect.end_y = image->memory.dims[0][VX_DIM_Y];
        }
        else
        {
            rect.start_y = image->region.start_y;
            rect.end_y = image->region.end_y;
        }
    }
    scaleOp->rect = rect;

    imageInputWidth = rect.end_x - rect.start_x;
    imageInputHeight  = rect.end_y - rect.start_y;
    scaleOp->x_scale = (imageInputWidth << 15) / outputWidth;
    scaleOp->y_scale = (imageInputHeight << 15) / outputHeight;

    memcpy(&SCOperation, scaleOp, sizeof(vxnne_yuv2rgb_scale_operation_s));

    /* for X */
    splitCount = 1;
    vxmONERROR(vxnneComputeYUV2RGBInputParameter(outputWidth, scaleOp->x_scale, rect.start_x,
                                                 &splitCount,
                                                 outputStarts, outputSizes,
                                                 inputStarts, inputSizes,
                                                 inputInitErrors, inputInitIntErrors));

    SCOperation.rect.start_x = inputStarts[0];
    SCOperation.rect.end_x = SCOperation.rect.start_x + inputSizes[0];
    SCOperation.x_init_error = inputInitErrors[0];
    SCOperation.x_init_int_error = inputInitIntErrors[0];

    /* for split Y */
    if (node->base.context->options.enableMultiVIPCombined)
    {
        gcmONERROR(gcoVX_GetHWConfigGpuCount(&splitCount));
    }
    else
    {
        splitCount = 1;
    }
    vxmONERROR(vxnneComputeYUV2RGBInputParameter(outputHeight, scaleOp->y_scale, rect.start_y,
                                                 &splitCount,
                                                 outputStarts, outputSizes,
                                                 inputStarts, inputSizes,
                                                 inputInitErrors, inputInitIntErrors));

    for (i = 0; i < splitCount; i++)
    {
        vx_uint32 outputDim = 0;
        vx_uint32 outputSizeStart[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
        vx_uint32 outputSizeEnd[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};
        vx_tensor_view outputView = 0;
        vx_tensor outputTensor = VX_NULL;

        vxmONERROR(vxQueryTensor(scaleOp->outputs, VX_TENSOR_DIMS, outputSizeEnd, sizeof(outputSizeEnd)));
        vxmONERROR(vxQueryTensor(scaleOp->outputs, VX_TENSOR_NUMBER_OF_DIMS, &outputDim, sizeof(outputDim)));

        /* split output y-axis */
        outputSizeStart[1] = outputStarts[i];
        outputSizeEnd[1] = outputStarts[i] + outputSizes[i];

        outputView = vxCreateTensorView(node->base.context, outputSizeStart, outputSizeEnd, (vx_uint8)outputDim);
        outputTensor  = vxoTensor_CreateTensorFromView(scaleOp->outputs, outputView);
        if (outputView != VX_NULL) vxReleaseTensorView(&outputView);

        SCOperation.base.references[VX_MULTIVIP_OUTPUT_TENSOR_REFERENCE] = (vx_reference)outputTensor;
        SCOperation.outputs = outputTensor;
        SCOperation.base.outputs[0] = (vx_reference)outputTensor;
        SCOperation.rect.start_y = inputStarts[i];
        SCOperation.rect.end_y = SCOperation.rect.start_y + inputSizes[i];
        SCOperation.y_init_error = inputInitErrors[i];
        SCOperation.y_init_int_error = inputInitIntErrors[i];

        SCOperation.base.gpuId = i;
        SCOperation.base.mGpuSync = i == splitCount - 1 ? vx_true_e : vx_false_e;

        operation->execute((vxnne_operation)&SCOperation);
    }

OnError:
    return status;
}

vx_status vxnneExecutionLayer_Execute(vxnne_layer layer)
{
    vx_uint32               i = 0, j = 0;
    vx_status               status = VX_SUCCESS;
    vxnne_execution_layer   executionLayer = (vxnne_execution_layer)layer;
    vxnne_operation         operation;
    vxnne_operation_target_e operationTarget;
    vx_graph graph = executionLayer->graph;

    vxnneMultiChannel_GetCurrentChannel(&operationTarget);

    for (i = 0; i < executionLayer->opIndicesNum; i++)
    {
        gctUINT64 operatorStart = 0;
        vx_uint32 opertorExcuteTime = 0;
        vx_bool wait = vx_false_e;

        operation = executionLayer->operations[executionLayer->opIndices[i].operationID];

        vxnneMultiChannel_SetCurrentChannel(operation->target);

        vxnneMultiChannel_ApplySyncMode(executionLayer->opIndices[i].waitMode, executionLayer->opIndices[i].semaWaitHandle);

        if (vxoContext_IsFeatureAvailable(executionLayer->graph->base.context, VX_NN_TP_PARALLEL))
        {
            gctUINT32 actualSize = 0;
            vx_status status = VX_SUCCESS;
            if (graph->binarySave)
            {
                graph->binarySave->waitCommandsSize = 0;
                if ((operation->engineSync.waitCnt > 0) && (i > 0))
                {
                    status = gcfVX_CaptureState(graph->binarySave->waitCommands,
                                                VX_MAX_WAIT_STATE_SIZE,
                                                &actualSize,
                                                gcvTRUE,
                                                gcvFALSE);
                    if (status != VX_SUCCESS)
                    {
                        vxError("failed to capture wait commands\n");
                        vxmASSERT(0);
                    }
                }
            }

            /* th/nn/sh parallel wait event ID */
            if (i > 0)
            {
                for (j = 0; j < operation->engineSync.waitCnt; j++)
                {
                    gcoVX_WaitNNEvent(operation->engineSync.waitId[j]);
                    wait = vx_true_e;
                }
            }

            if ((graph->binarySave) && (operation->engineSync.waitCnt > 0) && (i > 0))
            {
                status = gcfVX_CaptureState(gcvNULL, 0, &actualSize, gcvFALSE, gcvFALSE);
                if (status != VX_SUCCESS)
                {
                    vxError("failed to capture wait commands end\n");
                    vxmASSERT(0);
                }
                graph->binarySave->waitCommandsSize = (vx_uint32)actualSize;
            }

            if (operation->target == VXNNE_OPERATION_TARGET_NN)
            {
                vxInfo("NN %2d eventId: %2d waitId: ", operation->absoluteOperationID, operation->engineSync.eventId[0]);
            }
            else if (operation->target == VXNNE_OPERATION_TARGET_TP)
            {
                vxInfo("TP %2d eventId: %2d waitId: ", operation->absoluteOperationID, operation->engineSync.eventId[operation->engineSync.eventCnt-1]);
            }
            else if (operation->target == VXNNE_OPERATION_TARGET_SH)
            {
                vxInfo("SH %2d eventId:  0 waitId: ", operation->absoluteOperationID);
            }

            for (j = 0; j < operation->engineSync.waitCnt; j++)
            {
                vxInfo("%2d ", operation->engineSync.waitId[j]);
            }
            if (!wait)
            {
                vxInfo("Skip");
            }
            vxInfo("\n");
        }

        vxnneBatch_SetCurrentBatchArray(operation, executionLayer->opIndices[i].batchID);

        if (executionLayer->graph->base.context->options.enableCNNPerf)
        {
            operatorStart = gcfVX_PerfStart((vx_reference)executionLayer->graph);
        }

#if gcdFRAMEINFO_STATISTIC
        {
            gctUINT32 drawID = (operation->id & 0x3FF)                          /* 10 bit operation ID */
                             | ((executionLayer->graph->graphID & 0x3F) << 10)  /* 6  bit graph ID */
                             | ((i & 0xFFFF) << 16);                            /* 16 bit operationIndex */
            gcoHAL_FrameInfoOps(gcvNULL, gcvFRAMEINFO_COMPUTE_NUM, gcvFRAMEINFO_OP_SET, &drawID);
        }
#endif
        if (executionLayer->opIndices[i].commandBuffer.logical)
        {
            if (executionLayer->opIndices[i].dump)
            {
                executionLayer->opIndices[i].dump(&executionLayer->opIndices[i], operation, VXNNE_DUMP_PRE_EXECUTION);
            }

            status = vxnneOperation_ExecuteCommands(operation, &executionLayer->opIndices[i].commandBuffer);

        }
        else
        {
            if (executionLayer->opIndices[i].dump)
            {
                executionLayer->opIndices[i].dump(&executionLayer->opIndices[i], operation, VXNNE_DUMP_PRE_EXECUTION);
            }

            if (operation->target == VXNNE_OPERATION_TARGET_SC)
            {
               status = vxnneOperation_ExecuteYUVScalerCommand(operation->layer->node, operation);
            }
            else
            {
                operation->execute(operation);
            }
        }

        if (vxoContext_IsFeatureAvailable(executionLayer->graph->base.context, VX_NN_TP_PARALLEL) &&
            operation->target == VXNNE_OPERATION_TARGET_SH && operation->engineSync.eventId[0] != 0)
        {
            gcoVX_FlushCache(vx_false_e, vx_false_e, vx_false_e, vx_false_e, vx_true_e, vx_false_e);
        }

        if (executionLayer->opIndices[i].dump)
        {
            executionLayer->opIndices[i].dump(&executionLayer->opIndices[i], operation, VXNNE_DUMP_POST_EXECUTION);
        }

        if (operation->layer->node->base.context->options.enableNNLayerDump)
        {
            gcfVX_Flush(gcvTRUE);
            vxnneOperation_NodeDump(&executionLayer->opIndices[i]);
        }

        vxnneMultiChannel_ApplySyncMode(executionLayer->opIndices[i].wakeMode, executionLayer->opIndices[i].semaWakeHandle);
        if (executionLayer->graph->base.context->options.enableCNNPerf)
        {
            vxInfo("layer id: %d layer name:%s operation[%d]:%s target:%s.\n",
                operation->layer->node->id,
                operation->layer->name,
                operation->id,
                vxnneGetOperatorTypeName(operation->operatorType),
                vxnneGetOperatorTargetName(operation->target));

            gcfVX_Flush(gcvTRUE);

            opertorExcuteTime = gcfVX_PerfEnd((vx_reference)executionLayer->graph, operatorStart);

            vxInfo("execution time:%10d us\n", opertorExcuteTime);

            /* Calculate non-zero ratio, only for TP target */
            if (operation->layer->node->base.context->options.enableNNLayerDump
               && (operation->target == VXNNE_OPERATION_TARGET_TP)
               && (operation->operatorType == VXNNE_OPERATOR_FULLYCONNECTED))
            {
                /* for non-zero ratio calculate */
                vx_tensor  calNonZeroRatioinputs = NULL;
                vx_uint32 inputSize = 0, index = 0,inputZeroCount = 0;
                vx_float32 inputNonZeroRatio = 0.0;
                vx_uint8_ptr inputAddress = NULL;

                calNonZeroRatioinputs = (vx_tensor)operation->inputs[0];
                vxoTensor_GetTensorSize(calNonZeroRatioinputs, &inputSize);
                /*vxInfo("tensor input size is:%d, zero point is %d.\n", inputSize,calNonZeroRatioinputs->zeroPoint);*/

                vxoTensor_GetTensorBatchArrayViewMemory(calNonZeroRatioinputs,operation->currBatchIndex,(gctPOINTER *)&inputAddress,NULL);
                /* find out zero count */
                if(inputSize != 0)
                {
                    /* calculate zero count */
                    for(index = 0; index< inputSize; index++)
                    {
                        if(inputAddress[index] == calNonZeroRatioinputs->zeroPoint)
                            inputZeroCount++;
                    }
                    /*vxInfo("tensor zero count is:%d.\n", inputZeroCount);*/

                    inputNonZeroRatio = 1.0f * (inputSize - inputZeroCount) / inputSize;
                    vxInfo("==layer_id: %d abs_op_id: %d imageNonZeroRatio: %.07f\n", operation->layer->node->id, operation->absoluteOperationID, inputNonZeroRatio);

                    /* clear input Zero count */
                    inputZeroCount = 0;
                }
            }
#if VIVANTE_PROFILER
            vxoProfiler_End((vx_reference)executionLayer->graph);
#endif
        }

 #if VXM_FORCE_PER_OPERATION_IDLE
        else
        {
            gcoVX_Flush(gcvTRUE);

        }
#endif
    }

    if (graph->binarySave)
    {
        gcfVX_CaptureState(graph->binarySave->endCommands,
                            VX_MAX_INITIALIZE_COMMAND_SIZE,
                            &graph->binarySave->endCommandsSize,
                            gcvTRUE,
                            gcvFALSE);
    }

    vxnneMultiChannel_SetCurrentChannel(operationTarget);

    /* Dump all operation info if VIV_VX_OPS_DUMP_PATH enabled */
    vxnneOpDebug_DumpOperatoinInfos(layer);

    return status;
}

vx_status vxnneExecutionLayer_Deinitialize(vxnne_layer layer)
{
    vx_uint32 i;
    vxnne_execution_layer   executionLayer = (vxnne_execution_layer)layer;

    if (executionLayer->opIndices)
    {
        /* free command commandbuffer */
        for (i = 0; i < executionLayer->opIndicesNum; i++)
        {
            if (executionLayer->opIndices[i].commandBuffer.node)
            {
                gcoVX_FreeMemory(executionLayer->opIndices[i].commandBuffer.node);
                executionLayer->opIndices[i].commandBuffer.node = gcvNULL;
            }

            if (executionLayer->opIndices[i].commandBuffer.eventID)
            {
                vxFree(executionLayer->opIndices[i].commandBuffer.eventID);
                executionLayer->opIndices[i].commandBuffer.eventID = gcvNULL;
            }
        }

        gcoOS_Free(gcvNULL, executionLayer->opIndices);
        executionLayer->opIndicesNum    = 0;
        executionLayer->opIndices       = gcvNULL;
    }

    if (executionLayer->operations)
    {
        /* free opertions */
        gcoOS_Free(gcvNULL, executionLayer->operations);
        executionLayer->base.num_operations = 0;
        executionLayer->operations   = gcvNULL;
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Reorg2
 ***************************************************************************************************************************/
vx_status vxnneReorg2_Space2Depth(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;

    vxnne_reorg_operation reorgOperation   = (vxnne_reorg_operation)operation;

    vx_tensor  inputs           = (vx_tensor)reorgOperation->inputs;
    vx_tensor  strides          = (vx_tensor)reorgOperation->stride;
    vx_tensor  outputs          = (vx_tensor)reorgOperation->outputs;

    vx_uint32  block_size       = (vx_uint32)VX_GET_DATA_FROM_TENSOR(strides, 0);

    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_uint8_ptr inputBase;
    vx_uint8_ptr outputBase;

    const vx_uint32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    const vx_uint32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    const vx_uint32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    const vx_uint32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */

    const vx_uint32 output_width = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
    const vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    const vx_uint32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
    const vx_uint32 output_batch = TENSOR_SIZE_INDEX(outputs, 3);  /* N */

    const vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(outputs));

    vx_uint32 batch = 0, in_h = 0, in_w = 0;
    vx_float32 data = .0f;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);

    gcmASSERT(input_width == output_width * block_size);
    gcmASSERT(input_height == output_height * block_size);
    gcmASSERT(output_depth == input_depth * block_size * block_size);
    if (output_batch != input_batch)
    {
        gcmASSERT(0);
    }

    outputBase = TENSOR_LOGICAL_ADDR(outputs);
    inputBase = TENSOR_LOGICAL_ADDR(inputs);

    /**************************************************************************************************
     *
     *                 W H C N                                                            W H C N
     *                 4 4 2 1                   =>                                       2 2 8 1
     *   _______________     _______________
     *  | 10| 11| 12| 13|   | 20| 21| 22| 23|
     *  |___|___|___|___|   |___|___|___|___|          _______   _______   _______   _______   _______   _______   _______   _______
     *  | 14| 15| 16| 17|   | 24| 25| 26| 27|         | 10| 12| | 20| 22| | 11| 13| | 21| 23| | 14| 16| | 24| 26| | 15| 17| | 25| 27|
     *  |___|___|___|___|   |___|___|___|___|    =>   |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___|
     *  | 18| 19|110|111|   | 28| 29|210|211|         | 18|110| | 28|210| | 19|111| | 29|211| |112|114| |212|214| |113|115| |213|215|
     *  |___|___|___|___|   |___|___|___|___|         |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___|
     *  |112|113|114|115|   |212|213|214|215|
     *  |___|___|___|___|   |___|___|___|___|
     *
     **************************************************************************************************/

    for (batch = 0; batch < input_batch; ++ batch)
    {
        vx_uint32 output_batch_index = batch * output_height * output_width * output_depth;
        vx_uint32 input_batch_index = batch * input_height * input_width * input_depth;
        vx_uint32 in_d = 0;

        for (in_d = 0; in_d < input_depth; in_d ++)
        {
            for (in_h = 0; in_h < input_height; ++ in_h)
            {
                for (in_w = 0; in_w < input_width; in_w ++)
                {
                    vx_int32 out_w = in_w / block_size;
                    vx_int32 out_h = in_h / block_size;
                    vx_int32 out_d = (in_w  % block_size) * input_depth + (in_h % block_size) * block_size * input_depth + in_d;

                    vx_int32 in_index = in_w + in_h * input_width +in_d * input_height * input_width + input_batch_index;
                    vx_int32 out_index = out_w + out_h * output_width +  out_d * output_width * output_height + output_batch_index;

                    if (item_size == vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs)))
                    {
                        memcpy(outputBase + out_index * item_size, inputBase + in_index * item_size, item_size);
                    }
                    else
                    {
                        data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), in_index, inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));

                        status |= vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), out_index, data, outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                    }
                }
            }
        }
    }

    return status;
}

vx_status vxnneReorg2_Depth2Space(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;

    vxnne_reorg_operation reorgOperation   = (vxnne_reorg_operation)operation;

    vx_tensor  inputs           = (vx_tensor)reorgOperation->inputs;
    vx_tensor  strides          = (vx_tensor)reorgOperation->stride;
    vx_tensor  outputs          = (vx_tensor)reorgOperation->outputs;

    vx_uint32  block_size       = (vx_uint32)VX_GET_DATA_FROM_TENSOR(strides, 0);

    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_uint8_ptr inputBase;
    vx_uint8_ptr outputBase;

    const vx_uint32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    const vx_uint32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    const vx_uint32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    const vx_uint32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */

    const vx_uint32 output_width = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
    const vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    const vx_uint32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
    const vx_uint32 output_batch = TENSOR_SIZE_INDEX(outputs, 3);  /* N */

    const vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(outputs));

    vx_uint32 batch = 0, out_h = 0, out_w = 0;
    vx_float32 data = .0f;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);

    gcmASSERT(input_width * block_size == output_width);
    gcmASSERT(input_height * block_size == output_height);
    gcmASSERT(output_depth * block_size * block_size == input_depth);
    if (output_batch != input_batch)
    {
        gcmASSERT(0);
    }

    outputBase = TENSOR_LOGICAL_ADDR(outputs);
    inputBase = TENSOR_LOGICAL_ADDR(inputs);

    /**************************************************************************************************
     *                 output                                                              input
     *                 W H C N                                                            W H C N
     *                 4 4 2 1                   <=                                       2 2 8 1
     *   _______________     _______________
     *  | 10| 11| 12| 13|   | 20| 21| 22| 23|
     *  |___|___|___|___|   |___|___|___|___|          _______   _______   _______   _______   _______   _______   _______   _______
     *  | 14| 15| 16| 17|   | 24| 25| 26| 27|         | 10| 12| | 20| 22| | 11| 13| | 21| 23| | 14| 16| | 24| 26| | 15| 17| | 25| 27|
     *  |___|___|___|___|   |___|___|___|___|    <=   |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___|
     *  | 18| 19|110|111|   | 28| 29|210|211|         | 18|110| | 28|210| | 19|111| | 29|211| |112|114| |212|214| |113|115| |213|215|
     *  |___|___|___|___|   |___|___|___|___|         |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___| |___|___|
     *  |112|113|114|115|   |212|213|214|215|
     *  |___|___|___|___|   |___|___|___|___|
     *

     **************************************************************************************************/

    for (batch = 0; batch < output_batch; ++ batch)
    {
        vx_uint32 output_batch_index = batch * output_height * output_width * output_depth;
        vx_uint32 input_batch_index = batch * input_height * input_width * input_depth;
        vx_uint32 out_d = 0;

        for (out_d = 0; out_d < output_depth; out_d ++)
        {
            for (out_h = 0; out_h < output_height; ++ out_h)
            {
                for (out_w = 0; out_w < output_width; out_w ++)
                {
                    vx_int32 in_w = out_w / block_size;
                    vx_int32 in_h = out_h / block_size;
                    vx_int32 in_d = ((out_w  % block_size) + (out_h % block_size) * block_size) * output_depth + out_d;

                    vx_int32 in_index = in_w + in_h * input_width +  in_d * input_width * input_height + input_batch_index;
                    vx_int32 out_index = out_w + out_h * output_width + out_d * output_height * output_width + output_batch_index;

                    if (item_size == vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs)))
                    {
                        memcpy(outputBase + out_index * item_size, inputBase + in_index * item_size, item_size);
                    }
                    else
                    {
                        data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), in_index, inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));

                        status |= vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), out_index, data, outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                    }
                }
            }
        }
    }

    return status;
}

vx_status vxnneReorg2_Batch2SpaceND(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_reorg_operation reorgOperation = (vxnne_reorg_operation)operation;

    vx_tensor  inputs = (vx_tensor)reorgOperation->inputs;
    vx_tensor  strides = (vx_tensor)reorgOperation->stride;
    vx_tensor  outputs = (vx_tensor)reorgOperation->outputs;

    vx_type_e  inputFormat = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_uint8_ptr inputBase = VX_NULL, outputBase = VX_NULL;
    vx_int32_ptr block_size = VX_NULL;

    const vx_int32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    const vx_int32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    const vx_int32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    const vx_int32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */

    const vx_int32 output_width = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
    const vx_int32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    const vx_int32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
    const vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(outputs));

    vx_int32 in_h = 0, in_w = 0, in_d = 0, in_b = 0;
    vx_float32 data = .0f;
    vx_int32 block_w = 0, block_h = 0;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(strides, (gctPOINTER*)&block_size, VX_NULL);

    block_w = block_size[0];
    block_h = block_size[1];

    gcmASSERT(output_width == input_width * block_w);
    gcmASSERT(output_height == input_height * block_h);
    /*gcmASSERT(input_batch == output_batch * block_w * block_h);*/
    if (output_depth != input_depth)
    {
        gcmASSERT(0);
    }

    /**********************************************************************************************************
    *             intput          block_size      pad(t, b, l, r)    output
    *             W H C N         (w x h: 2x2)     (0, 0, 0, 0)      W H C N
    *             2 2 2 1                  =>                        1 1 2 4
    *
    *
    *                                                                 ___   ___
    *                                                                |1.4| |2.3|
    *                                                                |___| |___|
    *        _______   _______                                        ___   ___
    *       |1.4|3.2| |2.3|4.1|                                      |3.2| |4.1|
    *       |___|___| |___|___|             <=                       |___| |___|
    *       |5.4|7.2| |7.2|8.1|                                       ___   ___
    *       |___|___| |___|___|                                      |5.4| |6.3|
    *                                                                |___| |___|
    *                                                                 ___   ___
    *                                                                |7.2| |8.1|
    *                                                                |___| |___|
    *
    *
    *           intput         block_size     pad(t, b, l, r)       output
    *           C W H N       (w x h: 2x2)    (0, 0, 0, 0)          C W H N
    *           2 2 2 1                 =>                          2 1 1 4
    *
    *                                                                _______
    *                                                               |1.4 2.3|
    *                                                               |_______|
    *           _______________                                     |3.2 4.1|
    *          |1.4 2.3|3.2 4.1|                                    |_______|
    *          |_______|_______|             <=                     |5.4 6.3|
    *          |5.4 6.3|7.2 8.1|                                    |_______|
    *          |_______|_______|                                    |7.2 8.1|
    *                                                               |_______|
    *
    *
    **********************************************************************************************************/

    for (in_b = 0; in_b < input_batch; in_b++)
    {
        vx_int32 input_batch_index = in_b * input_height * input_width * input_depth;

        for (in_d = 0; in_d < input_depth; in_d++)
        {
            for (in_h = 0; in_h < input_height; ++in_h)
            {
                for (in_w = 0; in_w < input_width; in_w++)
                {
                    vx_int32 out_w = in_w * block_w + (in_b % block_w);
                    vx_int32 out_h = in_h * block_h + (in_b / block_h);
                    vx_int32 out_b = in_b / (output_width * output_height);
                    vx_int32 output_batch_index = out_b * output_height * output_width * output_depth;

                    vx_int32 out_index = out_w + out_h * output_width + in_d * output_height * output_width + output_batch_index;

                    if (in_w < 0 || in_w >= input_width || in_h < 0 || in_h >= input_height)
                    {
                        memset(outputBase + out_index * item_size, 0, item_size);
                    }
                    else
                    {
                        vx_int32 in_index = in_w + in_h * input_width + in_d * input_width * input_height + input_batch_index;

                        if (item_size == vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs)))
                        {
                            memcpy(outputBase + out_index * item_size, inputBase + in_index * item_size, item_size);
                        }
                        else
                        {
                            data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), in_index, inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));

                            status |= vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), out_index, data, outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                        }
                    }

                }
            }
        }
    }



    return status;
}

vx_status vxnneReorg2_Space2BatchND(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_reorg_operation reorgOperation = (vxnne_reorg_operation)operation;

    vx_tensor  inputs  = (vx_tensor)reorgOperation->inputs;
    vx_tensor  strides = (vx_tensor)reorgOperation->stride;
    vx_tensor  pad     = (vx_tensor)reorgOperation->pad;
    vx_tensor  outputs = (vx_tensor)reorgOperation->outputs;

    vx_type_e  inputFormat = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_uint8_ptr inputBase = VX_NULL, outputBase = VX_NULL;
    vx_int32_ptr block_size = VX_NULL, pads = VX_NULL;

    const vx_int32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    const vx_int32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    const vx_int32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    const vx_int32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */

    const vx_int32 output_width = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
    const vx_int32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    const vx_int32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
    const vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(outputs));

    vx_int32 in_h = 0, in_w = 0, in_d = 0, in_b = 0;
    vx_float32 data = .0f;
    vx_int32 pad_t = 0, pad_b = 0, pad_l = 0, pad_r = 0;
    vx_int32 block_w = 0, block_h = 0;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(strides, (gctPOINTER*)&block_size, VX_NULL);
    vxoTensor_GetTensorViewMemory(pad, (gctPOINTER*)&pads, VX_NULL);

    block_w = block_size[0];
    block_h = block_size[1];

    /* fetch pad height info from pad[1] and weight info from pad[0] */
    pad_l = pads[0];
    pad_r = pads[1];
    pad_t = pads[2];
    pad_b = pads[3];

    gcmASSERT(output_width * block_w == pad_l + input_width + pad_r);
    gcmASSERT(output_height * block_h == pad_t + input_height + pad_b);
    /*gcmASSERT(input_batch * block_w * block_h == output_batch);*/
    if (output_depth != input_depth)
    {
        gcmASSERT(0);
    }

    /******************************************************************************************************************************************************************************************************
    *                 intput               block_size         pad(t, b, l, r)            output       *            intput          block_size      pad(t, b, l, r)    output
    *                 W H C N              (w x h: 2x3)        (1, 1, 2, 4)              W H C N      *            W H C N         (w x h: 2x2)     (0, 0, 0, 0)      W H C N
    *                 2 4 1 1                       =>                                   4 2 1 6      *            2 2 2 1                  =>                        1 1 2 4
    *                                                                                                 *
    *                                                            _______________                      *
    *                                                           | 0 | 0 | 0 | 0 |                     *                                                                ___   ___
    *                                                           |___|___|___|___|                     *                                                               |1.4| |2.3|
    *                                                           | 0 | 5 | 0 | 0 |                     *                                                               |___| |___|
    *                                                           |___|___|___|___|                     *       _______   _______                                        ___   ___
    *        _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _                     _______________                      *      |1.4|3.2| |2.3|4.1|                                      |3.2| |4.1|
    *       | 0 | 0 |  0|  0| 0 | 0 | 0 | 0 |                   | 0 | 0 | 0 | 0 |                     *      |___|___| |___|___|             =>                       |___| |___|
    *       |_ _|_ _|___|___|___|___|___|___|                   |___|___|___|___|                     *      |5.4|7.2| |7.2|8.1|                                       ___   ___
    *       | 0 | 0 |  1|  2| 0 | 0 | 0 | 0 |                   | 0 | 6 | 0 | 0 |                     *      |___|___| |___|___|                                      |5.4| |6.3|
    *       |___|___|___|___|___|___|___|___|                   |___|___|___|___|                     *                                                               |___| |___|
    *       | 0 | 0 |  3|  4| 0 | 0 | 0 | 0 |                    _______________                      *                                                                ___   ___
    *       |___|___|___|___|___|___|___|___|       =>          | 0 | 1 | 0 | 0 |                     *                                                               |7.2| |8.1|
    *       | 0 | 0 |  5|  6| 0 | 0 | 0 | 0 |                   |___|___|___|___|                     *                                                               |___| |___|
    *       |___|___|___|___|___|___|___|___|                   | 0 | 7 | 0 | 0 |                     *
    *       | 0 | 0 |  7|  8| 0 | 0 | 0 | 0 |                   |___|___|___|___|                     *
    *       |___|___|___|___|___|___|___|___|                    _______________                      *          intput         block_size     pad(t, b, l, r)       output
    *       | 0 | 0 |  0|  0| 0 | 0 | 0 | 0 |                   | 0 | 2 | 0 | 0 |                     *          C W H N       (w x h: 2x2)    (0, 0, 0, 0)          C W H N
    *       |_ _|_ _|_ _|_ _|_ _|_ _|_ _|_ _|                   |___|___|___|___|                     *          2 2 2 1                 =>                          2 1 1 4
    *                                                           | 0 | 8 | 0 | 0 |                     *
    *                                                           |___|___|___|___|                     *                                                               _______
    *                                                            _______________                      *                                                              |1.4 2.3|
    *                                                           | 0 | 3 | 0 | 0 |                     *                                                              |_______|
    *                                                           |___|___|___|___|                     *          _______________                                     |3.2 4.1|
    *                                                           | 0 | 0 | 0 | 0 |                     *         |1.4 2.3|3.2 4.1|                                    |_______|
    *                                                           |___|___|___|___|                     *         |_______|_______|             =>                     |5.4 6.3|
    *                                                            _______________                      *         |5.4 6.3|7.2 8.1|                                    |_______|
    *                                                           | 0 | 4 | 0 | 0 |                     *         |_______|_______|                                    |7.2 8.1|
    *                                                           |___|___|___|___|                     *                                                              |_______|
    *                                                           | 0 | 0 | 0 | 0 |                     *
    *                                                           |___|___|___|___|                     *
    *                                                                                                 *
    ******************************************************************************************************************************************************************************************************/

    for (in_b = 0; in_b < input_batch; in_b++)
    {
        for (in_d = 0; in_d < input_depth; in_d++)
        {
            for (in_h = -pad_t; in_h < input_height + pad_b; ++in_h)
            {
                for (in_w = -pad_l; in_w < input_width + pad_r; in_w++)
                {
                    vx_int32 out_w = (in_w + pad_l) / block_w;
                    vx_int32 out_h = (in_h + pad_t) / block_h;
                    vx_int32 out_b = (in_w + pad_l) % block_w + ((in_h + pad_t) % block_h) * block_w;
                    vx_int32 output_batch_index = out_b * output_height * output_width * output_depth;

                    vx_int32 out_index = out_w + out_h * output_width + in_d * output_height * output_width + output_batch_index;

                    if (in_w < 0 || in_w >= input_width || in_h < 0 || in_h >= input_height)
                    {
                        memset(outputBase + out_index * item_size, 0, item_size);
                    }
                    else
                    {
                        vx_int32 in_index = in_w + in_h * input_width + in_d * input_width * input_height;

                        if (item_size == vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs)))
                        {
                            memcpy(outputBase + out_index * item_size, inputBase + in_index * item_size, item_size);
                        }
                        else
                        {
                            data = vxnneGetDataExt(inputFormat, TENSOR_QUANT_TYPE(inputs), in_index, inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));

                            status |= vxnneSaveDataExt(outputFormat, TENSOR_QUANT_TYPE(outputs), out_index, data, outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                        }
                    }

                }
            }
        }
    }


    return status;
}


vx_status vxnneExecuteSWReorg2(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_reorg_operation reorgOperation   = (vxnne_reorg_operation)operation;

    vx_scalar  types            = (vx_scalar)reorgOperation->type;

    vx_enum    type             = types->value->e;

    switch (type)
    {
        case VX_REORG_DEPTH_TO_SPACE:
            vxnneReorg2_Depth2Space(operation);
            break;
        case VX_REORG_SPACE_TO_DEPTH:

            vxnneReorg2_Space2Depth(operation);
            break;

        case VX_REORG_BATCH_TO_SPACE_ND:
            vxnneReorg2_Batch2SpaceND(operation);
            break;

        case VX_REORG_SPACE_TO_BATCH_ND:
            vxnneReorg2_Space2BatchND(operation);
            break;

        default:
            break;
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReOrg2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReOrg2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoReOrg2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status _GetTPReorgCmdType(vx_enum reorg_type,
                                            vx_tp_cmd_type_e *cmd_type)
{
    vx_status status = VX_SUCCESS;

    if (!cmd_type)
    {
        return VX_ERROR_INVALID_PARAMETERS;
    }

    switch (reorg_type)
    {
    case VX_REORG_DEPTH_TO_SPACE:
        *cmd_type = TP_REORG_DEPTH2SPACE;
        break;

    case VX_REORG_SPACE_TO_DEPTH:
        *cmd_type = TP_REORG_SPACE2DEPTH;
        break;

    case VX_REORG_SPACE_TO_BATCH_ND:
        *cmd_type = TP_REORG_SPACE2BATCH;
        break;

    case VX_REORG_BATCH_TO_SPACE_ND:
        *cmd_type = TP_REORG_BATCH2SPACE;
        break;

    default:
        status = VX_ERROR_NOT_SUPPORTED;
        break;
    }

    return status;
}

VX_PRIVATE_API vx_status _InitializeReorg2OperationTP(
    vxnne_reorg_layer layer,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_tensor block_size,
    vx_scalar type_s,
    vx_tensor pad,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    vx_op_param op_param = VX_NULL;
    vx_tp_cmd_type_e tp_cmd_type = TP_NONE;
    vx_uint32 block_size_width = (vx_uint32)VX_GET_DATA_FROM_TENSOR(block_size, 0);
    vx_uint32 block_size_height = (vx_uint32)VX_GET_DATA_FROM_TENSOR(block_size, 1);
    vx_enum type = type_s->value->e;

    if (!op_index)
    {
        vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
    }

    vxmONERROR(_GetTPReorgCmdType(type, &tp_cmd_type));

    vxmONERROR(vxnneOperation_Initialize(&layer->reorg_tp_operation.base,
                                         &layer->base,
                                         VXNNE_OPERATION_TARGET_TP,
                                         VXNNE_OPERATOR_REORG2,
                                         VX_NULL,
                                         vxnneOperation_TP_Deinitialize,
                                         batch_count,
                                         0));

    op_param = &layer->reorg_tp_operation.base.parameter;

    /* fetch pad height info from pad[1] and weight info from pad[0] */
    if (pad)
    {
        op_param->pad_x_left   = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 0);
        op_param->pad_x_right  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 1);
        op_param->pad_y_top    = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 2);
        op_param->pad_y_bottom = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 3);
    }

    op_param->pool_size_x = 0;
    op_param->pool_size_y = 0;
    op_param->pool_stride = 1;
    op_param->enable_relu = vx_false_e;
    op_param->pad_mode    = VX_PAD_CONSTANT;
    op_param->pad_const   = TENSOR_PAD_ZERO_VALUE(input);
    op_param->tpType      = tp_cmd_type;
    op_param->other_ref   = gcvNULL;
    op_param->data_buff   = gcvNULL;

    op_param->tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
    if (!op_param->tp_value)
    {
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }
    op_param->tp_value->u32[0] = block_size_width;
    op_param->tp_value->u32[1] = block_size_height;
    op_param->tp_value->u32[2] = TENSOR_VIEW_SIZE_INDEX(input, 3);

    vxmONERROR(vxnneLayer_SetOperation(&layer->base,
                                       &layer->reorg_tp_operation.base,
                                       (*op_index)++));

    layer->reorg_tp_operation.input  = input;
    layer->reorg_tp_operation.output = output;

    vxnneOperation_AddReference(&layer->reorg_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&layer->reorg_tp_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

OnError:
    return status;
}

VX_PRIVATE_API vx_status _InitializeReorg2OperationSH(
    vxnne_reorg_layer reorgLayer,
    vx_tensor inputs,
    vx_tensor outputs,
    vx_uint32 batchCount,
    vx_tensor block_size_s,
    vx_scalar type_s,
    vx_tensor pad,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    vx_scalar  stride;
    vx_uint32  block_size  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(block_size_s, 0);
    vx_uint32 pad_list[4]  = {0};
    vx_uint32  input_depth = TENSOR_SIZE_INDEX(inputs, 2);
    vx_scalar outc_s       = vxCreateScalar(reorgLayer->base.node->base.context, VX_TYPE_UINT32, &input_depth);
    vx_enum type           = type_s->value->e;
    vx_node node           = reorgLayer->base.node;
    vx_context context     = vxGetContext((vx_reference)node);
    vxnne_shader_executable shaderExecutable = NULL;

    if(pad)
    {
        pad_list[0]  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 0);
        pad_list[1]  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 1);
        pad_list[2]  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 2);
        pad_list[3]  = (vx_uint32)VX_GET_DATA_FROM_TENSOR(pad, 3);
    }

    stride = vxCreateScalar(context, VX_TYPE_UINT32, &block_size);

    if(context->evisNoInst.supportEVIS)
    {
        if (type == VX_REORG_DEPTH_TO_SPACE)
            shaderExecutable = vxnneGetDepth2SpaceShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, stride, outputs);
        else if(type == VX_REORG_SPACE_TO_DEPTH)
            shaderExecutable = vxnneGetSpace2DepthShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, stride, outc_s, outputs);
        else if(type == VX_REORG_SPACE_TO_BATCH_ND)
            shaderExecutable = vxnneGetSpace2BatchShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, block_size_s, pad, outc_s, outputs, pad_list);
        else if(type == VX_REORG_BATCH_TO_SPACE_ND)
            shaderExecutable = vxnneGetBatch2SpaceShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, block_size_s, outc_s, outputs);
    }
    else
    {
        if (type == VX_REORG_DEPTH_TO_SPACE)
            shaderExecutable = vxnneGetGPUDepth2SpaceShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, stride, outputs);
        else if(type == VX_REORG_SPACE_TO_DEPTH)
            shaderExecutable = vxnneGetGPUSpace2DepthShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, stride, outputs);
        else if(type == VX_REORG_BATCH_TO_SPACE_ND)
            shaderExecutable = vxnneGetGPUBatch2SpaceShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, block_size_s, outputs);
        else if(type == VX_REORG_SPACE_TO_BATCH_ND)
            shaderExecutable = vxnneGetGPUSpace2BatchShaderExecutable(context, VXNNE_KERNEL_REORG2, &node->kernelAttributes.borderMode,
                inputs, block_size_s, outc_s, outputs, pad_list);

        if(type == VX_REORG_BATCH_TO_SPACE_ND || type == VX_REORG_SPACE_TO_BATCH_ND)
            batchCount = 1;
    }

    if (!shaderExecutable)
    {
        status = VX_FAILURE;
        if (outc_s) vxReleaseScalar(&outc_s);
        if (stride) vxReleaseScalar(&stride);
        goto OnError;
    }

    status = vxnneShaderOperation_Initialize(&reorgLayer->reorg_sh_operation,
        &reorgLayer->base,
        VXNNE_OPERATOR_REORG2,
        batchCount,
        shaderExecutable);

    if (status != VX_SUCCESS)
    {
        if (outc_s) vxReleaseScalar(&outc_s);
        if (stride) vxReleaseScalar(&stride);
        if (outc_s) vxReleaseScalar(&outc_s);
        goto OnError;
    }

    vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)block_size_s, VXNNE_OPERATION_REFENRENCE_INPUT);
    if(pad)
        vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)pad, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&reorgLayer->reorg_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

    vxnneLayer_SetOperation(
        &reorgLayer->base,
        &reorgLayer->reorg_sh_operation.base,
        0);

    if (outc_s) vxReleaseScalar(&outc_s);
    if (stride) vxReleaseScalar(&stride);

OnError:
    return status;
}

VX_PRIVATE_API vx_status _InitializeReorg2OperationSW(
    vxnne_reorg_layer layer,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_tensor block_size,
    vx_scalar type_s,
    vx_tensor pad,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    if (!op_index)
    {
        vxmONERROR(VX_ERROR_INVALID_PARAMETERS);
    }

    vxmONERROR(vxnneOperation_Initialize(&layer->reorg_sw_operation.base,
                                         &layer->base,
                                         VXNNE_OPERATION_TARGET_SW,
                                         VXNNE_OPERATOR_REORG2,
                                         vxnneExecuteSWReorg2,
                                         VX_NULL,
                                         batch_count,
                                         0));

    vxmONERROR(vxnneLayer_SetOperation(&layer->base,
                                       &layer->reorg_sw_operation.base,
                                       (*op_index)++));

    layer->reorg_sw_operation.inputs  = input;
    layer->reorg_sw_operation.stride  = (vx_reference)block_size;
    layer->reorg_sw_operation.type    = type_s;
    layer->reorg_sw_operation.pad     = pad;
    layer->reorg_sw_operation.outputs = output;

    vxnneOperation_AddReference(&layer->reorg_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&layer->reorg_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

OnError:
    return status;
}

VX_PRIVATE_API vx_status _InitializeReorg2Operation(
    vxnne_reorg_layer layer,
    vxnne_operation_target_e target,
    vx_tensor input,
    vx_tensor output,
    vx_uint32 batch_count,
    vx_tensor block_size,
    vx_scalar type_s,
    vx_tensor pad,
    vx_uint32 *op_index)
{
    vx_status status = VX_SUCCESS;

    if (!op_index)
    {
        return VX_ERROR_INVALID_PARAMETERS;
    }

    switch (target)
    {
    case VXNNE_OPERATION_TARGET_TP:
        vxmONERROR(_InitializeReorg2OperationTP(layer,
                                                input,
                                                output,
                                                batch_count,
                                                block_size,
                                                type_s,
                                                pad,
                                                op_index));
        break;

    case VXNNE_OPERATION_TARGET_SH:
        vxmONERROR(_InitializeReorg2OperationSH(layer,
                                                input,
                                                output,
                                                batch_count,
                                                block_size,
                                                type_s,
                                                pad,
                                                op_index));
        break;

    case VXNNE_OPERATION_TARGET_SW:
        vxmONERROR(_InitializeReorg2OperationSW(layer,
                                                input,
                                                output,
                                                batch_count,
                                                block_size,
                                                type_s,
                                                pad,
                                                op_index));
        break;

    default:
        status = VX_ERROR_NOT_SUPPORTED;
        break;
    }

OnError:
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReOrg2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status                = VX_SUCCESS;
    vx_tensor inputs                = (vx_tensor)parameters[0];
    vx_tensor block_size            = (vx_tensor)parameters[1];
    vx_scalar type_s                = (vx_scalar)parameters[2];
    vx_tensor pad                   = (vx_tensor)parameters[3];
    vx_tensor outputs               = (vx_tensor)parameters[4];
    vx_context context              = vxGetContext((vx_reference)node);
    vx_enum type                    = type_s->value->e;
    vx_uint32 batch_count           = (type == VX_REORG_BATCH_TO_SPACE_ND || type == VX_REORG_SPACE_TO_BATCH_ND) ? 1 : TENSOR_SIZE_INDEX(inputs, 3);
    vxnne_operation_target_e target = VXNNE_OPERATION_TARGET_NONE;
    vx_tp_cmd_type_e tp_cmd_type    = TP_NONE;
    vxnne_reorg_layer reorg_layer   = VX_NULL;
    vx_uint32 op_index              = 0;
    vx_bool    dataFormat_flag[5]   = {vx_false_e};
    vx_bool    depth2Space_flag     = vx_false_e;
    vx_bool    space2Depth_flag     = vx_false_e;
    vx_bool    space2Batch_flag     = vx_false_e;
    vx_bool    batch2Space_flag     = vx_false_e;
    vx_int8    in_fixpoint          = TENSOR_POS(inputs);
    vx_int8    out_fixpoint         = TENSOR_POS(outputs);
    vx_enum    inputFormat          = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat         = TENSOR_DATA_TYPE(outputs);

    dataFormat_flag[0] = (vx_bool)((inputFormat == VX_TYPE_UINT8) && (outputFormat == VX_TYPE_UINT8));
    dataFormat_flag[1] = (vx_bool)((inputFormat == VX_TYPE_FLOAT16) && (outputFormat == VX_TYPE_FLOAT16));
    dataFormat_flag[2] = (vx_bool)((inputFormat == VX_TYPE_INT16) && (outputFormat == VX_TYPE_INT16) && (in_fixpoint == out_fixpoint));
    dataFormat_flag[3] = (vx_bool)((inputFormat == VX_TYPE_INT8) && (outputFormat == VX_TYPE_INT8) && (in_fixpoint == out_fixpoint));
    dataFormat_flag[4] = (vx_bool)((inputFormat == VX_TYPE_FLOAT32) && (outputFormat == VX_TYPE_FLOAT32) && (in_fixpoint == out_fixpoint));
    depth2Space_flag   = (vx_bool)(type == VX_REORG_DEPTH_TO_SPACE && (dataFormat_flag[0] || dataFormat_flag[1] || dataFormat_flag[4]));
    space2Depth_flag   = (vx_bool)(type == VX_REORG_SPACE_TO_DEPTH && (dataFormat_flag[0] || dataFormat_flag[1] || dataFormat_flag[2] || dataFormat_flag[3] || dataFormat_flag[4]));
    space2Batch_flag   = (vx_bool)(type == VX_REORG_SPACE_TO_BATCH_ND && (dataFormat_flag[0] || dataFormat_flag[1] || dataFormat_flag[4]));
    batch2Space_flag   = (vx_bool)(type == VX_REORG_BATCH_TO_SPACE_ND && (dataFormat_flag[0] || dataFormat_flag[1] || dataFormat_flag[4]));

    /* Destroy the existing layer. */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    /* Create the layer. */
    reorg_layer = (vxnne_reorg_layer)vxAllocate(sizeof(vxnne_reorg_layer_s));
    if (!reorg_layer)
    {
        vxError("Allocate memory fail at function %s line %d.", __FUNCTION__, __LINE__);
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }

    vxmONERROR(vxnneLayer_Initialize(&reorg_layer->base,
                                     "ReorgLayer2",
                                     node,
                                     vxmOPERATION_COUNT(reorg_layer),
                                     reorg_layer->operations,
                                     VX_NULL));

    /* Choose acceleration path. */
    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        !vxmIS_ERROR(_GetTPReorgCmdType(type, &tp_cmd_type)))
    {
        target = VXNNE_OPERATION_TARGET_TP;
    }
    else if ((depth2Space_flag || space2Depth_flag || space2Batch_flag || batch2Space_flag) && vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER))
    {
        target = VXNNE_OPERATION_TARGET_SH;
    }
    else
    {
        target = VXNNE_OPERATION_TARGET_SW;
    }

    /* Initialize the operation. */
    vxmONERROR(_InitializeReorg2Operation(reorg_layer,
                                          target,
                                          inputs,
                                          outputs,
                                          batch_count,
                                          block_size,
                                          type_s,
                                          pad,
                                          &op_index));

    node->layer = &reorg_layer->base;

    return status;

OnError:
    if (reorg_layer)
    {
        if (reorg_layer->reorg_tp_operation.base.parameter.tp_value)
        {
            gcoOS_Free(gcvNULL, (gctPOINTER)reorg_layer->reorg_tp_operation.base.parameter.tp_value);
        }
        gcoOS_Free(gcvNULL, (gctPOINTER)reorg_layer);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReOrg2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 TensorRounding
 ***************************************************************************************************************************/
vx_status vxnneExecuteSWTensorRounding(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_tensor_rounding_operation roundingOperation = (vxnne_tensor_rounding_operation)operation;

    vx_tensor input  = roundingOperation->inputs;
    vx_tensor output = roundingOperation->outputs;
    vx_scalar mode   = roundingOperation->mode;

    vx_int32 size = (vx_int32)vxoMemory_ComputeElementCount(&input->tensorBuffer->memory, 0);
    vx_int32 i = 0;

    vx_type_e input_format  = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e output_format = (vx_type_e)TENSOR_DATA_TYPE(output);

    vx_uint8_ptr input_ptr  = TENSOR_LOGICAL_ADDR(input);
    vx_uint8_ptr output_ptr = TENSOR_LOGICAL_ADDR(output);

    vx_float32 data         = .0f;

    vx_int8 in_fixpoint    = TENSOR_POS(input);
    vx_int8 out_fixpoint   = TENSOR_POS(output);

    vx_enum out_rounding_mode = TENSOR_ROUNDING_MODE(output);

    vx_enum rounding = mode->value->e;

    for (i = 0; i < size; i ++)
    {
        data = vxnneGetDataExt(input_format, TENSOR_QUANT_TYPE(input), i, input_ptr, in_fixpoint, TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input));
        status |= vxnneSaveDataExt(output_format, TENSOR_QUANT_TYPE(output), i, vxoNNExternsionConvlutionRound(data, rounding), output_ptr, out_fixpoint, TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), out_rounding_mode);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorRounding(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorRounding_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorRounding_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorRounding_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  mode_s                     = (vx_scalar)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_bool    dataFormat_flag            = vx_false_e;
    vx_enum    rounding                   = mode_s->value->e;

    vxnne_tensor_rounding_layer  roundingLayer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_rounding_layer_s), (gctPOINTER*)&roundingLayer);
    if (!roundingLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    if(node->base.context->evisNoInst.supportEVIS)
    {
        dataFormat_flag = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT8) ||
                                    (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT16) ||
                                    (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_UINT8) ||
                                    (inputFormat == VX_TYPE_INT8    && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_INT16   && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_UINT8   && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_UINT8   && outputFormat == VX_TYPE_UINT8  ) ||
                                    (inputFormat == VX_TYPE_INT8    && outputFormat == VX_TYPE_INT8   ) ||
                                    (inputFormat == VX_TYPE_INT16   && outputFormat == VX_TYPE_INT16  ) );
    }
    else
    {
        dataFormat_flag = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32));
    }
    gcoOS_ZeroMemory(roundingLayer, sizeof(vxnne_tensor_rounding_layer_s));

    vxnneLayer_Initialize(&roundingLayer->base,
                          "TensorRounding",
                          node,
                          vxmOPERATION_COUNT(roundingLayer),
                          roundingLayer->operations,
                          VX_NULL);

    if (dataFormat_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER))
        && rounding == VX_NN_DS_SIZE_ROUNDING_FLOOR)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetFloorShaderExecutable(node->base.context, VXNNE_KERNEL_FLOOR, &node->kernelAttributes.borderMode,
                inputs, mode_s, outputs);
        }
        else
        {
            shaderExecutable = vxnneGetGPUFloorShaderExecutable(node->base.context, VXNNE_KERNEL_FLOOR, &node->kernelAttributes.borderMode,
                inputs, mode_s, outputs);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&roundingLayer->tensor_rounding_sh_operation,
            &roundingLayer->base,
            VXNNE_OPERATOR_TENSOR_ROUNDING,
            batchCount,
            shaderExecutable);
        if (status != VX_SUCCESS) goto exit;

        vxnneOperation_AddReference(&roundingLayer->tensor_rounding_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&roundingLayer->tensor_rounding_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &roundingLayer->base,
            &roundingLayer->tensor_rounding_sh_operation.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&roundingLayer->tensor_rounding_sw_operation.base,
            &roundingLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_ROUNDING,
            vxnneExecuteSWTensorRounding,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &roundingLayer->base,
            &roundingLayer->tensor_rounding_sw_operation.base,
            0);

        roundingLayer->tensor_rounding_sw_operation.inputs           = inputs;
        roundingLayer->tensor_rounding_sw_operation.mode             = mode_s;
        roundingLayer->tensor_rounding_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&roundingLayer->tensor_rounding_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&roundingLayer->tensor_rounding_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    node->layer = &roundingLayer->base;

    return status;

exit:
    if(roundingLayer) gcoOS_Free(NULL, (gctPOINTER)roundingLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorRounding_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Hash Lookup Table
 ***************************************************************************************************************************/
vx_status vxnneExecuteSWHashLUT(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_hashlut_operation hashlutOperation = (vxnne_hashlut_operation)operation;

    vx_tensor input  = hashlutOperation->inputs;
    vx_tensor keys   = hashlutOperation->keys;
    vx_tensor values = hashlutOperation->values;
    vx_tensor hits   = hashlutOperation->hits;
    vx_tensor output = hashlutOperation->outputs;

    vx_int32 input_count = TENSOR_SIZE_INDEX(input, 0);

    vx_int32 key_count = TENSOR_SIZE_INDEX(keys, 0);

    vx_int32 value_count = TENSOR_SIZE_INDEX(values, 1);
    vx_int32 value_stride = TENSOR_SIZE_INDEX(values, 0);

    vx_int32 i = 0, j = 0, key = 0, index = -1, stride = value_stride * TENSOR_DATA_SIZE(values);
    vx_int32_ptr inPtr = (vx_int32_ptr)TENSOR_LOGICAL_ADDR(input);
    vx_int32_ptr keyPtr = (vx_int32_ptr)TENSOR_LOGICAL_ADDR(keys);
    vx_uint8_ptr hitPtr = (vx_uint8_ptr)TENSOR_LOGICAL_ADDR(hits);
    for (i = 0; i < input_count; i ++)
    {
        //key = (vx_int32)VX_GET_DATA_FROM_TENSOR(input, i);
        key = inPtr[i];

        index = -1;

        /* find the index of key from keys*/
        for (j = 0; j < key_count; j ++)
        {
            //if (key == VX_GET_DATA_FROM_TENSOR(keys, j))
            if (key == keyPtr[j])
            {
                index = j;
                break;
            }
        }

        if ((index < key_count) && (index >= 0 && index < value_count))
        {
            hitPtr[i] = 1;
            memcpy(TENSOR_LOGICAL_ADDR(output) + i * stride, TENSOR_LOGICAL_ADDR(values) + index * stride, stride);
        }
        else
        {
            hitPtr[i] = 0;
            memset(TENSOR_LOGICAL_ADDR(output) + i * stride, 0, stride);
        }
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNHashLUT(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    status = VX_SUCCESS;

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoHashLUT_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoHashLUT_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoHashLUT_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  keys                       = (vx_tensor)parameters[1];
    vx_tensor  values                     = (vx_tensor)parameters[2];
    vx_tensor  hits                       = (vx_tensor)parameters[3];
    vx_tensor  outputs                    = (vx_tensor)parameters[4];

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);

    vxnne_hashlut_layer  hashlutLayer     = VX_NULL;
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_enum    valueFormat                = TENSOR_DATA_TYPE(values);
    vx_bool    dataFormat_flag            = vx_false_e;
    vx_float32    input_scale             = TENSOR_TF_SCALE(values);
    vx_float32    output_scale            = TENSOR_TF_SCALE(outputs);
    vx_int32      inputZP                 = TENSOR_TF_ZEROPOINT(values);
    vx_int32      outputZP                = TENSOR_TF_ZEROPOINT(outputs);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_hashlut_layer_s), (gctPOINTER*)&hashlutLayer);
    if (!hashlutLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(hashlutLayer, sizeof(vxnne_hashlut_layer_s));

    if(node->base.context->evisNoInst.supportEVIS)
    {
        if (((valueFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            && (input_scale == output_scale && inputZP == outputZP))
            || (valueFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16))
            dataFormat_flag = vx_true_e;
    }
    else
    {
        if (((valueFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            && (input_scale == output_scale && inputZP == outputZP))
            || (valueFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
            || (valueFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32))
            dataFormat_flag = vx_true_e;
    }

    vxnneLayer_Initialize(&hashlutLayer->base,
                          "HashLUT",
                          node,
                          vxmOPERATION_COUNT(hashlutLayer),
                          hashlutLayer->operations,
                          VX_NULL);

    if (dataFormat_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetHashLUTShaderExecutable(node->base.context, VXNNE_KERNEL_HASHLUT,
                &node->kernelAttributes.borderMode, inputs, keys, values, hits, outputs);
        }
        else
        {
            shaderExecutable = vxnneGetGPUHashLUTShaderExecutable(node->base.context, VXNNE_KERNEL_HASHLUT,
                &node->kernelAttributes.borderMode, inputs, keys, values, hits, outputs);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto OnError;
        }

        status = vxnneShaderOperation_Initialize(&hashlutLayer->hashlut_sh_operation,
            &hashlutLayer->base,
            VXNNE_OPERATOR_HASHLUT,
            batchCount,
            shaderExecutable);
        if (status != VX_SUCCESS)
            goto OnError;

        vxnneOperation_AddReference(&hashlutLayer->hashlut_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sh_operation.base, (vx_reference)keys, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sh_operation.base, (vx_reference)values, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sh_operation.base, (vx_reference)hits, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &hashlutLayer->base,
            &hashlutLayer->hashlut_sh_operation.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&hashlutLayer->hashlut_sw_operation.base,
                                  &hashlutLayer->base,
                                  VXNNE_OPERATION_TARGET_SW,
                                  VXNNE_OPERATOR_HASHLUT,
                                  vxnneExecuteSWHashLUT,
                                  VX_NULL,
                                  batchCount,
                                  0);

        vxnneLayer_SetOperation(
            &hashlutLayer->base,
            &hashlutLayer->hashlut_sw_operation.base,
            0);

        hashlutLayer->hashlut_sw_operation.inputs           = inputs;
        hashlutLayer->hashlut_sw_operation.keys             = keys;
        hashlutLayer->hashlut_sw_operation.values           = values;
        hashlutLayer->hashlut_sw_operation.hits             = hits;
        hashlutLayer->hashlut_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&hashlutLayer->hashlut_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sw_operation.base, (vx_reference)keys, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sw_operation.base, (vx_reference)values, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sw_operation.base, (vx_reference)hits, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        vxnneOperation_AddReference(&hashlutLayer->hashlut_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &hashlutLayer->base;

    return status;

OnError:
    if (hashlutLayer) {
        gcoOS_Free(VX_NULL, (gctPOINTER)hashlutLayer);
        hashlutLayer = VX_NULL;
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoHashLUT_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 LSH Projection
 ***************************************************************************************************************************/
static const vx_uint64 k0 = 0xc3a5c85c97cb3127ULL;
static const vx_uint64 k1 = 0xb492b66fbe98f273ULL;
static const vx_uint64 k2 = 0x9ae16a3b2f90404fULL;

#define Fetch Fetch64
#define Rotate vxnneExecuteSWLSHRotate64

VX_PRIVATE_API vx_uint64 Fetch64(const vx_uint8_ptr p) {
    vx_uint64 result;
    memcpy(&result, p, sizeof(result));
    return result;
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWLSHFetch32(const vx_uint8_ptr p) {
    vx_uint32 result;
    memcpy(&result, p, sizeof(result));
    return result;
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWLSHRotate64(vx_uint64 val, vx_int32 shift) {
  return shift == 0 ? val : ((val >> shift) | (val << (64 - shift)));
}

VX_PRIVATE_API vx_uint64 ShiftMix(vx_uint64 val) {
    return val ^ (val >> 47);
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWLSHHashLen16(uint64_t u, uint64_t v, uint64_t mul) {
    vx_uint64 b = 0, a = (u ^ v) * mul;
    a ^= (a >> 47);
    b = (v ^ a) * mul;
    b ^= (b >> 47);
    b *= mul;
    return b;
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWHashLen0to16(const vx_uint8_ptr s, vx_size len) {
    if (len >= 8) {
        vx_uint64 mul = k2 + len * 2;
        vx_uint64 a = Fetch(s) + k2;
        vx_uint64 b = Fetch(s + len - 8);
        vx_uint64 c = Rotate(b, 37) * mul + a;
        vx_uint64 d = (Rotate(a, 25) + b) * mul;
        return vxnneExecuteSWLSHHashLen16(c, d, mul);
    }
    if (len >= 4) {
        vx_uint64 mul = k2 + len * 2;
        vx_uint64 a = vxnneExecuteSWLSHFetch32(s);
        return vxnneExecuteSWLSHHashLen16(len + (a << 3), vxnneExecuteSWLSHFetch32(s + len - 4), mul);
    }
    if (len > 0) {
        vx_int8 a = s[0];
        vx_int8 b = s[len >> 1];
        vx_int8 c = s[len - 1];
        vx_int32 y = (vx_int32)(a) + ((vx_int32)(b) << 8);
        vx_int32 z = (vx_int32)len + ((vx_int32)(c) << 2);
        return ShiftMix(y * k2 ^ z * k0) * k2;
    }
    return k2;
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWHashLen17to32(const vx_uint8_ptr s, vx_size len) {
    vx_uint64 mul = k2 + len * 2;
    vx_uint64 a = Fetch(s) * k1;
    vx_uint64 b = Fetch(s + 8);
    vx_uint64 c = Fetch(s + len - 8) * mul;
    vx_uint64 d = Fetch(s + len - 16) * k2;
    return vxnneExecuteSWLSHHashLen16(Rotate(a + b, 43) + Rotate(c, 30) + d,
                     a + Rotate(b + k2, 18) + c, mul);
}

VX_PRIVATE_API vx_uint64 vxnneExecuteSWLSHHashLen33to64(const vx_uint8_ptr s, vx_size len) {
    vx_uint64 mul = k2 + len * 2;
    vx_uint64 a = Fetch(s) * k2;
    vx_uint64 b = Fetch(s + 8);
    vx_uint64 c = Fetch(s + len - 8) * mul;
    vx_uint64 d = Fetch(s + len - 16) * k2;
    vx_uint64 y = Rotate(a + b, 43) + Rotate(c, 30) + d;
    vx_uint64 z = vxnneExecuteSWLSHHashLen16(y, a + Rotate(b + k2, 18) + c, mul);
    vx_uint64 e = Fetch(s + 16) * mul;
    vx_uint64 f = Fetch(s + 24);
    vx_uint64 g = (y + Fetch(s + len - 32)) * mul;
    vx_uint64 h = (z + Fetch(s + len - 24)) * mul;
    return vxnneExecuteSWLSHHashLen16(Rotate(e + f, 43) + Rotate(g, 30) + h,
                     e + Rotate(f + a, 18) + g, mul);
}
VX_PRIVATE_API vx_int64 vxnneExecuteSWLSHHash64(vx_uint8_ptr s, vx_size lenght)
{
    if (lenght <= 32) {
        if (lenght <= 16) {
            return vxnneExecuteSWHashLen0to16(s, lenght);
        } else {
            return vxnneExecuteSWHashLen17to32(s, lenght);
        }
    } else if (lenght <= 64) {
        return vxnneExecuteSWLSHHashLen33to64(s, lenght);
    }

    return 0;
}

VX_PRIVATE_API vx_int32 vxnneExecuteSWLSHRunningSignBit(vx_tensor input, vx_tensor weight, vx_float32 seed)
{
    vx_float64 score = .0, running_value = .0;
    vx_int32 count = TENSOR_SIZE_INDEX(input, 1);
    vx_int32 input_item_bytes = (vx_int32)vxoMemory_ComputeElementCount(&input->tensorBuffer->memory, 0) * TENSOR_DATA_SIZE(input) / count;
    vx_int32 i = 0, stride = input_item_bytes;
    vx_uint32 key_bytes = stride + sizeof(vx_float32);
    vx_uint8_ptr keys = (vx_uint8_ptr)vxAllocateAndZeroMemory(key_bytes);
    vx_int64 hash_signature = 0;
    vx_float32 w = .0f;

    for (i = 0; i < count; i ++)
    {
        memcpy(keys, &seed, sizeof(vx_float32));
        memcpy(keys + sizeof(vx_float32), TENSOR_LOGICAL_ADDR(input) + i * stride, stride);

        hash_signature = vxnneExecuteSWLSHHash64(keys, key_bytes);
        running_value = (vx_float64)hash_signature;
        w = (TENSOR_VALUED(weight) == vx_true_e) ? VX_GET_DATA_FROM_TENSOR(weight, i):1;

        score += w * running_value;
    }

    vxFree(keys);
    return (score > 0) ? 1 : 0;
}
vx_status vxnneExecuteSWLSHProjection(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_lshprojection_operation lshhashlutOperation = (vxnne_lshprojection_operation)operation;

    vx_tensor input     = lshhashlutOperation->inputs;
    vx_tensor hash      = lshhashlutOperation->hash_func;
    vx_tensor weight    = lshhashlutOperation->weight;
    vx_tensor types     = lshhashlutOperation->type;
    vx_tensor output    = lshhashlutOperation->outputs;

    vx_uint32 num_hash = TENSOR_SIZE_INDEX(hash, 1);
    vx_uint32 num_bits = TENSOR_SIZE_INDEX(hash, 0);
    vx_uint32 h = 0, b = 0;
    vx_int32 hash_signature = 0, bit = 0;
    vx_float32 seed = .0f;
    vx_int32 type = *((vx_int32_ptr)TENSOR_LOGICAL_ADDR((vx_tensor)types));


    for (h = 0; h < num_hash; h ++)
    {
        hash_signature = 0;
        for (b = 0; b < num_bits; b ++)
        {
            seed = VX_GET_DATA_FROM_TENSOR(hash, b + h * num_bits);
            bit = vxnneExecuteSWLSHRunningSignBit(input, weight, seed);

            switch (type)
            {
            case VX_LSH_PROJ_DENSE:
                VX_SAVE_DATA_TO_TENSOR(output, bit, b + h * num_bits);
                break;

            case VX_LSH_PROJ_SPARSE:
                hash_signature = (hash_signature << 1) | bit;
                break;
            }
        }

        if (type == VX_LSH_PROJ_SPARSE)
            VX_SAVE_DATA_TO_TENSOR(output, hash_signature, h);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLSHProjection(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLSHProjection_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoLSHProjection_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLSHProjection_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{

    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  type                       = (vx_tensor)parameters[1];
    vx_tensor  hash_func                  = (vx_tensor)parameters[2];
    vx_tensor  weight                     = (vx_tensor)parameters[3];
    vx_tensor  outputs                    = (vx_tensor)parameters[4];

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);

    vxnne_lshprojection_layer  lshprojectionLayer     = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_lshprojection_layer_s), (gctPOINTER*)&lshprojectionLayer);
    if (!lshprojectionLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(lshprojectionLayer, sizeof(vxnne_lshprojection_layer_s));

    vxnneLayer_Initialize(&lshprojectionLayer->base,
                          "LSHProjection",
                          node,
                          vxmOPERATION_COUNT(lshprojectionLayer),
                          lshprojectionLayer->operations,
                          VX_NULL);

    vxnneOperation_Initialize(&lshprojectionLayer->lshprojection_sw_operation.base,
                              &lshprojectionLayer->base,
                              VXNNE_OPERATION_TARGET_SW,
                              VXNNE_OPERATOR_LSH_PROJECTION,
                              vxnneExecuteSWLSHProjection,
                              VX_NULL,
                              batchCount,
                              0);

    vxnneLayer_SetOperation(
        &lshprojectionLayer->base,
        &lshprojectionLayer->lshprojection_sw_operation.base,
        0);

    lshprojectionLayer->lshprojection_sw_operation.inputs           = inputs;
    lshprojectionLayer->lshprojection_sw_operation.type             = type;
    lshprojectionLayer->lshprojection_sw_operation.hash_func        = hash_func;
    lshprojectionLayer->lshprojection_sw_operation.weight           = weight;
    lshprojectionLayer->lshprojection_sw_operation.outputs          = outputs;

    vxnneOperation_AddReference(&lshprojectionLayer->lshprojection_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&lshprojectionLayer->lshprojection_sw_operation.base, (vx_reference)hash_func, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&lshprojectionLayer->lshprojection_sw_operation.base, (vx_reference)weight, VXNNE_OPERATION_REFENRENCE_INPUT);
    vxnneOperation_AddReference(&lshprojectionLayer->lshprojection_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

    node->layer = &lshprojectionLayer->base;


    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLSHProjection_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Tensor Reshape
 ***************************************************************************************************************************/
vx_status vxnneExecuteSWReshape(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_reshape_operation reshapeOperation = (vxnne_reshape_operation)operation;

    vx_tensor input  = reshapeOperation->inputs;
    vx_tensor output = reshapeOperation->outputs;
    vx_tensor dims   = reshapeOperation->dims;

    vx_int32 in_size = (vx_int32)vxoMemory_ComputeElementCount(&input->tensorBuffer->memory, 0);
    vx_int32 out_size = (vx_int32)vxoMemory_ComputeElementCount(&output->tensorBuffer->memory, 0);
    vx_int32 i = 0;

    vx_type_e input_format  = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e output_format  = (vx_type_e)TENSOR_DATA_TYPE(output);

    vx_uint8_ptr input_ptr  = TENSOR_LOGICAL_ADDR(input);
    vx_uint8_ptr output_ptr = TENSOR_LOGICAL_ADDR(output);

    vx_enum count = dims->dimCount;
    vx_int32 reshape_size = 1;
    vx_int32_ptr dim = (dims != VX_NULL)?(vx_int32_ptr)TENSOR_LOGICAL_ADDR(dims):VX_NULL;

    if ((dim == VX_NULL) || ((dim != VX_NULL) && (count == 1) && (dim[i] == -1)))
    {
        reshape_size = in_size;
    }
    else
    {
        if (dim != VX_NULL)
        {
            for (i = 0; i < count; i ++)
            {
                reshape_size *= dim[i];
            }
        }
    }

    if ((reshape_size != in_size) || (reshape_size > out_size) || (output_format != input_format))
       vxError("Invalid parament! reshape_size = %d, in_size = %d, out_size = %d, output_format = %d, input_format = %d", reshape_size, in_size, out_size, output_format, input_format);

    memcpy(output_ptr, input_ptr, reshape_size * vxDataType_GetSize(input_format));

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNReshape(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReshape_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoReshape_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReshape_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{

    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  dims                       = (vx_tensor)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_bool    shExe_flag                 = vx_false_e;
    vx_uint32  src_elementCount           = 0;
    vx_uint32  dst_elementCount           = 0;
    vx_uint32  dimCount0                  = 0;
    vx_uint32  width0                     = 0;
    vx_uint32  height0                    = 0;
    vx_uint32  depth0                     = 0;
    vx_uint32  batch0                     = 0;
    vx_uint32  dimCount1                  = 0;
    vx_uint32  width1                     = 0;
    vx_uint32  height1                    = 0;
    vx_uint32  depth1                     = 0;
    vx_uint32  batch1                     = 0;
    vx_uint32  batchCount                 = 1;


    vxnne_reshape_layer  reshapeLayer     = VX_NULL;
    vx_context           context          = vxGetContext((vx_reference)node);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_reshape_layer_s), (gctPOINTER*)&reshapeLayer);
    if (!reshapeLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(reshapeLayer, sizeof(vxnne_reshape_layer_s));

    vxnneLayer_Initialize(&reshapeLayer->base,
                          "Reshape",
                          node,
                          vxmOPERATION_COUNT(reshapeLayer),
                          reshapeLayer->operations,
                          VX_NULL);

    dimCount0    = TENSOR_VIEW_DIM_NUM(inputs);
    width0       = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    height0      = (dimCount0 > 1) ? TENSOR_VIEW_SIZE_INDEX(inputs, 1) : 1;
    depth0       = (dimCount0 > 2) ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
    batch0       = (dimCount0 > 3) ? TENSOR_VIEW_SIZE_INDEX(inputs, 3) : 1;
    src_elementCount = width0 * height0 * depth0 * batch0;

    dimCount1    = TENSOR_VIEW_DIM_NUM(outputs);
    width1       = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    height1      = (dimCount1 > 1) ? TENSOR_VIEW_SIZE_INDEX(outputs, 1) : 1;
    depth1       = (dimCount1 > 2) ? TENSOR_VIEW_SIZE_INDEX(outputs, 2) : 1;
    batch1       = (dimCount1 > 3) ? TENSOR_VIEW_SIZE_INDEX(outputs, 3) : 1;
    dst_elementCount = width1 * height1 * depth1 * batch1;

    if(context->evisNoInst.supportEVIS)
    {
        shExe_flag = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
            || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
            || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16))
            && src_elementCount == dst_elementCount);
    }
    else
    {
        shExe_flag = (vx_bool)(((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
            || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT32)
            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16))
            && src_elementCount == dst_elementCount);
    }

    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
        (src_elementCount == dst_elementCount) &&
        vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs))
    {
        vx_tensor     src       = NULL;
        vx_tensor     dst       = NULL;
        vx_int32      sizes[4]  = {0};
        vx_op_param_s conv      = {0};

        sizes[0]   = gcmMAX(gcmMAX(width0, height0), depth0);
        sizes[1]   = gcmMAX(gcmMIN(width0, height0), gcmMIN(gcmMAX(width0, height0), depth0));
        sizes[2]   = gcmMIN(gcmMIN(width0, height0), depth0) * batch0;
        sizes[3]   = 1;

        src     = vxoTensor_ReshapeTensor(inputs, sizes, 3);
        dst     = vxoTensor_ReshapeTensor(outputs, sizes, 3);
        reshapeLayer->base.temp_tensors[0]      = src;
        reshapeLayer->base.temp_tensors[1]      = dst;
        reshapeLayer->base.num_temp_tensors     = 2;


        status = vxnneOperation_Initialize(&reshapeLayer->tensor_copy_tp_operation.base,
            &reshapeLayer->base,
            VXNNE_OPERATION_TARGET_TP,
            VXNNE_OPERATOR_TENSOR_COPY,
            VX_NULL,
            VX_NULL,
            batchCount,
            0);

        if (status != VX_SUCCESS) goto exit;

        memset(&conv, 0, sizeof(vx_op_param_s));

        conv.enable_relu = vx_false_e;
        conv.pool_stride = 1;
        conv.tpType = TP_TENSOR_COPY;

        memcpy(&reshapeLayer->tensor_copy_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneOperation_AddReference(&reshapeLayer->tensor_copy_tp_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reshapeLayer->tensor_copy_tp_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        reshapeLayer->tensor_copy_tp_operation.input = src;
        reshapeLayer->tensor_copy_tp_operation.output = dst;

        vxnneLayer_SetOperation(
            &reshapeLayer->base,
            &reshapeLayer->tensor_copy_tp_operation.base,
            0);
    }
    else if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) )
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_tensor input      = NULL;
        vx_tensor output     = NULL;
        vx_uint32 sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION];
        vx_uint32 dims = 0;

        vxoElementOptimization_GetTensorShape(inputs, sizes, &dims);

        input     = vxoTensor_ReshapeTensor(inputs, (vx_int32*)sizes, dims);
        output     = vxoTensor_ReshapeTensor(outputs, (vx_int32*)sizes, dims);

        if(node->base.context->evisNoInst.supportEVIS)
        {
            if (input && output)
                shaderExecutable = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, output);
        }
        else
        {
            if (input && output)
                shaderExecutable = vxnneGPUTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, output);
        }

        if (input) vxoTensor_ReleaseTensor(&input);
        if (output) vxoTensor_ReleaseTensor(&output);

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }
        status = vxnneShaderOperation_Initialize(&reshapeLayer->tensor_copy_sh_operation,
            &reshapeLayer->base,
            VXNNE_OPERATOR_CONVERT_FORMAT,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS)
            goto exit;

        vxnneOperation_AddReference(&reshapeLayer->tensor_copy_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reshapeLayer->tensor_copy_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &reshapeLayer->base,
            &reshapeLayer->tensor_copy_sh_operation.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&reshapeLayer->reshape_sw_operation.base,
            &reshapeLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_RESHAPE,
            vxnneExecuteSWReshape,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &reshapeLayer->base,
            &reshapeLayer->reshape_sw_operation.base,
            0);

        reshapeLayer->reshape_sw_operation.inputs           = inputs;
        reshapeLayer->reshape_sw_operation.dims             = dims;
        reshapeLayer->reshape_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&reshapeLayer->reshape_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reshapeLayer->reshape_sw_operation.base, (vx_reference)dims, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&reshapeLayer->reshape_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &reshapeLayer->base;


    return status;
exit:
    if (reshapeLayer != NULL)
        gcoOS_Free(NULL, reshapeLayer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoReshape_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Tensor Scale
 ***************************************************************************************************************************/
static vx_status _ExecuteSWScale(
    vx_enum   type,
    vx_uint8_ptr input_ptr,
    vx_uint8_ptr output_ptr,
    vx_uint32 input_width,
    vx_uint32 input_height,
    vx_uint32 input_depth,
    vx_uint32 output_width,
    vx_uint32 output_height,
    vx_uint32 output_depth,
    vx_uint32 output_batch,
    vx_uint32 input_width_orig,
    vx_uint32 output_width_orig,
    vx_type_e input_format,
    vx_type_e output_format,
    vx_int8 in_fixpoint,
    vx_int8 out_fixpoint,
    vx_uint32 in_tf_format,
    vx_uint32 out_tf_format,
    vx_uint32 in_tf_zp,
    vx_uint32 out_tf_zp,
    vx_float32 in_tf_scale,
    vx_float32 out_tf_scale,
    vx_enum out_rounding_mode
    )
{
    vx_status status = VX_SUCCESS;
    vx_float32 width_scale = (input_width * 1.0f) / output_width;
    vx_float32 height_scale = (input_height * 1.0f) / output_height;

    vx_uint32 b = 0, d = 0, w = 0, h = 0;

    vx_float32 data00 = .0f, data01 = .0f, data10 = .0f, data11 = .0f, interpolation = .0f;

    if (type == VX_INTERPOLATION_BILINEAR)
    {
        for (b = 0; b < output_batch; b ++)
        {
            for (d = 0; d < output_depth; d ++)
            {
                vx_int32 input_base = b * input_depth * input_width_orig * input_height + d * input_width_orig * input_height;
                vx_int32 output_base = b * output_depth * output_width_orig * output_height + d * output_width_orig * output_height;

                for (h = 0; h < output_height; h ++)
                {
                    vx_float32 input_h = h * height_scale;
                    vx_uint32 h0 = (vx_int32)input_h;
                    vx_uint32 h1 = gcmMIN(h0 + 1, input_height - 1);

                    for (w = 0; w < output_width; w ++)
                    {
                        vx_float32 input_w = w * width_scale;
                        vx_int32 w0 = (vx_int32)input_w;
                        vx_int32 w1 = gcmMIN(w0 + 1, (vx_int32)(input_width - 1));

                        data00 = vxnneGetDataExt((vx_type_e)input_format, in_tf_format, input_base + h0 * input_width_orig + w0, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);
                        data01 = vxnneGetDataExt((vx_type_e)input_format, in_tf_format, input_base + h0 * input_width_orig + w1, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);
                        data10 = vxnneGetDataExt((vx_type_e)input_format, in_tf_format, input_base + h1 * input_width_orig + w0, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);
                        data11 = vxnneGetDataExt((vx_type_e)input_format, in_tf_format, input_base + h1 * input_width_orig + w1, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);

                        interpolation = data00 * (1 - (input_h - h0)) * (1 - (input_w - w0)) +
                                        data10 * (input_h - h0) * (1 - (input_w - w0)) +
                                        data01 * (1 - (input_h - h0)) * (input_w - w0) +
                                        data11 * (input_h - h0) * (input_w - w0);

                        status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, output_base + h * output_width_orig + w, interpolation, output_ptr, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                    }
                }
            }
        }
    }
    else if (type == VX_INTERPOLATION_NEAREST_NEIGHBOR)
    {
        for (d = 0; d < output_depth; d ++)
        {
            for (h = 0; h < output_height; h ++)
            {
                vx_uint32 in_y = gcmMIN((vx_uint32)floorf(h * height_scale), input_height - 1);

                for (w = 0; w < output_width; w ++)
                {
                    vx_uint32   in_x        = gcmMIN((vx_uint32)floorf(w * width_scale), input_width - 1);
                    vx_int32    in_index    = in_x + in_y * input_width_orig + d * input_width_orig * input_height;
                    vx_int32    out_index   = w + h * output_width_orig + d * output_width_orig * output_height;
                    vx_float32  data;

                    data = vxnneGetDataExt((vx_type_e)input_format, in_tf_format, in_index, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);

                    status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, out_index, data, output_ptr, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                }
            }
        }
    }

    return status;
}

vx_status vxnneExecuteSWTensorScale(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_scale_operation scaleOperation = (vxnne_tensor_scale_operation)operation;

    vx_tensor input  = scaleOperation->inputs;
    vx_tensor output = scaleOperation->outputs;
    vx_scalar types  = scaleOperation->type;

    vx_type_e input_format  = (vx_type_e)TENSOR_DATA_TYPE(input);
    vx_type_e output_format = (vx_type_e)TENSOR_DATA_TYPE(output);

    vx_uint8_ptr input_ptr;
    vx_uint8_ptr output_ptr;

    vx_enum in_tf_format    = TENSOR_QUANT_TYPE(input);
    vx_enum out_tf_format   = TENSOR_QUANT_TYPE(output);

    vx_uint32 in_tf_zp      = TENSOR_TF_ZEROPOINT(input);
    vx_uint32 out_tf_zp     = TENSOR_TF_ZEROPOINT(output);

    vx_float32 in_tf_scale  = TENSOR_TF_SCALE(input);
    vx_float32 out_tf_scale = TENSOR_TF_SCALE(output);

    vx_int8 in_fixpoint    = TENSOR_POS(input);
    vx_int8 out_fixpoint   = TENSOR_POS(output);

    vx_enum out_rounding_mode = TENSOR_ROUNDING_MODE(output);

    vx_enum type = types->value->e;

    vx_uint32 input_width = TENSOR_SIZE_INDEX(input, 0);  /* W */
    vx_uint32 input_height = TENSOR_SIZE_INDEX(input, 1); /* H */
    vx_uint32 input_depth = TENSOR_SIZE_INDEX(input, 2);  /* C */
    /*vx_uint32 input_batch = TENSOR_SIZE_INDEX(input, 3);   N */

    vx_uint32 output_width = TENSOR_SIZE_INDEX(output, 0);  /* W */
    vx_uint32 output_height = TENSOR_SIZE_INDEX(output, 1); /* H */
    vx_uint32 output_depth = TENSOR_SIZE_INDEX(output, 2);  /* C */
    vx_uint32 output_batch = TENSOR_SIZE_INDEX(output, 3);  /* N */

    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&input_ptr, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&output_ptr, VX_NULL);

    return _ExecuteSWScale(
            type,
            input_ptr,
            output_ptr,
            input_width,
            input_height,
            input_depth,
            output_width,
            output_height,
            output_depth,
            output_batch,
            input_width,
            output_width,
            input_format,
            output_format,
            in_fixpoint,
            out_fixpoint,
            in_tf_format,
            out_tf_format,
            in_tf_zp,
            out_tf_zp,
            in_tf_scale,
            out_tf_scale,
            out_rounding_mode);
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorScale(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorScale_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorScale_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorScale_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{

    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  type_s                     = (vx_scalar)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];
    vx_bool    useShadeExe                = vx_false_e;
    vx_bool    enable_format              = vx_false_e;
    vx_bool    enable_nearest_format      = vx_false_e;
    vx_bool    enable_tmp_format          = vx_false_e;
    vx_bool    enable_nearest_neighbor    = vx_false_e;
    vx_bool    enable_nearest_scaleVal    = vx_false_e;
    vx_enum    srcFormat                  = TENSOR_DATA_TYPE(inputs);
    vx_enum    dstFormat                  = TENSOR_DATA_TYPE(outputs);
    vx_enum    type                       = type_s->value->e;
    vx_uint32  in_width                   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32  in_height                  = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
    vx_uint32  out_width                  = TENSOR_VIEW_SIZE_INDEX(outputs, 0);
    vx_uint32  out_height                 = TENSOR_VIEW_SIZE_INDEX(outputs, 1);
    vx_float32 width_scale                = (vx_float32)in_width / out_width;
    vx_float32 height_scale               = (vx_float32)in_height / out_height;

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);

    vxnne_tensor_scale_layer  tensor_scaleLayer     = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_scale_layer_s), (gctPOINTER*)&tensor_scaleLayer);
    if (!tensor_scaleLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(tensor_scaleLayer, sizeof(vxnne_tensor_scale_layer_s));

    vxnneLayer_Initialize(&tensor_scaleLayer->base,
                          "TensorScale",
                          node,
                          vxmOPERATION_COUNT(tensor_scaleLayer),
                          tensor_scaleLayer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enable_format           = (vx_bool)((srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_FLOAT16)
                                         ||(srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_FLOAT16)
                                         ||(srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_UINT8)
                                         ||(srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_INT8)
                                         ||(srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_INT16)
                                         ||(srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8));
        enable_nearest_format   = (vx_bool)(!checkOutputTensorDoAlu(inputs, outputs)
                                        || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8)
                                        || (srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_UINT8)
                                        || (srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_INT8));

        enable_tmp_format       = (vx_bool)((srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_FLOAT16)
                                         || (srcFormat == VX_TYPE_INT16 && dstFormat == VX_TYPE_INT16)
                                         || (srcFormat == VX_TYPE_INT8 && dstFormat == VX_TYPE_INT8)
                                         || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8));
    }
    else
    {
        enable_format           = (vx_bool)((srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_FLOAT16)
                                         ||(srcFormat == VX_TYPE_FLOAT32 && dstFormat == VX_TYPE_FLOAT32)
                                         ||(srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_FLOAT16)
                                         ||(srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_FLOAT32)
                                         ||(srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_UINT8)
                                         ||(srcFormat == VX_TYPE_FLOAT32 && dstFormat == VX_TYPE_UINT8)
                                         ||(srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8));
        enable_nearest_format   = (vx_bool)(!checkOutputTensorDoAlu(inputs, outputs)
                                        || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8)
                                        || (srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_UINT8)
                                        || (srcFormat == VX_TYPE_FLOAT32 && dstFormat == VX_TYPE_UINT8));

        enable_tmp_format       = (vx_bool)((srcFormat == VX_TYPE_FLOAT16 && dstFormat == VX_TYPE_FLOAT16)
                                         || (srcFormat == VX_TYPE_FLOAT32 && dstFormat == VX_TYPE_FLOAT32)
                                         || (srcFormat == VX_TYPE_UINT8 && dstFormat == VX_TYPE_UINT8));
    }

    enable_nearest_scaleVal = (vx_bool) ((width_scale == 2.0f && height_scale == 2.0f && in_width * in_height < IMG_MAX_WIDTH) || (width_scale == 0.5f && height_scale == 0.5f));
    enable_nearest_neighbor = (vx_bool) (((enable_nearest_format  && enable_nearest_scaleVal) || enable_tmp_format) && type == VX_INTERPOLATION_NEAREST_NEIGHBOR);

    useShadeExe     =  (vx_bool)((enable_format && type == VX_INTERPOLATION_BILINEAR) || enable_nearest_neighbor);

    if(useShadeExe && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if (type == VX_INTERPOLATION_BILINEAR)
        {
            if(node->base.context->evisNoInst.supportEVIS)
            {
                shaderExecutable = vxnneGetTensorScaleShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_SCALE, &node->kernelAttributes.borderMode, inputs, type, outputs);
            }
            else
            {
                shaderExecutable = vxnneGetGPUTensorScaleShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_SCALE, &node->kernelAttributes.borderMode, inputs, type, outputs);
            }
        }
        else
        {
            shaderExecutable = vxnneGetResizeNearestNeighborShaderExecutable(node->base.context, VXNNE_KERNEL_RESIZE_NEAREST_NEIGHBOR, &node->kernelAttributes.borderMode, inputs, type, outputs);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&tensor_scaleLayer->tensor_scale_sh_operation,
            &tensor_scaleLayer->base,
            VXNNE_OPERATOR_TENSOR_SCALE,
            batchCount,
            shaderExecutable);

        if (status != VX_SUCCESS) {
            goto exit;
        }

        vxnneOperation_AddReference(&tensor_scaleLayer->tensor_scale_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_scaleLayer->tensor_scale_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &tensor_scaleLayer->base,
            &tensor_scaleLayer->tensor_scale_sh_operation.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&tensor_scaleLayer->tensor_scale_sw_operation.base,
            &tensor_scaleLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_SCALE,
            vxnneExecuteSWTensorScale,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &tensor_scaleLayer->base,
            &tensor_scaleLayer->tensor_scale_sw_operation.base,
            0);

        tensor_scaleLayer->tensor_scale_sw_operation.inputs           = inputs;
        tensor_scaleLayer->tensor_scale_sw_operation.type             = type_s;
        tensor_scaleLayer->tensor_scale_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&tensor_scaleLayer->tensor_scale_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_scaleLayer->tensor_scale_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &tensor_scaleLayer->base;


    return status;
exit:
    if (tensor_scaleLayer) gcoOS_Free(gcvNULL, (gctPOINTER)tensor_scaleLayer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoTensorScale_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Tensor YUV2RGB Scale
 ***************************************************************************************************************************/
vx_status vxnneComputeYUV2RGBInputParameter(
    vx_uint32 outputSize,
    vx_uint32 scale,
    vx_uint32 inputStart,
    vx_uint32 * splitNum,
    vx_uint32 * outputStarts,
    vx_uint32 * outputSizes,
    vx_uint32 * inputStarts,
    vx_uint32 * inputSizes,
    vx_uint16 * inputInitErrors,
    vx_uint16 * inputInitIntErrors
    )
{
    vx_uint32 num, i, offset, inputSize;

    inputSize = (vx_uint32)((vx_float32)(outputSize * scale) / (1 << 15) + 0.5f);
    offset = gcmMAX(0, ((vx_int32)scale >> 1) - (1 << 14));

    num = gcmMIN(gcmMIN(inputSize, outputSize), *splitNum);

    calculateSplitSize(outputSize, num, outputSizes, outputStarts);

    for (i = 0; i < num; i++)
    {
        inputStarts[i] = (vx_uint16)inputStart + (vx_uint16)((offset & 0xFFFF8000) >> 15);
        if (inputStarts[i] & 0x1)
        {
            inputStarts[i]--;
            inputInitIntErrors[i] = 0x1;
        }
        else
        {
            inputInitIntErrors[i] = 0x0;
        }
        if (i > 0)
        {
            inputSizes[i-1] = gcmMAX(1, inputStarts[i] - inputStarts[i-1]);
        }
        inputInitErrors[i] = (vx_uint16)(offset & 0x7FFF);
        offset += scale * outputSizes[i];
    }

    inputSizes[i-1] = inputStart + inputSize - inputStarts[i-1];

    *splitNum = num;

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWYUV2RGBScale(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_yuv2rgb_scale_operation scaleOperation = (vxnne_yuv2rgb_scale_operation)operation;

    vx_image image = scaleOperation->inputs;
    vx_tensor output = scaleOperation->outputs;
    vx_float32 r_mean = scaleOperation->r_mean->value->f32;
    vx_float32 g_mean = scaleOperation->g_mean->value->f32;
    vx_float32 b_mean = scaleOperation->b_mean->value->f32;
    vx_float32 rgb_scale = scaleOperation->rgb_scale->value->f32;
    vx_bool y_only = scaleOperation->y_only->value->b;

    vx_rectangle_t rect = scaleOperation->rect;

    vx_type_e output_format = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_type_e input_format = VX_TYPE_UINT8;

    vx_uint32 input_widths[3], input_heights[3];
    vx_uint32 output_width  = TENSOR_SIZE_INDEX(output, 0);  /* W */
    vx_uint32 output_height = TENSOR_SIZE_INDEX(output, 1);  /* H */

    vx_uint8_ptr input_bases[3], output_base;

    vx_enum out_tf_format     = TENSOR_QUANT_TYPE(output);
    vx_uint32 out_tf_zp       = TENSOR_TF_ZEROPOINT(output);
    vx_float32 out_tf_scale   = TENSOR_TF_SCALE(output);
    vx_int8 out_fixpoint      = TENSOR_POS(output);
    vx_enum out_rounding_mode = TENSOR_ROUNDING_MODE(output);

    vx_float32 width_scale, height_scale;

    vx_uint32 i;

    gcmASSERT(image->format == VX_DF_IMAGE_IYUV);
    gcmASSERT(TENSOR_SIZE_INDEX(output, 2) == 3);

    input_widths[0] = rect.end_x - rect.start_x;
    input_widths[1] = input_widths[0] / image->scales[1][VX_DIM_X];
    input_widths[2] = input_widths[0] / image->scales[2][VX_DIM_X];

    input_heights[0] = rect.end_y - rect.start_y;
    input_heights[1] = input_heights[0] / image->scales[1][VX_DIM_Y];
    input_heights[2] = input_heights[0] / image->scales[2][VX_DIM_Y];

    for (i = 0; i < 3; i++)
    {
        vx_uint32 offset = vxComputePlaneOffset(image, rect.start_x, rect.start_y, i);
        input_bases[i] = image->memory.logicals[i] + offset;
    }

    width_scale = (input_widths[0] * 1.0f) / output_width;
    height_scale = (input_heights[0] * 1.0f) / output_height;

    vxoTensor_GetTensorViewMemory(output, (vx_ptr_ptr)&output_base, VX_NULL);

    {
        vx_uint32 h, w;
        vx_float32 y, u, v, r = .0f, g = .0f, b = .0f;
        vx_float32 data00 = .0f, data01 = .0f, data10 = .0f, data11 = .0f;

        vx_int32 yy, uu, vv;
        vx_int32 post_shift = 8;
        const vx_int32 CST_CY  =  298;
        const vx_int32 CST_CUB =  517;
        const vx_int32 CST_CUG = -100;
        const vx_int32 CST_CVG = -208;
        const vx_int32 CST_CVR =  409;
        vx_int32 c0 =        (vx_int32)(0.5f                              + CST_CY  * rgb_scale);
        vx_int32 c1 =        (vx_int32)(0.5f                              + CST_CVR * rgb_scale);
        vx_int32 c2 = (-1) * (vx_int32)(0.5f                              + CST_CVG * rgb_scale);
        vx_int32 c3 = (-1) * (vx_int32)(0.5f                              + CST_CUG * rgb_scale);
        vx_int32 c4 =        (vx_int32)(0.5f                              + CST_CUB * rgb_scale);
        vx_int32 c5 =        (vx_int32)(0.5f - (56992 + r_mean * (1 << post_shift)) * rgb_scale + (1 << (post_shift - 1)));
        vx_int32 c6 =        (vx_int32)(0.5f + (34784 - g_mean * (1 << post_shift)) * rgb_scale + (1 << (post_shift - 1)));
        vx_int32 c7 =        (vx_int32)(0.5f - (70816 + b_mean * (1 << post_shift)) * rgb_scale + (1 << (post_shift - 1)));

        for (h = 0; h < output_height; h++)
        {
            vx_float32 y_input_h = h * height_scale;
            vx_uint32 yh0 = (vx_int32)y_input_h;
            vx_uint32 yh1 = gcmMIN(yh0 + 1, input_heights[0] - 1);

            for (w = 0; w < output_width; w++)
            {
                vx_float32 y_input_w = w * width_scale;
                vx_int32 yw0 = (vx_int32)y_input_w;
                vx_int32 yw1 = gcmMIN(yw0 + 1, (vx_int32)(input_widths[0] - 1));

                data00 = vxnneGetData((vx_type_e)input_format, yh0 * input_widths[0] + yw0, input_bases[0], 0);
                data01 = vxnneGetData((vx_type_e)input_format, yh0 * input_widths[0] + yw1, input_bases[0], 0);
                data10 = vxnneGetData((vx_type_e)input_format, yh1 * input_widths[0] + yw0, input_bases[0], 0);
                data11 = vxnneGetData((vx_type_e)input_format, yh1 * input_widths[0] + yw1, input_bases[0], 0);

                y = data00 * (1 - (y_input_h - yh0)) * (1 - (y_input_w - yw0)) +
                    data10 * (y_input_h - yh0)       * (1 - (y_input_w - yw0)) +
                    data01 * (1 - (y_input_h - yh0)) * (y_input_w - yw0) +
                    data11 * (y_input_h - yh0)       * (y_input_w - yw0);

                if (!y_only)
                {
                    vx_int32 uw0, uw1, uh0, uh1, vw0, vw1, vh0, vh1;

                    uw0 = yw0 / image->scales[1][VX_DIM_X];
                    uw1 = gcmMIN(uw0 + 1, (vx_int32)(input_widths[1] - 1));
                    uh0 = yh0 / image->scales[1][VX_DIM_Y];
                    uh1 = gcmMIN(uh0 + 1, (vx_int32)(input_heights[1] - 1));

                    data00 = vxnneGetData((vx_type_e)input_format, uh0 * input_widths[1] + uw0, input_bases[1], 0);
                    data01 = vxnneGetData((vx_type_e)input_format, uh0 * input_widths[1] + uw1, input_bases[1], 0);
                    data10 = vxnneGetData((vx_type_e)input_format, uh1 * input_widths[1] + uw0, input_bases[1], 0);
                    data11 = vxnneGetData((vx_type_e)input_format, uh1 * input_widths[1] + uw1, input_bases[1], 0);
                    u = data00 * (1 - (y_input_h - yh0)) * (1 - (y_input_w - yw0)) +
                        data10 * (y_input_h - yh0)       * (1 - (y_input_w - yw0)) +
                        data01 * (1 - (y_input_h - yh0)) * (y_input_w - yw0) +
                        data11 * (y_input_h - yh0)       * (y_input_w - yw0);

                    vw0 = yw0 / image->scales[2][VX_DIM_X];
                    vw1 = gcmMIN(vw0 + 1, (vx_int32)(input_widths[2] - 1));
                    vh0 = yh0 / image->scales[2][VX_DIM_Y];
                    vh1 = gcmMIN(vh0 + 1, (vx_int32)(input_heights[2] - 1));

                    data00 = vxnneGetData((vx_type_e)input_format, vh0 * input_widths[2] + vw0, input_bases[2], 0);
                    data01 = vxnneGetData((vx_type_e)input_format, vh0 * input_widths[2] + vw1, input_bases[2], 0);
                    data10 = vxnneGetData((vx_type_e)input_format, vh1 * input_widths[2] + vw0, input_bases[2], 0);
                    data11 = vxnneGetData((vx_type_e)input_format, vh1 * input_widths[2] + vw1, input_bases[2], 0);
                    v = data00 * (1 - (y_input_h - yh0)) * (1 - (y_input_w - yw0)) +
                        data10 * (y_input_h - yh0)       * (1 - (y_input_w - yw0)) +
                        data01 * (1 - (y_input_h - yh0)) * (y_input_w - yw0) +
                        data11 * (y_input_h - yh0)       * (y_input_w - yw0);

                    {
                        /* hardware conversion, similiar to digital BT601 */
                        yy = (vx_int32)y;
                        uu = (vx_int32)u;
                        vv = (vx_int32)v;
                        r  = (vx_float32)((yy * c0 + vv * c1           + c5) >> post_shift);
                        g  = (vx_float32)((yy * c0 - vv * c2 - uu * c3 + c6) >> post_shift);
                        b  = (vx_float32)((yy * c0           + uu * c4 + c7) >> post_shift);
                    }

                    status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, 0 * output_width * output_height + h * output_width + w, b, output_base, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                    status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, 1 * output_width * output_height + h * output_width + w, g, output_base, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                    status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, 2 * output_width * output_height + h * output_width + w, r, output_base, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                }
                else
                {
                    r = (y - r_mean) * rgb_scale;

                    status |= vxnneSaveDataExt((vx_type_e)output_format, out_tf_format, 0 * output_width * output_height + h * output_width + w, r, output_base, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);
                }
            }
        }
    }

    return status;
}

vx_status vxnneExecuteSCYUV2RGBScale(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_yuv2rgb_scale_operation scaleOperation = (vxnne_yuv2rgb_scale_operation)operation;
    vx_graph graph = operation->layer->node->graph;
    gctUINT8 *stateBuffer = VX_NULL;
    vx_nn_cmd_info_u info;

    vx_image image = scaleOperation->inputs;
    vx_tensor output = scaleOperation->outputs;

    vx_float32 r_mean = ((vx_scalar)operation->layer->node->paramTable[2])->value->f32;
    vx_float32 g_mean = ((vx_scalar)operation->layer->node->paramTable[3])->value->f32;
    vx_float32 b_mean = ((vx_scalar)operation->layer->node->paramTable[4])->value->f32;
    vx_float32 rgb_scale = ((vx_scalar)operation->layer->node->paramTable[5])->value->f32;
    vx_bool y_only = ((vx_scalar)operation->layer->node->paramTable[6])->value->b;

    vx_rectangle_t rect = scaleOperation->rect;
    vx_uint32 scale_x = scaleOperation->x_scale;
    vx_uint32 scale_y = scaleOperation->y_scale;
    vx_uint16 x_init_err = scaleOperation->x_init_error;
    vx_uint16 y_init_err = scaleOperation->y_init_error;
    vx_uint16 x_init_int_err = scaleOperation->x_init_int_error;
    vx_uint16 y_init_int_err = scaleOperation->y_init_int_error;

    vx_uint32 input_address_y, input_address_u, input_address_v;
    vx_uint32 output_address, output_address_r, output_address_g, output_address_b;

    vx_uint32 input_hstride_y = image->memory.strides[0][VX_DIM_Y];

    vx_uint32 output_width  = TENSOR_VIEW_SIZE_INDEX(output, 0);
    vx_uint32 output_height = TENSOR_VIEW_SIZE_INDEX(output, 1);
    vx_uint32 output_stride = TENSOR_STRIDE_INDEX(output, 1);

    vx_uint32 output_bits_size = TENSOR_DATA_SIZE(output) * 8;

    vx_type_e output_format = (vx_type_e)TENSOR_DATA_TYPE(output);
    vx_int8 out_fixpoint      = TENSOR_POS(output);
    vx_enum out_tf_format     = TENSOR_QUANT_TYPE(output);
    vx_uint32 out_tf_zp       = TENSOR_TF_ZEROPOINT(output);
    vx_float32 out_tf_scale   = TENSOR_TF_SCALE(output);

    vx_uint32 input_rect_width, input_rect_height, input_width, input_height;

    vx_uint8 post_shift;
    vx_int32 c0 = 0, c1 = 0, c2 = 0, c3 = 0, c4 = 0, c5 = 0, c6 = 0, c7 = 0;
    vx_float32 fc0 = .0f, fc1 = .0f, fc2 = .0f, fc3 = .0f, fc4 = .0f, fc5 = .0f, fc6 = .0f, fc7 = .0f, fmin, fmax, fr, fg, fb, fp = 1.0f;
    vx_int16 min_r_clamp, max_r_clamp, min_g_clamp, max_g_clamp, min_b_clamp, max_b_clamp;

    gcmASSERT(output_format == VX_TYPE_INT8 || output_format == VX_TYPE_INT16 || output_format == VX_TYPE_UINT8);

    if (output_format == VX_TYPE_UINT8 && out_tf_format != VX_QUANT_AFFINE_SCALE)
    {
        out_tf_scale = 1.0f;
        out_tf_zp = 0;
    }

    if (y_only)
    {
        if (output_format == VX_TYPE_UINT8)
        {
            fc0 = rgb_scale / out_tf_scale;
            fc5 = 0.5f - r_mean * rgb_scale / out_tf_scale + out_tf_zp;
        }
        else /* output_format == VX_TYPE_INT8 || output_format == VX_TYPE_INT16 */
        {
            fp = out_fixpoint > 0 ? (vx_float32)(1 << out_fixpoint) : (1.0f / (vx_float32)(1 << -out_fixpoint));

            fc0 = rgb_scale * fp;
            fc5 = 0.5f - r_mean * rgb_scale * fp;
        }
    }
    else
    {
        const vx_float32 CST_CY  =  298.0f / 256;
        const vx_float32 CST_CUB =  517.0f / 256;
        const vx_float32 CST_CUG = -100.0f / 256;
        const vx_float32 CST_CVG = -208.0f / 256;
        const vx_float32 CST_CVR =  409.0f / 256;
        const vx_float32 CST_R   = 56992.0f / 256;
        const vx_float32 CST_G   = 34784.0f / 256;
        const vx_float32 CST_B   = 70816.0f / 256;

        if (output_format == VX_TYPE_UINT8)
        {
            fc0 =            CST_CY * rgb_scale / out_tf_scale;
            fc1 =           CST_CVR * rgb_scale / out_tf_scale;
            fc2 =           CST_CVG * rgb_scale / out_tf_scale;
            fc3 =           CST_CUG * rgb_scale / out_tf_scale;
            fc4 =           CST_CUB * rgb_scale / out_tf_scale;
            fc5 = 0.5f - (CST_R + r_mean) * rgb_scale / out_tf_scale + out_tf_zp;
            fc6 = 0.5f + (CST_G - g_mean) * rgb_scale / out_tf_scale + out_tf_zp;
            fc7 = 0.5f - (CST_B + b_mean) * rgb_scale / out_tf_scale + out_tf_zp;
        }
        else /* output_format == VX_TYPE_INT8 || output_format == VX_TYPE_INT16 */
        {
            fp = out_fixpoint > 0 ? (vx_float32)(1 << out_fixpoint) : (1.0f / (vx_float32)(1 << -out_fixpoint));

            fc0 =            CST_CY * rgb_scale * fp;
            fc1 =           CST_CVR * rgb_scale * fp;
            fc2 =           CST_CVG * rgb_scale * fp;
            fc3 =           CST_CUG * rgb_scale * fp;
            fc4 =           CST_CUB * rgb_scale * fp;
            fc5 = 0.5f - (CST_R + r_mean) * rgb_scale * fp;
            fc6 = 0.5f + (CST_G - g_mean) * rgb_scale * fp;
            fc7 = 0.5f - (CST_B + b_mean) * rgb_scale * fp;
        }
    }

    fr = (vx_float32)fabs(fc0 * 255 + fc1 * 127 + fc5);
    fg = (vx_float32)fabs(fc0 * 255 + fc3 * 127 + fc2 * 127 + fc6);
    fb = (vx_float32)fabs(fc0 * 255 + fc4 * 127 + fc7);

    fmin = gcmMIN(fr, gcmMIN(fg, fb));

    if (fmin <= 0x07)
      post_shift = 13;
    else if (fmin <= 0x0F)
      post_shift = 12;
    else if (fmin <= 0x1F)
      post_shift = 11;
    else if (fmin <= 0x3F)
      post_shift = 10;
    else if (fmin <= 0x7F)
      post_shift = 9;
    else /* if (fmin <= 0xFF) */
      post_shift = 8;

    fmax = gcmMAX(fc0, gcmMAX(fc1, gcmMAX((fc2 * -1.0f), gcmMAX((fc3 * -1.0f), fc4))));
    if (fmax * (1 << post_shift) >= 1023)
    {
        /* c0 - c4 register is 10 bit */
        post_shift = (vx_uint8)(log(1023.0f / fmax) / log(2.0f));
    }

    c0 = (vx_int32)(fc0 * (1 << post_shift) + 0.5f);
    c1 = (vx_int32)(fc1 * (1 << post_shift) + 0.5f);
    c2 = (vx_int32)-(fc2 * (1 << post_shift) + 0.5f);
    c3 = (vx_int32)-(fc3 * (1 << post_shift) + 0.5f);
    c4 = (vx_int32)(fc4 * (1 << post_shift) + 0.5f);
    c5 = (vx_int32)(fc5 * (1 << post_shift) + 0.5f);
    c6 = (vx_int32)(fc6 * (1 << post_shift) + 0.5f);
    c7 = (vx_int32)(fc7 * (1 << post_shift) + 0.5f);

    gcmASSERT(image->format == VX_DF_IMAGE_IYUV);
    gcmASSERT(output_bits_size == 16 || output_bits_size == 8);

    input_rect_width  = rect.end_x - rect.start_x;
    input_rect_height = rect.end_y - rect.start_y;

    input_width  = image->region.start_x > image->region.end_x ? image->memory.dims[0][VX_DIM_X] : image->region.end_x - image->region.start_x;
    input_height = image->region.start_y > image->region.end_y ? image->memory.dims[0][VX_DIM_Y] : image->region.end_y - image->region.start_y;

    gcmASSERT(input_rect_width <= input_width);
    gcmASSERT(input_rect_height <= input_height);

    input_address_y = image->memory.physicals[0] + vxComputePlaneOffset(image, rect.start_x, rect.start_y, 0);
    input_address_u = image->memory.physicals[1] + vxComputePlaneOffset(image, rect.start_x, rect.start_y, 1);
    input_address_v = image->memory.physicals[2] + vxComputePlaneOffset(image, rect.start_x, rect.start_y, 2);

    vxoTensor_GetTensorViewMemory(output, VX_NULL, &output_address);
    output_address_r = output_address;
    output_address_g = output_address + TENSOR_STRIDE_INDEX(output, 2) * 1;
    output_address_b = output_address + TENSOR_STRIDE_INDEX(output, 2) * 2;

    {
        vx_float32 min = 0;
        vx_float32 max = 255;
        vx_float32 min_r = (min - r_mean) * rgb_scale;
        vx_float32 max_r = (max - r_mean) * rgb_scale;
        vx_float32 min_g = (min - g_mean) * rgb_scale;
        vx_float32 max_g = (max - g_mean) * rgb_scale;
        vx_float32 min_b = (min - b_mean) * rgb_scale;
        vx_float32 max_b = (max - b_mean) * rgb_scale;

        if (output_format == VX_TYPE_UINT8)
        {
            min_r_clamp = (vx_int16)gcmMAX(0, vxnneRound(min_r / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            max_r_clamp = (vx_int16)gcmMIN(255, vxnneRound(max_r / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            min_g_clamp = (vx_int16)gcmMAX(0, vxnneRound(min_g / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            max_g_clamp = (vx_int16)gcmMIN(255, vxnneRound(max_g / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            min_b_clamp = (vx_int16)gcmMAX(0, vxnneRound(min_b / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            max_b_clamp = (vx_int16)gcmMIN(255, vxnneRound(max_b / out_tf_scale + out_tf_zp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
        }
        else
        {
            if (output_format == VX_TYPE_INT8)
            {
                min_r_clamp = (vx_int16)gcmMAX(-128,vxnneRound(min_r * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_r_clamp = (vx_int16)gcmMIN(127,vxnneRound(max_r * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                min_g_clamp = (vx_int16)gcmMAX(-128,vxnneRound(min_g * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_g_clamp = (vx_int16)gcmMIN(127,vxnneRound(max_g * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                min_b_clamp = (vx_int16)gcmMAX(-128,vxnneRound(min_b * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_b_clamp = (vx_int16)gcmMIN(127,vxnneRound(max_b * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            }
            else /* INT16 */
            {
                min_r_clamp = (vx_int16)gcmMAX(-32768,vxnneRound(min_r * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_r_clamp = (vx_int16)gcmMIN(32767,vxnneRound(max_r * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                min_g_clamp = (vx_int16)gcmMAX(-32768,vxnneRound(min_g * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_g_clamp = (vx_int16)gcmMIN(32767,vxnneRound(max_g * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                min_b_clamp = (vx_int16)gcmMAX(-32768,vxnneRound(min_b * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
                max_b_clamp = (vx_int16)gcmMIN(32767,vxnneRound(max_b * fp, VX_NN_ROUNDING_MODE_SIMPLE_ROUNDING));
            }
        }
    }

    /* send command */
    memset(&info, 0, sizeof(vx_nn_cmd_info_u));

    info.vx_yuv2rgb_scaler_cmd_info.inImageBaseY     = input_address_y;
    info.vx_yuv2rgb_scaler_cmd_info.inImageBaseU     = input_address_u;
    info.vx_yuv2rgb_scaler_cmd_info.inImageBaseV     = input_address_v;

    info.vx_yuv2rgb_scaler_cmd_info.inRectX          = (vx_uint16)rect.start_x;
    info.vx_yuv2rgb_scaler_cmd_info.inRectY          = (vx_uint16)rect.start_y;

    info.vx_yuv2rgb_scaler_cmd_info.inRectWidth      = (vx_uint16)input_rect_width;
    info.vx_yuv2rgb_scaler_cmd_info.inRectHeight     = (vx_uint16)input_rect_height;

    info.vx_yuv2rgb_scaler_cmd_info.inImageWidth     = (vx_uint16)input_width;
    info.vx_yuv2rgb_scaler_cmd_info.inImageHeight    = (vx_uint16)input_height;

    info.vx_yuv2rgb_scaler_cmd_info.inImageStrideY   = (vx_uint16)input_hstride_y;

    info.vx_yuv2rgb_scaler_cmd_info.outImageBaseR    = !y_only ? output_address_b : output_address_r;
    info.vx_yuv2rgb_scaler_cmd_info.outImageBaseG    = output_address_g;
    info.vx_yuv2rgb_scaler_cmd_info.outImageBaseB    = !y_only ? output_address_r : output_address_b;

    info.vx_yuv2rgb_scaler_cmd_info.outImageWidth    = (vx_uint16)output_width;
    info.vx_yuv2rgb_scaler_cmd_info.outImageHeight   = (vx_uint16)output_height;

    info.vx_yuv2rgb_scaler_cmd_info.outImageStride   = (vx_uint16)output_stride;
    info.vx_yuv2rgb_scaler_cmd_info.outImageBitsSize = (vx_uint16)output_bits_size;

    info.vx_yuv2rgb_scaler_cmd_info.scaleX           = scale_x;
    info.vx_yuv2rgb_scaler_cmd_info.scaleY           = scale_y;

    info.vx_yuv2rgb_scaler_cmd_info.inImageInitErrX     = x_init_err;
    info.vx_yuv2rgb_scaler_cmd_info.inImageInitErrY     = y_init_err;
    info.vx_yuv2rgb_scaler_cmd_info.inImageInitIntErrX  = x_init_int_err;
    info.vx_yuv2rgb_scaler_cmd_info.inImageInitIntErrY  = y_init_int_err;

    info.vx_yuv2rgb_scaler_cmd_info.yOnly            = y_only ? 1 : 0;
    info.vx_yuv2rgb_scaler_cmd_info.outSigned        = TENSOR_DATA_TYPE(output) == VX_TYPE_INT8 || TENSOR_DATA_TYPE(output) == VX_TYPE_INT16 ? 1 : 0;
    info.vx_yuv2rgb_scaler_cmd_info.postShift        = post_shift;

    info.vx_yuv2rgb_scaler_cmd_info.c0               = (vx_uint16)c0;
    info.vx_yuv2rgb_scaler_cmd_info.c1               = (vx_uint16)c1;
    info.vx_yuv2rgb_scaler_cmd_info.c2               = (vx_uint16)c2;
    info.vx_yuv2rgb_scaler_cmd_info.c3               = (vx_uint16)c3;
    info.vx_yuv2rgb_scaler_cmd_info.c4               = (vx_uint16)c4;
    info.vx_yuv2rgb_scaler_cmd_info.c5               = c5;
    info.vx_yuv2rgb_scaler_cmd_info.c6               = c6;
    info.vx_yuv2rgb_scaler_cmd_info.c7               = c7;

    info.vx_yuv2rgb_scaler_cmd_info.minRClamp        = min_r_clamp;
    info.vx_yuv2rgb_scaler_cmd_info.maxRClamp        = max_r_clamp;
    info.vx_yuv2rgb_scaler_cmd_info.minGClamp        = min_g_clamp;
    info.vx_yuv2rgb_scaler_cmd_info.maxGClamp        = max_g_clamp;
    info.vx_yuv2rgb_scaler_cmd_info.minBClamp        = min_b_clamp;
    info.vx_yuv2rgb_scaler_cmd_info.maxBClamp        = max_b_clamp;

    /*Per HW suggestion, default set 32*/
    {
        gctSTRING envctrl = gcvNULL;
        if (gcmIS_SUCCESS(gcoOS_GetEnv(gcvNULL, "SCALER_OUTSTANDING_REQUEST", &envctrl)) && envctrl)
        {
            info.vx_yuv2rgb_scaler_cmd_info.outRequestCount = atoi(envctrl);
        }
        else
        {
            info.vx_yuv2rgb_scaler_cmd_info.outRequestCount  = 32;
        }
        vxInfo("YUV2RGB scaler outstanding request is %d\n", info.vx_yuv2rgb_scaler_cmd_info.outRequestCount);
    }

    if (graph->binarySave)
    {
        vxmONERROR(gcoOS_Allocate(gcvNULL, VX_MAX_SC_OPERATION_STATE_SIZE, (gctPOINTER *)&stateBuffer));
        status = gcfVX_CaptureState(stateBuffer,
                                    VX_MAX_SC_OPERATION_STATE_SIZE,
                                    gcvNULL,
                                    gcvTRUE, gcvFALSE);
        if (status != VX_SUCCESS)
        {
            vxError("fail to capture scale states\n");
            vxmONERROR(VX_FAILURE);
        }
    }

    status = gcoVX_ProgrammYUV2RGBScale((void*)&info, operation->gpuId, operation->mGpuSync);

    if (graph->binarySave)
    {
        vx_node node = operation->layer->node;
        gctUINT32 actualSize = 0;

        status = gcfVX_CaptureState(gcvNULL, 0, &actualSize, gcvFALSE, gcvFALSE);
        if (actualSize <= 0)
        {
            vxError("error: fail to save layer name : %s to binary in SC operation\n", node->layer->name);
            vxmONERROR(VX_FAILURE);
        }

        vxmONERROR(vxoBinaryGraph_SaveScalerOperation(operation, stateBuffer, actualSize));
    }

OnError:
    if (stateBuffer != VX_NULL)
    {
        gcmVERIFY_OK(gcmOS_SAFE_FREE(gcvNULL, stateBuffer));
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNYUV2RGBScale(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    status = VX_SUCCESS;

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoYUV2RGBScale_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoYUV2RGBScale_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoYUV2RGBScale_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_image   image                      = (vx_image)parameters[0];
    vx_array   rects                      = (vx_array)parameters[1];
    vx_scalar  r_mean                     = (vx_scalar)parameters[2];
    vx_scalar  g_mean                     = (vx_scalar)parameters[3];
    vx_scalar  b_mean                     = (vx_scalar)parameters[4];
    vx_scalar  rgb_scale                  = (vx_scalar)parameters[5];
    vx_scalar  y_only                     = (vx_scalar)parameters[6];
    vx_tensor  outputs                    = (vx_tensor)parameters[7];

    vx_rectangle_t rect;
    vx_uint32 input_rect_width, input_rect_height, output_width, output_height, scale_x, scale_y;

    vxnne_yuv2rgb_scale_layer  yuv2rgb_scaleLayer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_yuv2rgb_scale_layer_s), (gctPOINTER*)&yuv2rgb_scaleLayer);
    if (!yuv2rgb_scaleLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(yuv2rgb_scaleLayer, sizeof(vxnne_yuv2rgb_scale_layer_s));

    vxnneLayer_Initialize(&yuv2rgb_scaleLayer->base,
                          "YUV2RGBScale",
                          node,
                          vxmOPERATION_COUNT(yuv2rgb_scaleLayer),
                          yuv2rgb_scaleLayer->operations,
                          VX_NULL);

    rect.start_x = *((vx_uint32_ptr)rects->memory.logicals[0] + 0);
    rect.start_y = *((vx_uint32_ptr)rects->memory.logicals[0] + 1);
    rect.end_x   = *((vx_uint32_ptr)rects->memory.logicals[0] + 2);
    rect.end_y   = *((vx_uint32_ptr)rects->memory.logicals[0] + 3);

    if (!rect.end_x || rect.start_x >= rect.end_x)
    {
        rect.start_x = 0;
        rect.end_x = image->memory.dims[0][VX_DIM_X];
    }
    if (!rect.end_y || rect.start_y >= rect.end_y)
    {
        rect.start_y = 0;
        rect.end_y = image->memory.dims[0][VX_DIM_Y];
    }
    if (rect.end_x > (vx_uint32)image->memory.dims[0][VX_DIM_X]) rect.end_x = image->memory.dims[0][VX_DIM_X];
    if (rect.end_y > (vx_uint32)image->memory.dims[0][VX_DIM_Y]) rect.end_y = image->memory.dims[0][VX_DIM_Y];
    if (rect.start_x > rect.end_x) rect.start_x = 0;
    if (rect.start_y > rect.end_y) rect.start_y = 0;

    input_rect_width  = rect.end_x - rect.start_x;
    input_rect_height = rect.end_y - rect.start_y;

    output_width  = TENSOR_SIZE_INDEX(outputs, 0);
    output_height = TENSOR_SIZE_INDEX(outputs, 1);

    scale_x = (input_rect_width << 15) / output_width;
    scale_y = (input_rect_height << 15) / output_height;

    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SCALER))
    {
        vxnneOperation_Initialize(
            &yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.base,
            &yuv2rgb_scaleLayer->base,
            VXNNE_OPERATION_TARGET_SC,
            VXNNE_OPERATOR_YUV2RGB_SCALE,
            vxnneExecuteSCYUV2RGBScale,
            VX_NULL,
            1,
            0);

        vxnneLayer_SetOperation(
            &yuv2rgb_scaleLayer->base,
            &yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.base,
            0);

        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.inputs     = image;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.r_mean     = r_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.g_mean     = g_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.b_mean     = b_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.rgb_scale  = rgb_scale;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.y_only     = y_only;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.rect       = rect;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.x_scale    = scale_x;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.y_scale    = scale_y;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.outputs    = outputs;

        vxnneOperation_AddReference(&yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.base, (vx_reference)image, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&yuv2rgb_scaleLayer->yuv2rgb_scale_sc_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else
    {
        vxnneOperation_Initialize(
            &yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.base,
            &yuv2rgb_scaleLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_YUV2RGB_SCALE,
            vxnneExecuteSWYUV2RGBScale,
            VX_NULL,
            1,
            0);

        vxnneLayer_SetOperation(
            &yuv2rgb_scaleLayer->base,
            &yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.base,
            0);

        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.inputs    = image;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.r_mean    = r_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.g_mean    = g_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.b_mean    = b_mean;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.rgb_scale = rgb_scale;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.y_only    = y_only;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.rect      = rect;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.x_scale   = scale_x;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.y_scale   = scale_y;
        yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.outputs   = outputs;

        vxnneOperation_AddReference(&yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.base, (vx_reference)image, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&yuv2rgb_scaleLayer->yuv2rgb_scale_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &yuv2rgb_scaleLayer->base;

    return status;

}

VX_PRIVATE_API vx_status VX_CALLBACK vxoYUV2RGBScale_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}


vx_float32 vxnneActivation(vx_enum func_v, vx_float32 a_v, vx_float32 b_v, vx_float32 value)
{
    vx_float64 result = value;

    switch (func_v)
    {
    case VX_NN_ACTIVATION_LOGISTIC:
        {
            result = 1.0f / (1 + gcoMATH_Exp(value * (-1)));
        }
        break;

    case VX_NN_ACTIVATION_HYPERBOLIC_TAN:
        {
            result = a_v * gcoMATH_TangentH(b_v * value);
        }
        break;

    case VX_NN_ACTIVATION_RELU:
        {
            result = gcoMATH_MAX(0.0f, value);
        }
        break;

    case VX_NN_ACTIVATION_BRELU:
        {
            result = gcoMATH_MIN(a_v, gcoMATH_MAX(0.0f, value));
        }
        break;

    case VX_NN_ACTIVATION_SOFTRELU:
        {
            result = gcoMATH_Log(1 + gcoMATH_Exp(value));
        }
        break;

    case VX_NN_ACTIVATION_ABS:
        {
            result = gcoMATH_Absolute(value);
        }
        break;

    case VX_NN_ACTIVATION_SQUARE:
        {
            result = gcoMATH_Power(value, 2);
        }
        break;

    case VX_NN_ACTIVATION_SQRT:
        {
            result = gcoMATH_SquareRoot(value);
        }
        break;

    case VX_NN_ACTIVATION_LINEAR:
        {
            result = a_v * value + b_v;
        }
        break;

    case VX_NN_ACTIVATION_LEAKYRELU:
        {
            result = (value > 0.0f) ? value : 0.1f * value;
        }
        break;

    case VX_NN_ACTIVATION_RELU6:
        {
            result = gcoMATH_MIN(gcoMATH_MAX(value, 0), 6);
        }
        break;

    case VX_NN_ACTIVATION_RELU1:
        {
            result = gcoMATH_MIN(gcoMATH_MAX(value, -1), 1);
        }
        break;

    case VX_NN_ACTIVATION_RSQRT:
        {
            result = gcoMATH_ReciprocalSquareRoot(value);
        }
        break;

    default:
        vxError("this activation func not support");
        break;
    }
    return (vx_float32)result;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNSoftmaxLayer2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  beta                       = (vx_scalar)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];
    vx_float32 betaVal                    = beta->value->f32;
    vx_bool    useShadeExe                = vx_false_e;
    vx_bool    enable_format              = vx_false_e;
    vx_bool    enable_tf_quantize         = vx_false_e;
    vx_bool    enable_float32             = vx_false_e;
    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);
    vx_enum    srcFormat                  = TENSOR_DATA_TYPE(inputs);
    vx_enum    dstFormat                  = TENSOR_DATA_TYPE(outputs);
    vx_uint32  dims                       = TENSOR_DIM_NUM(inputs);
    vx_uint32  width                      = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32  height                     = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
    vx_uint32  idx                        = 0;
    vx_uint32  numTmpTensor               = 0;
    vxnne_softmax2_layer  softmax2Layer   = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_softmax2_layer_s), (gctPOINTER*)&softmax2Layer);
    if (!softmax2Layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(softmax2Layer, sizeof(vxnne_softmax2_layer_s));

    vxnneLayer_Initialize(&softmax2Layer->base,
                          "SoftMax2",
                          node,
                          vxmOPERATION_COUNT(softmax2Layer),
                          softmax2Layer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enable_float32 = (vx_bool)(dstFormat == VX_TYPE_FLOAT32 && ((width % 4 == 0) || ((width * height < IMG_MAX_WIDTH) && ((width * height % 4 == 0) || dims < 3)) || dims == 1));
        enable_format = (((srcFormat == VX_TYPE_INT8 ||  srcFormat == VX_TYPE_FLOAT16) && (dstFormat == VX_TYPE_FLOAT16 || enable_float32)) || (srcFormat == VX_TYPE_INT16 && (dstFormat == VX_TYPE_INT16 || dstFormat == VX_TYPE_FLOAT16))
                         || (srcFormat == VX_TYPE_INT8 &&  dstFormat == VX_TYPE_INT8));
        enable_tf_quantize = ((srcFormat == VX_TYPE_UINT8) && (dstFormat == VX_TYPE_FLOAT16 || enable_float32 || dstFormat == VX_TYPE_UINT8));
    }
    else
    {
        enable_format = ((srcFormat == VX_TYPE_FLOAT32 ||  srcFormat == VX_TYPE_FLOAT16) && (dstFormat == VX_TYPE_FLOAT16 || dstFormat == VX_TYPE_FLOAT32));
        enable_tf_quantize = ((srcFormat == VX_TYPE_UINT8) && (dstFormat == VX_TYPE_FLOAT16 || dstFormat == VX_TYPE_FLOAT32 || dstFormat == VX_TYPE_UINT8));
    }

    /* Current the SH layer only process 3D tensor*/
    useShadeExe  = (enable_format || enable_tf_quantize);

    if(useShadeExe && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetSoftmaxShaderExecutable(node->base.context, VXNNE_KERNEL_SOFTMAX, &node->kernelAttributes.borderMode, dims, inputs, betaVal, outputs);
            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&softmax2Layer->softmax2_SHoperation,
                &softmax2Layer->base,
                VXNNE_OPERATOR_SOFTMAX,
                batchCount,
                shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&softmax2Layer->softmax2_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&softmax2Layer->softmax2_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                &softmax2Layer->base,
                &softmax2Layer->softmax2_SHoperation.base,
                0);
        }
        else
        {
            shaderExecutable = vxnneGetGPUSoftmaxShaderExecutable(node->base.context, VXNNE_KERNEL_SOFTMAX, &node->kernelAttributes.borderMode, beta, inputs, outputs);
            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&softmax2Layer->softmax2_SHoperation,
                &softmax2Layer->base,
                VXNNE_OPERATOR_SOFTMAX,
                batchCount,
                shaderExecutable);
            if (status != VX_SUCCESS) goto exit;

            vxnneOperation_AddReference(&softmax2Layer->softmax2_SHoperation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&softmax2Layer->softmax2_SHoperation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(
                &softmax2Layer->base,
                &softmax2Layer->softmax2_SHoperation.base,
                idx++);
        }
    }
    else
    {
        vxnneOperation_Initialize(&softmax2Layer->softmax2_sw_operation.base,
            &softmax2Layer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_SOFTMAX,
            vxnneExecuteSWSoftmax,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &softmax2Layer->base,
            &softmax2Layer->softmax2_sw_operation.base,
            0);

        softmax2Layer->softmax2_sw_operation.inputs           = inputs;
        softmax2Layer->softmax2_sw_operation.beta             = beta;
        softmax2Layer->softmax2_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&softmax2Layer->softmax2_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&softmax2Layer->softmax2_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    softmax2Layer->base.num_temp_tensors = numTmpTensor;
    node->layer = &softmax2Layer->base;

    return status;
exit:
    if (softmax2Layer) gcoOS_Free(gcvNULL, (gctPOINTER)softmax2Layer);
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoSoftmaxLayer2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 LUT2
 ***************************************************************************************************************************/
vx_status vxnneExecuteSWLUT2(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_lut2_operation lutOperation = (vxnne_lut2_operation)operation;

    vx_tensor input  = lutOperation->inputs;
    vx_tensor lut  = lutOperation->lut;
    vx_tensor output = lutOperation->outputs;

    vx_int32 input_index = TENSOR_SIZE_INDEX(input, 0);

    vx_int32 lut_index = TENSOR_SIZE_INDEX(lut, 2);
    vx_int32 lut_width = TENSOR_SIZE_INDEX(lut, 0);
    vx_int32 lut_height = TENSOR_SIZE_INDEX(lut, 1);

    vx_int32 i = 0, index = 0, stride = lut_width * lut_height * TENSOR_DATA_SIZE(lut);

    for (i = 0; i < input_index; i ++)
    {
        index = (vx_int32)VX_GET_DATA_FROM_TENSOR(input, i);

        if (index >= 0 && index < lut_index)
        {
            memcpy(TENSOR_LOGICAL_ADDR(output) + i * stride, TENSOR_LOGICAL_ADDR(lut) + index * stride, stride);
        }
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNLUT2(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLUT2_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoLUT2_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLUT2_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_tensor  lut                        = (vx_tensor)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];

    vx_uint32  batchCount                 = TENSOR_SIZE_INDEX(inputs, 3);

    vxnne_lut2_layer  lut2Layer           = VX_NULL;
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_enum    valueFormat                = TENSOR_DATA_TYPE(lut);
    vx_bool    dataFormat_flag            = vx_false_e;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_lut2_layer_s), (gctPOINTER*)&lut2Layer);
    if (!lut2Layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("Out of Memory at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    if(node->base.context->evisNoInst.supportEVIS)
    {
        if ((valueFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            || (valueFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16))
            dataFormat_flag = vx_true_e;
    }
    else
    {
        if ((valueFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
            || (valueFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
            || (valueFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32))
            dataFormat_flag = vx_true_e;
    }

    gcoOS_ZeroMemory(lut2Layer, sizeof(vxnne_lut2_layer_s));

    vxnneLayer_Initialize(&lut2Layer->base,
                          "LUT2",
                          node,
                          vxmOPERATION_COUNT(lut2Layer),
                          lut2Layer->operations,
                          VX_NULL);

    if (dataFormat_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        if(node->base.context->evisNoInst.supportEVIS)
        {
            shaderExecutable = vxnneGetEmbeddingLUTShaderExecutable(node->base.context, VXNNE_KERNEL_EMBEDDINGLUT,
                &node->kernelAttributes.borderMode, inputs, lut, outputs);
        }
        else
        {
            shaderExecutable = vxnneGetGPUEmbeddingLUTShaderExecutable(node->base.context, VXNNE_KERNEL_EMBEDDINGLUT,
                &node->kernelAttributes.borderMode, inputs, lut, outputs);
        }

        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto exit;
        }

        status = vxnneShaderOperation_Initialize(&lut2Layer->lut2_sh_operation,
            &lut2Layer->base,
            VXNNE_OPERATOR_LUT2,
            batchCount,
            shaderExecutable);
        if (status != VX_SUCCESS)
            goto exit;

        vxnneOperation_AddReference(&lut2Layer->lut2_sh_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&lut2Layer->lut2_sh_operation.base, (vx_reference)lut, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&lut2Layer->lut2_sh_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        vxnneLayer_SetOperation(
            &lut2Layer->base,
            &lut2Layer->lut2_sh_operation.base,
            0);
    }
    else
    {
        vxnneOperation_Initialize(&lut2Layer->lut2_sw_operation.base,
                                  &lut2Layer->base,
                                  VXNNE_OPERATION_TARGET_SW,
                                  VXNNE_OPERATOR_LUT2,
                                  vxnneExecuteSWLUT2,
                                  VX_NULL,
                                  batchCount,
                                  0);

        vxnneLayer_SetOperation(
            &lut2Layer->base,
            &lut2Layer->lut2_sw_operation.base,
            0);

        lut2Layer->lut2_sw_operation.inputs           = inputs;
        lut2Layer->lut2_sw_operation.lut              = lut;
        lut2Layer->lut2_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&lut2Layer->lut2_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&lut2Layer->lut2_sw_operation.base, (vx_reference)lut, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&lut2Layer->lut2_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    node->layer = &lut2Layer->base;

    return status;

exit:
    if (lut2Layer) {
        gcoOS_Free(VX_NULL, (gctPOINTER)lut2Layer);
        lut2Layer = VX_NULL;
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoLUT2_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
 *                                                 Adapter
 ***************************************************************************************************************************/
vx_status vxnneAdapter_SWCWHN2WHCN(
            vx_uint8_ptr input_ptr, vx_type_e input_format, vx_enum input_quant_type, vx_uint32 input_depth, vx_uint32 input_width, vx_uint32 input_height,
            vx_uint32 input_batch, vx_int8 in_fixpoint, vx_int32 in_tf_zp, vx_float32 in_tf_scale,
            vx_uint8_ptr output_ptr, vx_type_e output_format, vx_enum output_quant_type, vx_uint32 output_depth, vx_uint32 output_width, vx_uint32 output_height,
            vx_int8 out_fixpoint, vx_int32 out_tf_zp, vx_float32 out_tf_scale, vx_enum out_rounding_mode)
{
    vx_status status = VX_SUCCESS;

    vx_uint32 batch = 0, in_h = 0, in_w = 0;
    vx_float32 data = .0f;
    /*vx_int32 in_item_size = vxnneGetTypeSize(input_format); */

    /**************************************************************************************************
     *       C W H N                                      W H C N
     *       2 4 4 1             =>                       4 4 2 1
     *   ___________________            ___________________         ___________________
     *  |10, |11, |12, |13, |          | 10 | 11 | 12 | 13 |       | 20 | 21 | 22 | 23 |
     *  |__20|__21|__22|__23|          |____|____|____|____|       |____|____|____|____|
     *  |14, |15, |16, |17, |          | 14 | 15 | 16 | 17 |       | 24 | 25 | 26 | 27 |
     *  |__24|__25|__26|__27|    =>    |____|____|____|____|       |____|____|____|____|
     *  |18, |19, |110,|111,|          | 18 | 19 | 110| 111|       | 28 | 29 | 210| 211|
     *  |__28|__29|_210|_211|          |____|____|____|____|       |____|____|____|____|
     *  |112,|113,|114,|115,|          | 112| 113| 114| 115|       | 212| 213| 214| 215|
     *  |_212|_213|_214|_215|          |____|____|____|____|       |____|____|____|____|
     *
     **************************************************************************************************/

    for (batch = 0; batch < input_batch; ++ batch)
    {
        vx_uint32 output_batch_index = batch * output_height * output_width * output_depth;
        vx_uint32 input_batch_index = batch * input_height * input_width * input_depth;

        {
            for (in_h = 0; in_h < input_height; ++ in_h)
            {
                for (in_w = 0; in_w < (input_width * input_depth); in_w ++)
                {
                    vx_int32 out_w = in_w / input_depth;
                    vx_int32 out_h = in_h;
                    vx_int32 out_d = in_w % input_depth;

                    vx_int32 in_index = in_w + in_h * input_width * input_depth + input_batch_index;
                    vx_int32 out_index = (out_w + out_h * output_width) + out_d * output_width * output_height + output_batch_index;

                    /*comment the direct copy, because of dst's quantized parameter may be different from src's*/
                    /*if (in_item_size == vxnneGetTypeSize(output_format))
                    {
                        memcpy(output_ptr + out_index * in_item_size, input_ptr + in_index * in_item_size, in_item_size);
                    }
                    else*/
                    {
                        data = vxnneGetDataExt(input_format, input_quant_type, in_index, input_ptr, in_fixpoint, in_tf_zp, in_tf_scale);

                        vxnneSaveDataExt(output_format, output_quant_type, out_index, data, output_ptr, out_fixpoint, out_tf_zp, out_tf_scale, out_rounding_mode);


                    }
                }
            }
        }
    }

    return status;
}

vx_status vxnneAdapter_Tensor_CWHN2WHCN(vx_tensor inputs, vx_tensor outputs)
{
    vx_status status = VX_SUCCESS;


    vx_type_e  inputFormat      = (vx_type_e)TENSOR_DATA_TYPE(inputs);
    vx_type_e  outputFormat     = (vx_type_e)TENSOR_DATA_TYPE(outputs);

    vx_uint8_ptr inputBase;
    vx_uint8_ptr outputBase;


    vx_uint32 input_batch = TENSOR_SIZE_INDEX(inputs, 0);  /* N */
    vx_uint32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    vx_uint32 input_width = TENSOR_SIZE_INDEX(inputs, 2);  /* W */
    vx_uint32 input_depth = TENSOR_SIZE_INDEX(inputs, 3);  /* C */

    vx_uint32 output_width = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
    vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    vx_uint32 output_depth = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
    vx_uint32 output_batch = TENSOR_SIZE_INDEX(outputs, 3);  /* N */


    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);

    gcmASSERT(input_width == output_width);
    gcmASSERT(input_height == output_height);
    gcmASSERT(output_depth == input_depth);
    if (output_batch != input_batch)
    {
        gcmASSERT(0);
    }

    //outputBase = TENSOR_LOGICAL_ADDR(outputs);
    //inputBase = TENSOR_LOGICAL_ADDR(inputs);


    status = vxnneAdapter_SWCWHN2WHCN(inputBase, inputFormat, TENSOR_QUANT_TYPE(inputs), input_depth, input_width, input_height, input_batch, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs),
        outputBase, outputFormat, TENSOR_QUANT_TYPE(outputs), output_depth, output_width, output_height, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));

    return status;
}

vx_status vxnneAdapter_CWHN2WHCN(struct _vxnne_operation_s *operation)
{

    vxnne_adapter_operation adapterOperation   = (vxnne_adapter_operation)operation;

    vx_tensor  inputs           = (vx_tensor)adapterOperation->inputs;
    vx_tensor  outputs          = (vx_tensor)adapterOperation->outputs;

    return vxnneAdapter_Tensor_CWHN2WHCN(inputs, outputs);
}


vx_status vxnneAdapter_WHCN2CWHN(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;

    vxnne_adapter_operation adapterOperation   = (vxnne_adapter_operation)operation;

    vx_tensor  inputs           = (vx_tensor)adapterOperation->inputs;
    vx_tensor  outputs          = (vx_tensor)adapterOperation->outputs;


    vx_uint8_ptr inputBase;
    vx_uint8_ptr outputBase;


    vx_uint32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    vx_uint32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    vx_uint32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    vx_uint32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */

    vx_uint32 output_batch = TENSOR_SIZE_INDEX(outputs, 0);  /* N */
    vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
    vx_uint32 output_width = TENSOR_SIZE_INDEX(outputs, 2);  /* W */
    vx_uint32 output_depth = TENSOR_SIZE_INDEX(outputs, 3);  /* C */

    /*vx_int32 item_size = vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(outputs));*/

    vx_uint32 batch = 0, in_h = 0, in_w = 0, in_d = 0;
    vx_float32 data = .0f;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER*)&inputBase, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER*)&outputBase, VX_NULL);

    gcmASSERT(input_width == output_width);
    gcmASSERT(input_height == output_height);
    gcmASSERT(output_depth == input_depth);
    if (output_batch != input_batch)
    {
        gcmASSERT(0);
    }
    //outputBase = TENSOR_LOGICAL_ADDR(outputs);
    //inputBase = TENSOR_LOGICAL_ADDR(inputs);


    /**************************************************************************************************
     *                       W H C N                                        C W H N
     *                       4 4 2 1                          =>            2 4 4 1
     *   ___________________         ___________________               ___________________
     *  | 10 | 11 | 12 | 13 |       | 20 | 21 | 22 | 23 |             |10, |11, |12, |13, |
     *  |____|____|____|____|       |____|____|____|____|             |__20|__21|__22|__23|
     *  | 14 | 15 | 16 | 17 |       | 24 | 25 | 26 | 27 |             |14, |15, |16, |17, |
     *  |____|____|____|____|       |____|____|____|____|     =>      |__24|__25|__26|__27|
     *  | 18 | 19 | 110| 111|       | 28 | 29 | 210| 211|             |18, |19, |110,|111,|
     *  |____|____|____|____|       |____|____|____|____|             |__28|__29|_210|_211|
     *  | 112| 113| 114| 115|       | 212| 213| 214| 215|             |112,|113,|114,|115,|
     *  |____|____|____|____|       |____|____|____|____|             |_212|_213|_214|_215|
     *
     **************************************************************************************************/

    for (batch = 0; batch < input_batch; ++ batch)
    {
        vx_uint32 output_batch_index = batch * output_height * output_width * output_depth;
        vx_uint32 input_batch_index = batch * input_height * input_width * input_depth;

        for (in_d = 0; in_d < input_depth; in_d ++)
        {
            for (in_h = 0; in_h < input_height; ++ in_h)
            {
                for (in_w = 0; in_w < input_width; in_w ++)
                {
                    vx_int32 out_w = in_w * input_depth;
                    vx_int32 out_h = in_h;

                    vx_int32 in_index = in_w + in_h * input_width + in_d * input_width * input_height + input_batch_index;

                    vx_int32 out_index = out_w + in_d + out_h * output_width * input_depth + output_batch_index;

                    /*comment the direct copy, because of dst's quantized parameter may be different from src's*/
                    /*if (item_size == vxnneGetTypeSize((vx_type_e)TENSOR_DATA_TYPE(inputs)))
                    {
                        memcpy(outputBase + out_index * item_size, inputBase + in_index * item_size, item_size);
                    }
                    else*/
                    {

                        data = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(inputs), TENSOR_QUANT_TYPE(inputs), in_index, inputBase, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
                        vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(outputs), TENSOR_QUANT_TYPE(outputs), out_index, data, outputBase, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
                    }
                }
            }
        }
    }
    return status;
}


vx_status vxnneAdapter_Tensor_FormatConvert(vx_tensor inputs, vx_tensor outputs)
{
    vx_status status = VX_SUCCESS;
    vx_uint32 i = 0;
    vx_float32 data = 0;
    const vx_uint32 input_width = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
    const vx_uint32 input_height = TENSOR_SIZE_INDEX(inputs, 1); /* H */
    const vx_uint32 input_depth = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
    const vx_uint32 input_batch = TENSOR_SIZE_INDEX(inputs, 3);  /* N */
    vx_uint8_ptr input_base = VX_NULL, output_base = VX_NULL;

    vxoTensor_GetTensorViewMemory(inputs, (gctPOINTER *)&input_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(outputs, (gctPOINTER *)&output_base, VX_NULL);

    for (i = 0; i < input_width * input_height * input_depth * input_batch; i ++)
    {
        data = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(inputs), TENSOR_QUANT_TYPE(inputs), i, input_base, TENSOR_POS(inputs), TENSOR_TF_ZEROPOINT(inputs), TENSOR_TF_SCALE(inputs));
        status |= vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(outputs), TENSOR_QUANT_TYPE(outputs), i, data, output_base, TENSOR_POS(outputs), TENSOR_TF_ZEROPOINT(outputs), TENSOR_TF_SCALE(outputs), TENSOR_ROUNDING_MODE(outputs));
    }

    return status;
}

vx_status vxnneExecuteSWAdapter(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_adapter_operation adapterOperation   = (vxnne_adapter_operation)operation;

    vx_tensor  inputs           = (vx_tensor)adapterOperation->inputs;
    vx_scalar  types            = (vx_scalar)adapterOperation->type;
    vx_tensor  outputs          = (vx_tensor)adapterOperation->outputs;

    vx_enum    type             = types->value->e;

    vxSetTensorAttribute(outputs, VX_TENSOR_VALUE, &TENSOR_VALUED(inputs), sizeof(vx_bool));

    switch (type)
    {
        case VX_ADAPTER_CWHN_TO_WHCN:
            vxnneAdapter_CWHN2WHCN(operation);
            break;
        case VX_ADAPTER_WHCN_TO_CWHN:
            vxnneAdapter_WHCN2CWHN(operation);
            break;

        case VX_ADAPTER_F32_TO_F16:
        case VX_ADAPTER_F16_TO_F32:
            status = vxnneAdapter_Tensor_FormatConvert(inputs, outputs);
            break;

        default:
            break;
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNAdapter(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoAdapter_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}
VX_PRIVATE_API vx_status VX_CALLBACK vxoAdapter_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoAdapter_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status                      = VX_SUCCESS;

    vx_tensor  inputs                     = (vx_tensor)parameters[0];
    vx_scalar  type_s                     = (vx_scalar)parameters[1];
    vx_tensor  outputs                    = (vx_tensor)parameters[2];
    vx_enum    inputFormat                = TENSOR_DATA_TYPE(inputs);
    vx_enum    outputFormat               = TENSOR_DATA_TYPE(outputs);
    vx_tensor  src                        = NULL;
    vx_tensor  dst                        = NULL;
    vx_tensor  temp_tensor[2]             = {NULL};
    vx_enum    type                       = type_s->value->e;
    vx_uint32  perm_array[4]              = {0, 1, 2, 3};
    vx_uint32  dnum                       = 0;
    vx_bool    shExe_flag                 = vx_true_e;
    vx_bool    enable_dataFormat          = vx_false_e;
    vx_bool    enable_dataConvert         = vx_false_e;
    vx_uint32  dims                       = TENSOR_VIEW_DIM_NUM(inputs);
    vx_uint32  width                      = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
    vx_uint32  height                     = dims > 1 ? TENSOR_VIEW_SIZE_INDEX(inputs, 1) : 1;
    vx_uint32  depth                      = dims > 2 ? TENSOR_VIEW_SIZE_INDEX(inputs, 2) : 1;
    vx_uint32  batch                      = dims > 3 ? TENSOR_VIEW_SIZE_INDEX(inputs, 3) : 1;
    vx_uint32  batchCount                 = 1;/*TENSOR_SIZE_INDEX(inputs, 3);*/
    vx_uint32  temp_tensor_idx            = 0;

    vxnne_adapter_layer  adapterLayer      = VX_NULL;
    vx_context           context           = vxGetContext((vx_reference)node);

    vxInfo("%s[%d] batchCount = %d", __FUNCTION__, __LINE__, batchCount);

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_adapter_layer_s), (gctPOINTER*)&adapterLayer);
    if (!adapterLayer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return status;
    }

    gcoOS_ZeroMemory(adapterLayer, sizeof(vxnne_adapter_layer_s));

    vxnneLayer_Initialize(&adapterLayer->base,
                          "AdapterLayer",
                          node,
                          vxmOPERATION_COUNT(adapterLayer),
                          adapterLayer->operations,
                          VX_NULL);

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enable_dataFormat  = (vx_bool)((inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                                    || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT32)
                                    || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16));

        if ((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT32)
         || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT16))
        {
            if (type == VX_ADAPTER_CWHN_TO_WHCN)
            {
                batch   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);
                depth   = TENSOR_VIEW_SIZE_INDEX(inputs, 1);
                height  = TENSOR_VIEW_SIZE_INDEX(inputs, 2);
                width   = TENSOR_VIEW_SIZE_INDEX(inputs, 3);
            }
            else if (type == VX_ADAPTER_WHCN_TO_CWHN)
            {
                batch  = TENSOR_SIZE_INDEX(outputs, 0);
                depth  = TENSOR_SIZE_INDEX(outputs, 1);
                height = TENSOR_SIZE_INDEX(outputs, 2);
                width  = TENSOR_SIZE_INDEX(outputs, 3);
            }

            if (width * height * depth * batch < IMG_MAX_WIDTH)
            {
                enable_dataConvert = vx_true_e;
            }
            else if ((width * height * depth < IMG_MAX_WIDTH) && (width * height * depth % INPUT_SIZE_ALIGN_4 == 0))
            {
                enable_dataConvert = vx_true_e;
            }
            else if ((width * height < IMG_MAX_WIDTH) && (width * height % INPUT_SIZE_ALIGN_4 == 0))
            {
                enable_dataConvert = vx_true_e;
            }
            else if (width % INPUT_SIZE_ALIGN_4 == 0)
            {
                enable_dataConvert = vx_true_e;
            }
            else
            {
                enable_dataFormat = vx_false_e;
            }
        }
    }
    else
    {
        enable_dataFormat = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16) ||
                                    (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)     ||
                                    (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8));
    }

    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP_TRANSPOSE) &&
        vxnneIsTPSupportFormat(context, inputs, VX_NULL, outputs) &&
        (type == VX_ADAPTER_CWHN_TO_WHCN || type == VX_ADAPTER_WHCN_TO_CWHN))
    {
        vx_op_param_s conv = {0};
        vx_uint32 dnum = 4;

        if (type == VX_ADAPTER_CWHN_TO_WHCN)
        {
            vx_uint32 sizes[4] = {TENSOR_VIEW_SIZE_INDEX(inputs, 3),
                                  TENSOR_VIEW_SIZE_INDEX(inputs, 2),
                                  TENSOR_VIEW_SIZE_INDEX(inputs, 1),
                                  TENSOR_VIEW_SIZE_INDEX(inputs, 0)};

            src = vxoTensor_ReshapeTensor(inputs, (vx_int32*)sizes, dnum);

            perm_array[0] = 1;
            perm_array[1] = 2;
            perm_array[2] = 0;
            perm_array[3] = 3;
        }
        else /* type == VX_ADAPTER_WHCN_TO_CWHN */
        {
            perm_array[0] = 2;
            perm_array[1] = 0;
            perm_array[2] = 1;
            perm_array[3] = 3;
        }

        status = vxnneOperation_Initialize(&adapterLayer->adapter_tp_operation.base,
                                           &adapterLayer->base,
                                           VXNNE_OPERATION_TARGET_TP,
                                           VXNNE_OPERATOR_TENSOR_TRANS,
                                           VX_NULL,
                                           vxnneOperation_TP_Deinitialize,
                                           1,
                                           0);
        if (status != VX_SUCCESS) goto exit;

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.conv_rounding_type = 0;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;
        conv.tpType = TP_TRANSPOSE;
        conv.other_ref = src == VX_NULL ? (vx_reference)inputs : (vx_reference)src;
        conv.data_buff = gcvNULL;
        conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        conv.tp_value->u32[0] = dnum;
        conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(vx_uint32) * dnum);
        vxMemCopy(conv.tp_value->p8[0], perm_array, sizeof(vx_uint32) * dnum);

        vxMemCopy(&adapterLayer->adapter_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

        vxnneLayer_SetOperation(
            &adapterLayer->base,
            &adapterLayer->adapter_tp_operation.base,
            0);

        adapterLayer->adapter_tp_operation.input  = inputs;
        adapterLayer->adapter_tp_operation.output = outputs;

        vxnneOperation_AddReference(&adapterLayer->adapter_tp_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&adapterLayer->adapter_tp_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }
    else if ((vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)) && (enable_dataFormat || enable_dataConvert) )
    {
        vx_uint32_ptr pPerm         = VX_NULL;
        vx_uint32     num           = 0;
        vx_int32      sizes[]       = {1, 1, 1, 1};
        vx_uint32     convOPIdx     = 0;
        vx_uint32     transOPIdx    = 0;
        vx_uint32     batchCount0   = 1;
        vx_uint32     batchCount1   = 1;

        if (enable_dataConvert)
        {
            if (width * height * depth * batch < IMG_MAX_WIDTH)
            {
                sizes[0]        = width * height * depth * batch;
                sizes[1]        = 1;

                batchCount0 = 1;
            }
            else if ((width * height * depth < IMG_MAX_WIDTH) && batch < IMG_MAX_WIDTH && (width * height * depth % INPUT_SIZE_ALIGN_4 == 0))
            {
                sizes[0]        = width * height * depth;
                sizes[1]        = batch;

                batchCount0 = 1;
            }
            else if ((width * height < IMG_MAX_WIDTH) && (depth * batch < IMG_MAX_WIDTH) && (width * height % INPUT_SIZE_ALIGN_4 == 0))
            {
                sizes[0]        = width * height;
                sizes[1]        = depth * batch;

                batchCount0 = 1;
            }
            else if ((width * height < IMG_MAX_WIDTH) && depth  < IMG_MAX_WIDTH && (width * height % INPUT_SIZE_ALIGN_4 == 0))
            {
                sizes[0]        = width * height;
                sizes[1]        = depth;
                sizes[2]        = 1;
                sizes[3]        = batch;

                batchCount0 = batch;
            }
            else
            {
                sizes[0]        = width;
                sizes[1]        = height;
                sizes[2]        = depth;
                sizes[3]        = batch;

                batchCount0 = batch;
            }

            if (type == VX_ADAPTER_F16_TO_F32 || type == VX_ADAPTER_F32_TO_F16)
            {
                temp_tensor[0] = vxoTensor_ReshapeTensor(inputs, sizes, dims);
                temp_tensor[1] = vxoTensor_ReshapeTensor(outputs, sizes, dims);
            }
            else if (type == VX_ADAPTER_CWHN_TO_WHCN)
            {
                vx_tensor_create_params_t tensor_create_params;

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = TENSOR_DIM_NUM(inputs);
                tensor_create_params.sizes = (vx_uint32 *)sizes;
                tensor_create_params.data_format = VX_TYPE_FLOAT16;
                tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);

                temp_tensor[0] = vxoTensor_ReshapeTensor(inputs, sizes, dims);
                temp_tensor[1] = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);

                convOPIdx  = 0;
                transOPIdx = 1;
            }
            else if (type == VX_ADAPTER_WHCN_TO_CWHN)
            {
                vx_tensor_create_params_t tensor_create_params;

                gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
                tensor_create_params.num_of_dims = TENSOR_DIM_NUM(inputs);
                tensor_create_params.sizes = (vx_uint32 *)sizes;
                tensor_create_params.data_format = VX_TYPE_FLOAT16;
                tensor_create_params.quant_format = TENSOR_QUANT_TYPE(inputs);

                temp_tensor[0] = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);
                temp_tensor[1] = vxoTensor_ReshapeTensor(outputs, sizes, dims);

                convOPIdx  = 1;
                transOPIdx = 0;
            }

            adapterLayer->base.temp_tensors[temp_tensor_idx ++] = temp_tensor[0];
            adapterLayer->base.temp_tensors[temp_tensor_idx ++] = temp_tensor[1];

            enable_dataConvert = vx_true_e;
        }
        else
        {
            temp_tensor[0] = outputs;
            temp_tensor[1] = inputs;
        }

        if (type == VX_ADAPTER_CWHN_TO_WHCN)
        {
            vx_uint32 input_batch   = TENSOR_VIEW_SIZE_INDEX(inputs, 0);  /* N */
            vx_uint32 input_height  = TENSOR_VIEW_SIZE_INDEX(inputs, 1); /* H */
            vx_uint32 input_width   = TENSOR_VIEW_SIZE_INDEX(inputs, 2);  /* W */
            vx_uint32 input_depth   = TENSOR_VIEW_SIZE_INDEX(inputs, 3);  /* C */
            vx_uint32 output_width  = TENSOR_SIZE_INDEX(outputs, 0);  /* W */
            vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
            vx_uint32 output_depth  = TENSOR_SIZE_INDEX(outputs, 2);  /* C */
            vx_uint32 output_batch  = TENSOR_SIZE_INDEX(outputs, 3);  /* N */
            vx_uint32 dims          = TENSOR_VIEW_DIM_NUM(inputs);

            sizes[0]                = input_depth;
            sizes[1]                = input_width;
            sizes[2]                = input_height;
            sizes[3]                = input_batch;
            src                     = vxoTensor_ReshapeTensor(temp_tensor[1], (vx_int32*)sizes, dims);

            sizes[0]                = output_width;
            sizes[1]                = output_height;
            sizes[2]                = output_depth;
            sizes[3]                = output_batch;
            dst                     = vxoTensor_ReshapeTensor(outputs, (vx_int32*)sizes, dims);

            perm_array[0]           = 1;
            perm_array[1]           = 2;
            perm_array[2]           = 0;
            perm_array[3]           = 3;

            dnum                    = 3;

            batchCount1             = dims > 3 ? output_batch : 1;
        }
        else if (type == VX_ADAPTER_WHCN_TO_CWHN)
        {
            vx_uint32 input_width   = TENSOR_SIZE_INDEX(inputs, 0);  /* W */
            vx_uint32 input_height  = TENSOR_SIZE_INDEX(inputs, 1); /* H */
            vx_uint32 input_depth   = TENSOR_SIZE_INDEX(inputs, 2);  /* C */
            vx_uint32 input_batch   = TENSOR_SIZE_INDEX(inputs, 3);  /* N */
            vx_uint32 output_batch  = TENSOR_SIZE_INDEX(outputs, 0);  /* N */
            vx_uint32 output_height = TENSOR_SIZE_INDEX(outputs, 1); /* H */
            vx_uint32 output_width  = TENSOR_SIZE_INDEX(outputs, 2);  /* W */
            vx_uint32 output_depth  = TENSOR_SIZE_INDEX(outputs, 3);  /* C */
            vx_uint32 dims          = TENSOR_VIEW_DIM_NUM(inputs);

            sizes[0]                = input_width;
            sizes[1]                = input_height;
            sizes[2]                = input_depth;
            sizes[3]                = input_batch;
            src                     = vxoTensor_ReshapeTensor(inputs, (vx_int32*)sizes, dims);

            sizes[0]                = output_depth;
            sizes[1]                = output_width;
            sizes[2]                = output_height;
            sizes[3]                = output_batch;
            dst                     = vxoTensor_ReshapeTensor(temp_tensor[0], (vx_int32*)sizes, dims);

            perm_array[0]           = 2;
            perm_array[1]           = 0;
            perm_array[2]           = 1;
            perm_array[3]           = 3;

            dnum                    = 3;

            batchCount1             = dims > 3 ? input_batch : 1;
        }

        if (type == VX_ADAPTER_CWHN_TO_WHCN || type == VX_ADAPTER_WHCN_TO_CWHN)
        {
            pPerm = (vx_uint32_ptr)perm_array;
            num   = dnum;

            shExe_flag    = (vx_bool)((enable_dataFormat && pPerm[0] == 2 && pPerm[1] == 0 && pPerm[2] == 1  && num == 3)
                ||(enable_dataFormat && pPerm[0] == 2 && pPerm[1] == 1 && pPerm[2] == 0  && num == 3)
                ||(enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 2 && pPerm[2] == 0  && num == 3)
                ||(enable_dataFormat && pPerm[0] == 0 && pPerm[1] == 2 && pPerm[2] == 1  && num == 3)
                ||(enable_dataFormat && pPerm[0] == 1 && pPerm[1] == 0 && num <= 3 && num >= 2));

            if(shExe_flag && dnum > 1)
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    if (src && dst)
                        shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src, pPerm, num, dst);
                }
                else
                {
                    if (src && dst)
                        shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, src, pPerm, num, dst);
                }

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&adapterLayer->adapter_sh_operation,
                    &adapterLayer->base,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    batchCount1,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&adapterLayer->adapter_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&adapterLayer->adapter_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &adapterLayer->base,
                    &adapterLayer->adapter_sh_operation.base,
                    transOPIdx);
            }
        }

        if (enable_dataConvert)
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;

            if (temp_tensor[0] && temp_tensor[1])
                shaderExecutable = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, temp_tensor[0], temp_tensor[1]);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&adapterLayer->adapter_convert_sh_operation,
                &adapterLayer->base,
                VXNNE_OPERATOR_TENSOR_COPY,
                batchCount0,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&adapterLayer->adapter_convert_sh_operation.base, (vx_reference)temp_tensor[0], VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&adapterLayer->adapter_convert_sh_operation.base, (vx_reference)temp_tensor[1], VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &adapterLayer->base,
                &adapterLayer->adapter_convert_sh_operation.base,
                convOPIdx);
        }
    }
    else
    {
        vxnneOperation_Initialize(&adapterLayer->adapter_sw_operation.base,
            &adapterLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_ADAPTER,
            vxnneExecuteSWAdapter,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &adapterLayer->base,
            &adapterLayer->adapter_sw_operation.base,
            0);

        adapterLayer->adapter_sw_operation.inputs           = inputs;
        adapterLayer->adapter_sw_operation.type             = type_s;
        adapterLayer->adapter_sw_operation.outputs          = outputs;

        vxnneOperation_AddReference(&adapterLayer->adapter_sw_operation.base, (vx_reference)inputs, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&adapterLayer->adapter_sw_operation.base, (vx_reference)outputs, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    if (src != VX_NULL) adapterLayer->base.temp_tensors[temp_tensor_idx ++] = src;
    if (dst != VX_NULL) adapterLayer->base.temp_tensors[temp_tensor_idx ++] = dst;

    adapterLayer->base.num_temp_tensors = temp_tensor_idx;

    node->layer = &adapterLayer->base;

    return status;

exit:
    if (adapterLayer) {
        gcoOS_Free(NULL, (gctPOINTER)adapterLayer);
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoAdapter_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = NULL;
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_bool _Is_concat_on_highest_dimension(
    vx_tensor tensor,
    vx_uint32 axis
    )
{
    vx_uint32 dim       = TENSOR_DIM_NUM(tensor);
    vx_bool   result    = vx_true_e;
    vx_uint32 i         = 0;

    if(axis == dim - 1)
    {
        return result;
    }

    for (i = axis + 1; i < dim; i++)
    {
        if (TENSOR_VIEW_SIZE_INDEX(tensor, i) != 1)
            return vx_false_e;
    }

    return result;
}

vx_status vxnneExecuteSWConcatIndefinite(struct _vxnne_operation_s *operation)
{
    vx_status status = VX_SUCCESS;
    vxnne_concatIndefinite_sw_operation concatOp = (vxnne_concatIndefinite_sw_operation)operation;

    vx_object_array inputArray = concatOp->inputs;
    vx_tensor output = concatOp->outputs;
    vx_uint32 axis = concatOp->axis->value->n32;
    vx_uint32 i, offset = 0, dim_count = TENSOR_DIM_NUM(output);
    vx_uint8_ptr pOutputBuf = NULL;

    vxoTensor_GetTensorViewMemory(output, (gctPOINTER *)&pOutputBuf, VX_NULL);

    vxmASSERT(inputArray->itemType == VX_TYPE_TENSOR && inputArray->itemCount > 0);

    for (i = 0; i < inputArray->itemCount; i++)
    {
        vx_tensor input = (vx_tensor)inputArray->itemsTable[i];

        vx_uint8_ptr pInputBuf = NULL;
        vx_uint32 inputSize = 0;
        vx_uint32 input_slice = 1, count = 1, output_slice = 1;
        vx_uint32 m = 0, n = 0;

        if (input->isViewed)
        {
            status = vxoTensor_GetTensorSize(input, &inputSize) == VX_SUCCESS;
        }
        else
        {
            inputSize = (vx_uint32)vxoMemory_ComputeSize(&input->tensorBuffer->memory, 0);
        }

        vxoTensor_GetTensorViewMemory(input, (gctPOINTER *)&pInputBuf, VX_NULL);
        gcmASSERT(axis < dim_count);

        for (m = 0; m <= axis; m++)
        {
            input_slice *= TENSOR_SIZE_INDEX(input, m);
            output_slice *= TENSOR_SIZE_INDEX(output, m);
        }

        for (m = axis + 1; m < dim_count; m++)
        {
            count *= TENSOR_SIZE_INDEX(input, m);
        }

        /*
         *     30 * 212 * 50 * 1          60 * 212 * 50 * 1                          90 * 212 * 50 * 1
         *          _________             ____________                               ________________
         *         /        /|           /           /|                             /               /|
         *  0)    /________/ |    +     /___________/ |          =>                /_______________/ |
         *        |        | /          |           | /                            |               | /
         *        |________|/           |___________|/                             |_______________|/
         *
         *      ----------------------------------------------------------------------------------------------------
         *     212 * 30 * 50 * 10                                212 * 90 * 50 * 10
         *          _________                                    _________
         *         /        /|                                  /        /|
         *  1)    /________/ |               =>                /________/ |
         *        |        | /                                 |        | |
         *        |________|/                                  |        | |
         *                                                     |        | |
         *             +                                       |        | |
         *     212 * 60 * 50 * 10                              |        | |
         *          ________                                   |        | /
         *         /       /|                                  |________|/
         *        /_______/ |
         *        |       | |
         *        |       | |
         *        |       | /
         *        |__ ____|/
         *
         *      ----------------------------------------------------------------------------------------------------
         *     212 * 50 * 30 * 1                             212 * 50 * 90 * 1
         *          _________                                 _________
         *         /        /|                               /        /|
         *  2)    /________/ |          =>                  /        / |
         *        |        | /                             /        /  |
         *        |________|/                             /        /  /
         *                                               /________/  /
         *              +                                |        | /
         *      212 * 50 * 60 * 1                        |________|/
         *           _________
         *          /        /|
         *         /        / |
         *        /________/  |
         *        |        | /
         *        |________|/
         */
        for (m = 0; m < count; m++)
        {
            if (_IsSameType(input, output))
            {
                memcpy(pOutputBuf + (output_slice * m + offset) * TENSOR_DATA_SIZE(input), pInputBuf + input_slice * m * TENSOR_DATA_SIZE(input), input_slice * TENSOR_DATA_SIZE(input));
            }
            else
            {
                for (n = 0; n < input_slice; n++)
                {
                    vx_float32 src0 = vxnneGetDataExt((vx_type_e)TENSOR_DATA_TYPE(input), TENSOR_QUANT_TYPE(input), input_slice * m + n, pInputBuf,
                        TENSOR_POS(input), TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input));

                    status = vxnneSaveDataExt((vx_type_e)TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), output_slice * m + n + offset, src0, pOutputBuf,
                        TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
                }
            }

        }

        offset += input_slice;
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNConcatIndefiniteLayer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcatIndefiniteLayer_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcatIndefiniteLayer_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

vx_status vxnneConcatIndefiniteLayer_Deinitialize(struct _vxnne_layer_s* layer)
{
    vx_uint32 i;
    vxnne_concatIndefinite_layer concatNLayer = (vxnne_concatIndefinite_layer)layer;

    for (i = 0; i < VX_MAX_TEMP_TENSORS; i++)
    {
        if (layer->temp_tensors[i] != VX_NULL)
        {
            vxoTensor_ReleaseTensor(&layer->temp_tensors[i]);
        }
    }

    for (i = 0; i < layer->num_operations; i++)
    {
        if (layer->operations[i]->deinitialize != VX_NULL)
        {
            layer->operations[i]->deinitialize(layer->operations[i]);
        }
    }

    if(concatNLayer->operations2)
    {
        gcoOS_Free(NULL, concatNLayer->operations2);
        concatNLayer->operations2 = VX_NULL;
    }

    if(concatNLayer->concat_sh_unit_operation)
    {
        gcoOS_Free(NULL, concatNLayer->concat_sh_unit_operation);
        concatNLayer->concat_sh_unit_operation = VX_NULL;
    }

    if (concatNLayer->tp_operation)
    {
        gcoOS_Free(NULL, concatNLayer->tp_operation);
        concatNLayer->tp_operation = VX_NULL;
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcatIndefiniteLayer_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_context context          = vxGetContext((vx_reference)node);

    vx_object_array  input_s    = (vx_object_array)parameters[0];
    vx_scalar  axis_s           = (vx_scalar)parameters[1];
    vx_tensor  output_s         = (vx_tensor)parameters[2];

    vx_bool    enable_SHExe     = vx_false_e;
    vx_uint32  axis             = (vx_uint32)axis_s->value->n32;
    vx_uint32  i                = 0;
    vx_uint32  dimCount         = TENSOR_VIEW_DIM_NUM(output_s);
    vx_uint32  itemCount        = (vx_uint32)input_s->itemCount;
    vx_uint32  batchCount       = dimCount > 3 ? TENSOR_SIZE_INDEX(output_s, 3) : 1;
    vx_uint32  operationCount   = 1;
    vx_uint32  opIdx            = 0;
    vx_uint32  operationIndex   = 0;
    vx_uint32  tmpTensorIndex   = 0;
    vx_bool    isTpSupportFormat = vx_true_e;

    vxnne_concatIndefinite_layer  concatNLayer = VX_NULL;
    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    if(node->base.context->evisNoInst.supportEVIS)
    {
        enable_SHExe = (vx_bool)(TENSOR_DATA_TYPE(output_s) != VX_TYPE_FLOAT32);
        for (i = 0; i < itemCount; i++)
        {
            vx_tensor input = (vx_tensor)input_s->itemsTable[i];

            enable_SHExe = (vx_bool)(TENSOR_DATA_TYPE(input) != VX_TYPE_FLOAT32 && enable_SHExe);
        }
        enable_SHExe = enable_SHExe && (_Is_concat_on_highest_dimension(output_s, axis) || (axis < 3 && batchCount == 1));
    }
    else
    {
        enable_SHExe = (vx_bool)((dimCount - 1) == axis || ((dimCount - 1) > axis && dimCount < 4));
    }

    for (i = 0; i < itemCount; i++)
    {
        vx_tensor input = (vx_tensor)input_s->itemsTable[i];

        if (!vxnneIsTPSupportFormat(context, input, VX_NULL, output_s))
        {
            isTpSupportFormat = vx_false_e;
            break;
        }
    }

    enable_SHExe = enable_SHExe && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER));

    if (enable_SHExe)
        operationCount = itemCount + 1;

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_concatIndefinite_layer_s), (gctPOINTER*)&concatNLayer);
    if (!concatNLayer)
    {
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        vxmONERROR(VX_ERROR_NO_MEMORY);
    }

    gcoOS_ZeroMemory(concatNLayer, sizeof(vxnne_concatIndefinite_layer_s));

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_operation) * (itemCount + 1), (gctPOINTER*)&concatNLayer->operations2);

    gcoOS_ZeroMemory(concatNLayer->operations2, sizeof(vxnne_operation) * (itemCount + 1));

    vxmONERROR(vxnneLayer_Initialize(&concatNLayer->base,
                                     "ConcatLayer",
                                     node,
                                     itemCount + 1,
                                     concatNLayer->operations2,
                                     vxnneConcatIndefiniteLayer_Deinitialize));

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP) &&
        isTpSupportFormat)
    {
        vx_tensor input = VX_NULL, output = VX_NULL;
        vx_tensor_view tensor_view = VX_NULL;
        vx_uint32 start[4] = {0, 0, 0, 0}, end[4] = {0, 0, 0, 0};
        vx_uint32 op_count = itemCount;
        vx_op_param param = VX_NULL;

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_tp_operation_s) * op_count, (gctPOINTER*)&concatNLayer->tp_operation);
        gcoOS_ZeroMemory(concatNLayer->tp_operation, sizeof(vxnne_tp_operation_s) * op_count);

        for (i = 0; i < dimCount; i++)
        {
            if (i != axis)
            {
                end[i] = TENSOR_VIEW_SIZE_INDEX(output_s, i);
            }
        }

        end[axis] = start[axis];

        for (i = 0; i < itemCount; i++)
        {
            input = (vx_tensor)input_s->itemsTable[i];
            batchCount = TENSOR_VIEW_DIM_NUM(input) > 3 ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;

            end[axis] = start[axis] + TENSOR_VIEW_SIZE_INDEX(input, axis);

            tensor_view = vxCreateTensorView(context, start, end, (vx_uint8)dimCount);
            if (!tensor_view)
            {
                vxmONERROR(VX_ERROR_NO_MEMORY);
            }

            output = vxoTensor_CreateTensorFromView(output_s, tensor_view);

            vxmONERROR(vxnneOperation_Initialize(&concatNLayer->tp_operation[i].base,
                                                 &concatNLayer->base,
                                                 VXNNE_OPERATION_TARGET_TP,
                                                 VXNNE_OPERATOR_CONCATINDEFINITE,
                                                 VX_NULL,
                                                 VX_NULL,
                                                 batchCount,
                                                 0));

            param = &concatNLayer->tp_operation[i].base.parameter;
            param->tpType = TP_TENSOR_COPY4CONCAT;

            concatNLayer->tp_operation[i].input = input;
            concatNLayer->tp_operation[i].output = output;

            vxmONERROR(vxnneLayer_SetOperation(&concatNLayer->base,
                                               &concatNLayer->tp_operation[i].base,
                                               operationIndex++));

            vxnneOperation_AddReference(&concatNLayer->tp_operation[i].base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&concatNLayer->tp_operation[i].base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            concatNLayer->base.temp_tensors[tmpTensorIndex++] = output;

            vxReleaseTensorView(&tensor_view);

            /* Update the start point of view. */
            start[axis] = end[axis];
        }
    }
    else if (enable_SHExe)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_uint32 i          = 0;
        vx_uint32 w         = TENSOR_VIEW_SIZE_INDEX(output_s, 0);
        vx_uint32 h         = TENSOR_VIEW_SIZE_INDEX(output_s, 1);
        vx_uint32 c         = TENSOR_VIEW_SIZE_INDEX(output_s, 2);
        vx_uint32 n         = TENSOR_VIEW_SIZE_INDEX(output_s, 3);
        vx_uint32 sizes[4]  = {w, h, c, n};
        vx_uint32 start[4]  = {0, 0, 0, 0};
        vx_uint32 end[4]    = {0, 0, 0, 0};

        gcoOS_Allocate(gcvNULL, sizeof(vxnne_shader_operation_s) * operationCount, (gctPOINTER*)&concatNLayer->concat_sh_unit_operation);
        if (!concatNLayer->concat_sh_unit_operation)
        {
            vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
            vxmONERROR(VX_ERROR_NO_MEMORY);
        }
        else
            gcoOS_ZeroMemory(concatNLayer->concat_sh_unit_operation, sizeof(vxnne_shader_operation_s) * operationCount);

        if (_Is_concat_on_highest_dimension(output_s, axis))
        {
            for (i = 0; i < axis; i ++)
            {
                end[i] = sizes[i];
            }

            for (i = 0; i < itemCount; i ++)
            {
                vx_tensor input = (vx_tensor)input_s->itemsTable[i];
                vx_tensor_view  tensor_view  = NULL;
                vx_tensor       subtensor    = NULL;

                batchCount     = TENSOR_VIEW_DIM_NUM(input) > 3 ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;

                start[axis]    = end[axis];
                end[axis]      += TENSOR_VIEW_SIZE_INDEX(input, axis);

                tensor_view = vxCreateTensorView(node->base.context, start, end, (vx_uint8)dimCount);
                if (tensor_view == NULL)
                {
                    vxError("vxCreateTensorView failure! at line %d\n", __LINE__);
                    vxmONERROR(VX_ERROR_NO_MEMORY);
                }

                subtensor           = vxoTensor_CreateTensorFromView(output_s, tensor_view);

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable    = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, subtensor);
                }
                else
                {
                    shaderExecutable    = vxnneGPUTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, input, subtensor);
                }

                if (!shaderExecutable)
                {
                    vxmONERROR(VX_FAILURE);
                }

                vxmONERROR(vxnneShaderOperation_Initialize(&concatNLayer->concat_sh_unit_operation[operationIndex],
                    &concatNLayer->base,
                    VXNNE_OPERATOR_TENSOR_COPY,
                    batchCount,
                    shaderExecutable));

                vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)subtensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &concatNLayer->base,
                    &concatNLayer->concat_sh_unit_operation[operationIndex].base,
                    opIdx++);
                operationIndex ++;

                concatNLayer->base.temp_tensors[tmpTensorIndex++] = subtensor;
                if (tensor_view) vxReleaseTensorView(&tensor_view);
            }

        }
        else if ((dimCount - 1) > axis)
        {
            vx_tensor_create_params_t tensor_create_params;
            vx_tensor output_tmp                = NULL;
            vx_uint32 new_sizes[4]              = {w, h, c, n};
            vx_uint32 dims_idx[4]               = {0, 1, 2, 3};
            vx_uint32 perm_array[4]             = {0, 1, 2, 3};

            dimCount = dimCount < 4 ? dimCount : dimCount - 1;

            perm_array[axis]            = dims_idx[dimCount - 1];
            perm_array[dimCount - 1]    = dims_idx[axis];
            new_sizes[axis]             = sizes[dimCount - 1];
            new_sizes[dimCount - 1]     = sizes[axis];

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = dimCount;
            tensor_create_params.sizes = new_sizes;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(output_s);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(output_s);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(output_s);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(output_s);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(output_s);
            }
            output_tmp = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);

            concatNLayer->base.temp_tensors[tmpTensorIndex++] = output_tmp;

            for (i = 0; i < dimCount - 1; i ++)
            {
                end[i] = new_sizes[i];
            }

            for (i = 0; i < itemCount; i ++)
            {
                vx_tensor input = (vx_tensor)input_s->itemsTable[i];
                vx_tensor_view  tensor_view  = NULL;
                vx_tensor       subtensor    = NULL;

                batchCount                   = 1;

                start[dimCount - 1]    = end[dimCount - 1];
                end[dimCount - 1]      += TENSOR_VIEW_SIZE_INDEX(input, axis);

                tensor_view = vxCreateTensorView(node->base.context, start, end, (vx_uint8)dimCount);
                if (tensor_view == NULL)
                {
                    vxError("vxCreateTensorView failure! at line %d\n", __LINE__);
                    vxmONERROR(VX_FAILURE);
                }

                subtensor           = vxoTensor_CreateTensorFromView(output_tmp, tensor_view);

                /* operation i: permute input concat dimension to the highest dimension */
                if(node->base.context->evisNoInst.supportEVIS)
                {
                   shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, perm_array, 3, subtensor);
                }
                else
                {
                    shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, perm_array, 3, subtensor);
                }

                if (!shaderExecutable)
                {
                    vxmONERROR(VX_FAILURE);
                }

                vxmONERROR(vxnneShaderOperation_Initialize(&concatNLayer->concat_sh_unit_operation[operationIndex],
                    &concatNLayer->base,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    batchCount,
                    shaderExecutable));

                vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)subtensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &concatNLayer->base,
                    &concatNLayer->concat_sh_unit_operation[operationIndex].base,
                    opIdx++);
                operationIndex ++;

                concatNLayer->base.temp_tensors[tmpTensorIndex++] = subtensor;
                if (tensor_view) vxReleaseTensorView(&tensor_view);
            }

            /* operation : permute to get final result */
            if(node->base.context->evisNoInst.supportEVIS)
            {
               shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, output_tmp, perm_array, 3, output_s);
            }
            else
            {
               shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, output_tmp, perm_array, 3, output_s);
            }

            if (!shaderExecutable)
            {
                vxmONERROR(VX_FAILURE);
            }

            vxmONERROR(vxnneShaderOperation_Initialize(&concatNLayer->concat_sh_unit_operation[operationIndex],
                &concatNLayer->base,
                VXNNE_OPERATOR_TENSOR_TRANS,
                batchCount,
                shaderExecutable));

            vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)output_tmp, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&concatNLayer->concat_sh_unit_operation[operationIndex].base, (vx_reference)output_s, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &concatNLayer->base,
                &concatNLayer->concat_sh_unit_operation[operationIndex].base,
                opIdx++);
            operationIndex ++;
        }
        concatNLayer->base.num_temp_tensors = tmpTensorIndex;
    }
    else
    {
        vxnneOperation_Initialize(&concatNLayer->concatIndefinite_operation.base,
            &concatNLayer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_CONCATINDEFINITE,
            vxnneExecuteSWConcatIndefinite,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &concatNLayer->base,
            &concatNLayer->concatIndefinite_operation.base,
            0);

        concatNLayer->concatIndefinite_operation.inputs          = input_s;
        concatNLayer->concatIndefinite_operation.axis            = axis_s;
        concatNLayer->concatIndefinite_operation.outputs         = output_s;

        for (i = 0; i < itemCount; i++)
        {
            vx_tensor input = (vx_tensor)input_s->itemsTable[i];
            vxnneOperation_AddReference(&concatNLayer->concatIndefinite_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        }
        vxnneOperation_AddReference(&concatNLayer->concatIndefinite_operation.base, (vx_reference)output_s, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &concatNLayer->base;

    return status;

OnError:
    if(concatNLayer)
    {
        if (concatNLayer->tp_operation)
        {
            gcoOS_Free(gcvNULL, concatNLayer->tp_operation);
            concatNLayer->tp_operation = VX_NULL;
        }

        if(concatNLayer->operations2)
        {
            gcoOS_Free(gcvNULL, concatNLayer->operations2);
        }
        if(concatNLayer->concat_sh_unit_operation)
        {
            gcoOS_Free(gcvNULL, concatNLayer->concat_sh_unit_operation);
        }

        gcoOS_Free(gcvNULL, (gctPOINTER)concatNLayer);
    }

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNConcatIndefiniteLayer_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = NULL;
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API
vx_status vxnneUserOperation_Execute(
    vxnne_operation operation
    )
{
    vx_node node = operation->layer->node;
    vx_status status = VX_SUCCESS;

    if (node->kernel->function)
    {
        vxmONERROR(node->kernel->function(node,
                                          node->paramTable,
                                          node->kernel->signature.paramCount));
    }

OnError:
    return status;
}

VX_PRIVATE_API
vx_status VX_CALLBACK vxnneUserNode_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = NULL;
    }

    return VX_SUCCESS;
}

vx_status vxnneWrapUserNode(
    vx_context context,
    vx_node node,
    vxnne_user_node_type_e userNodeType
    )
{
    vxnne_user_layer userLayer = gcvNULL;
    vx_uint32 operationIndex = 0;
    vx_uint32 i;

    vx_status status = VX_SUCCESS;

    if (!node || node->layer)
    {
        return VX_ERROR_INVALID_PARAMETERS;
    }

    /* Create layer. */
    gcoOS_Allocate(gcvNULL, sizeof(vxnne_user_layer_s), (gctPOINTER*)&userLayer);
    if (!userLayer)
    {
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        return VX_ERROR_NO_MEMORY;
    }

    gcoOS_ZeroMemory(userLayer, sizeof(vxnne_user_layer_s));

    vxnneLayer_Initialize(&userLayer->base,
                          node->kernel->name,
                          node,
                          vxmOPERATION_COUNT(userLayer),
                          userLayer->operations,
                          VX_NULL);

    node->kernel->deinitializeWrapFunction = vxnneUserNode_Deinitializer;

    if (userNodeType == VXNNE_USER_NODE_TYPE_VXC)
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;

        shaderExecutable = vxnneGetUserShaderExecutable(node->base.context,
                                                        node->kernel,
                                                        node->paramTable,
                                                        node->kernel->signature.dataTypeTable,
                                                        node->kernel->signature.paramCount,
                                                        node->uniforms,
                                                        node->uniformCount,
                                                        &node->kernelAttributes.borderMode,
                                                        &node->kernelAttributes.shaderParameter);
        if (!shaderExecutable)
        {
            status = VX_FAILURE;
            goto OnError;
        }

        status = vxnneShaderOperation_Initialize(
                    &userLayer->user_shader_operation,
                    &userLayer->base,
                    VXNNE_OPERATOR_USER_VXC,
                    1,
                    shaderExecutable);
        if (status != VX_SUCCESS)
        {
            goto OnError;
        }

        for (i = 0; i < node->kernel->signature.paramCount; i++)
        {
            if (node->kernel->signature.directionTable[i] == VX_INPUT &&
                (node->kernel->signature.isStaticTable[i] == vx_false_e ||
                 (node->kernel->signature.dataTypeTable[i] == VX_TYPE_TENSOR &&
                  TENSOR_DATA_LIFETIME((vx_tensor)(node->paramTable[i])) == VX_TENSOR_LIFE_TIME_DYNAMIC)))
            {
                vxnneOperation_AddReference(&userLayer->user_shader_operation.base, node->paramTable[i], VXNNE_OPERATION_REFENRENCE_INPUT);
            }
            else if (node->kernel->signature.directionTable[i] == VX_OUTPUT)
            {
                vxnneOperation_AddReference(&userLayer->user_shader_operation.base, node->paramTable[i], VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
        }

        vxnneLayer_SetOperation(
            &userLayer->base,
            &userLayer->user_shader_operation.base,
            operationIndex++);
    }
    else if (userNodeType == VXNNE_USER_NODE_TYPE_CPU)
    {
        /* Create CPU operation. */
        vxmONERROR(vxnneOperation_Initialize(&userLayer->user_cpu_operation.base,
                                             &userLayer->base,
                                             VXNNE_OPERATION_TARGET_SW,
                                             VXNNE_OPERATOR_USER_CPU,
                                             vxnneUserOperation_Execute,
                                             VX_NULL,
                                             1,
                                             0));

        for (i = 0; i < node->kernel->signature.paramCount; i++)
        {
            if (node->kernel->signature.directionTable[i] == VX_INPUT &&
                (node->kernel->signature.isStaticTable[i] == vx_false_e ||
                 (node->kernel->signature.dataTypeTable[i] == VX_TYPE_TENSOR &&
                  TENSOR_DATA_LIFETIME((vx_tensor)(node->paramTable[i])) == VX_TENSOR_LIFE_TIME_DYNAMIC)))
            {
                vxnneOperation_AddReference(&userLayer->user_cpu_operation.base, node->paramTable[i], VXNNE_OPERATION_REFENRENCE_INPUT);
            }
            else if (node->kernel->signature.directionTable[i] == VX_OUTPUT)
            {
                vxnneOperation_AddReference(&userLayer->user_cpu_operation.base, node->paramTable[i], VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
        }

        vxnneLayer_SetOperation(
            &userLayer->base,
            &userLayer->user_cpu_operation.base,
            operationIndex++);
    }

    node->layer = &userLayer->base;

    return VX_SUCCESS;

OnError:
    if (userLayer)
    {
        gcoOS_Free(gcvNULL, userLayer);
    }

    return status;
}


/***************************************************************************************************************************
*                                                 TENSOR MEAN
***************************************************************************************************************************/

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorMean(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMean_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMean_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorMean_GetSlices(vx_int32_ptr dims, vx_int32 dim_count, vx_int32_ptr front_slice, vx_int32 axis, vx_int32_ptr back_slice)
{
    vx_int32 i = 0, _axis = axis < 0 ? axis + dim_count : axis;

    gcmASSERT(dim_count > 0);
    gcmASSERT(front_slice != VX_NULL);
    gcmASSERT(back_slice != VX_NULL);

    if (_axis < dim_count)
    {
        *front_slice = 1;
        for (i = 0; i < _axis; i++)
            *front_slice *= dims[i];

        *back_slice = 1;
        for (i = _axis + 1; i < dim_count; i++)
            *back_slice *= dims[i];
    }

    return VX_SUCCESS;
}

vx_status vxnneExecuteSWTensorMean_ReduceSum(vx_uint8_ptr input_base, vx_int32_ptr input_dims, vx_int32 dim_count,
    vx_enum in_format, vx_enum in_quant_format, vx_uint8 in_fp, vx_int32 in_zp, vx_float32 in_scale,
    vx_enum out_format, vx_enum out_quant_format, vx_uint8 out_fp, vx_int32 out_zp, vx_float32 out_scale, vx_enum rounding,
   vx_int32_ptr axis_dims, vx_int32 axis_count, vx_int32 current_axis_index, vx_int32 count, vx_uint8_ptr output_base)
{
    /*****************************************************************************************************************************************************************
    *                 output                   axis                    input     *        output                   axis                    input
    *                 C H W                   1, 2                     C H W     *        C H W                   0, 2                     C H W
    *                 2 3 4                     =>                     2 1 1     *        2 3 4                     =>                     3 1 1
    *             ____________________                                           *    ____________________
    *            |  1  2|  3  4|  5  6|                                          *   |  1  2|  3  4|  5  6|
    *            |______|______|______|               _______                    *   |______|______|______|               ______________
    *            |  7  8|  9 10| 11 12|              | 12| 13|                   *   |  7  8|  9 10| 11 12|              |10.5|12.5|14.5|
    *            |______|______|______|         =>   |___|___|                   *   |______|______|______|         =>   |____|____|____|
    *            | 13 14| 15 16| 17 18|                                          *   | 13 14| 15 16| 17 18|
    *            |______|______|______|                                          *   |______|______|______|
    *            | 19 20| 21 22| 23 24|                                          *   | 19 20| 21 22| 23 14|
    *            |______|______|______|                                          *   |______|______|______|
    *****************************************************************************************************************************************************************/
    if (current_axis_index < axis_count)
    {
        vx_uint8_ptr output_ptr = VX_NULL;
        vx_int32 axis = axis_dims[current_axis_index];
        vx_int32 current_dim = input_dims[axis];

        vx_int32 i = 0, b = 0, f = 0, front_slice = 1, back_slice = 1;

        vx_int32 out_item_size = vxnneGetTypeSize((vx_type_e)out_format);
        vx_bool last = (current_axis_index < (axis_count - 1)) ? vx_false_e : vx_true_e;
        vx_float32 sum = 0.f;
        vx_int32 _count = last ? count * current_dim : 1;

        vxnneExecuteSWTensorMean_GetSlices(input_dims, dim_count, &front_slice, axis, &back_slice);

        output_ptr = last? output_base:(vx_uint8_ptr)malloc(front_slice * back_slice * out_item_size);


        for (b = 0; b < back_slice; b++)/*4*/
        {
            for (f = 0; f < front_slice; f++)/*2*/
            {
                sum = 0.f;
                for (i = 0; i < current_dim; i++)/*3*/
                    sum += vxnneGetDataExt((vx_type_e)in_format, in_quant_format, f + i * front_slice + b * current_dim * front_slice, input_base, in_fp, in_zp, in_scale);

                vxnneSaveDataExt((vx_type_e)out_format, out_quant_format, f + b * front_slice, sum / _count, output_ptr, out_fp, out_zp, out_scale, rounding);
            }
        }

        input_dims[axis] = 1;

        if (current_axis_index > 0)
            free(input_base);


        return vxnneExecuteSWTensorMean_ReduceSum(output_ptr, input_dims, dim_count,
            in_format, in_quant_format, in_fp, in_zp, in_scale,
            out_format, out_quant_format, out_fp, out_zp, out_scale, rounding,
            axis_dims, axis_count, current_axis_index + 1, count * current_dim, output_base);
    }
    else
        return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneExecuteSWTensorMean(struct _vxnne_operation_s *operation)
{

    vxnne_tensor_mean_operation transOperation = (vxnne_tensor_mean_operation)operation;
    vx_tensor input     = (vx_tensor)transOperation->input;
    vx_tensor axis      = (vx_tensor)transOperation->axis;
    /*vx_scalar keep_dims = (vx_scalar)transOperation->keep_dims;*/
    vx_tensor output    = (vx_tensor)transOperation->output;
    vx_int32_ptr axis_base = VX_NULL;
    vx_uint8_ptr input_ptr = VX_NULL, output_ptr = VX_NULL;

    vx_uint32 i = 0, j = 0, count = 1;
    vx_int32 resolved_dim[4] = {-1, -1, -1, -1};
    vx_int32 resolved_dim_count = 0;
    vx_int32 dims[4] = { TENSOR_SIZE_INDEX(input, 0), TENSOR_SIZE_INDEX(input, 1), TENSOR_SIZE_INDEX(input, 2), TENSOR_SIZE_INDEX(input, 3) };

    vxoTensor_GetTensorViewMemory(axis, (gctPOINTER*)&axis_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&input_ptr, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&output_ptr, VX_NULL);

    gcmASSERT(axis->dimCount == 1);

    /*****************************************************************************************************************************************************************
    *                                                                            *
    *                 output                   axis                    input     *        output                   axis                    input
    *                 C H W                   1, 2                     C H W     *        C H W                   0, 2                     C H W
    *                 2 3 4                     =>                     2 1 1     *        2 3 4                     =>                     3 1 1
    *             ____________________                                           *    ____________________
    *            |  1  2|  3  4|  5  6|                                          *   |  1  2|  3  4|  5  6|
    *            |______|______|______|               _______                    *   |______|______|______|               ______________
    *            |  7  8|  9 10| 11 12|              | 12| 13|                   *   |  7  8|  9 10| 11 12|              |10.5|12.5|14.5|
    *            |______|______|______|         =>   |___|___|                   *   |______|______|______|         =>   |____|____|____|
    *            | 13 14| 15 16| 17 18|                                          *   | 13 14| 15 16| 17 18|
    *            |______|______|______|                                          *   |______|______|______|
    *            | 19 20| 21 22| 23 24|                                          *   | 19 20| 21 22| 23 14|
    *            |______|______|______|                                          *   |______|______|______|
    *                                                                            *
    *                            output                axis     input            *        output                   axis                    input
    *                            W H C                 1, 2     W H C            *        C H W                   0, 2                     C H W
    *                            4 3 2                  =>      1 1 2            *        2 3 4                     =>                     3 1 1
    *             _______________     _______________                            *    ___________     ___________
    *            |  1|  3|  5|  7|   |  2|  4|  6|  8|                           *   |  1|  3|  5|   |  2|  4|  6|
    *            |___|___|___|___|   |___|___|___|___|        _______            *   |___|___|___|   |___|___|___|         ______________
    *            |  9| 11| 13| 15|   | 10| 12| 14| 16|       | 12| 13|           *   |  7|  9| 11|   |  8| 10| 12|        |10.5|12.5|14.5|
    *            |___|___|___|___|   |___|___|___|___|  =>   |___|___|           *   |___|___|___|   |___|___|___|   =>   |____|____|____|
    *            | 17| 19| 21| 23|   | 18| 20| 22| 24|                           *   | 13| 15| 17|   | 14| 16| 18|
    *            |___|___|___|___|   |___|___|___|___|                           *   |___|___|___|   |___|___|___|
                                                                                                                               *
    *****************************************************************************************************************************************************************/
    /* resolve axis dims */
    for (i = 0; i < TENSOR_SIZE_INDEX(axis, 0); i++)
    {
        vx_int32 current_axis = axis_base[i] < 0 ? TENSOR_DIM_NUM(input) + axis_base[i] : axis_base[i];
        for (j = 0; j < 4; j++)
        {
            if (resolved_dim[j] == current_axis)
                break;
        }

        if (j == 4)
            resolved_dim[resolved_dim_count++] = current_axis;
    }

    vxnneExecuteSWTensorMean_ReduceSum(input_ptr, dims, TENSOR_DIM_NUM(input),
        TENSOR_DATA_TYPE(input), TENSOR_QUANT_TYPE(input), TENSOR_POS(input), TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input),
        TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output),
        axis_base, resolved_dim_count, 0, count, output_ptr
    );
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMean_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input     = (vx_tensor)parameters[0];
    vx_tensor axis      = (vx_tensor)parameters[1];
    vx_scalar keep_dims = (vx_scalar)parameters[2];
    vx_tensor output    = (vx_tensor)parameters[3];
    vx_uint32 batchCount = 1;

    vx_int32_ptr axis_base          = VX_NULL;
    vx_int32     resolved_dim[4]    = {-1, -1, -1, -1};
    vx_int32     resolved_dim_count = 0;
    vx_uint32    i                  = 0;
    vx_uint32    j                  = 0;
    vx_uint32   input_axis;
    vx_enum     inputFormat         = TENSOR_DATA_TYPE(input);
    vx_enum     outputFormat        = TENSOR_DATA_TYPE(output);
    vx_uint32   tmpTensorIndex      = 0;
    vx_uint32   operationIdx        = 0;

    vx_bool     shExe_flag          = vx_false_e;

    vx_context  context             = vxGetContext((vx_reference)node);

    vxnne_tensor_mean_layer tensor_mean_layer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_mean_layer_s), (gctPOINTER*)&tensor_mean_layer);
    if (!tensor_mean_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_mean_layer, sizeof(vxnne_tensor_mean_layer_s));

    vxnneLayer_Initialize(&tensor_mean_layer->base,
        "TensorMean",
        node,
        vxmOPERATION_COUNT(tensor_mean_layer),
        tensor_mean_layer->operations,
        VX_NULL);

    vxoTensor_GetTensorViewMemory(axis, (gctPOINTER*)&axis_base, VX_NULL);

    for (i = 0; i < TENSOR_SIZE_INDEX(axis, 0); i++)
    {
        vx_int32 current_axis = axis_base[i] < 0 ? TENSOR_DIM_NUM(input) + axis_base[i] : axis_base[i];

        if (current_axis < 0 || current_axis >= (vx_int32)TENSOR_DIM_NUM(input))
        {
            vxError("error: the axis value must be in the range [0, %d)\n", TENSOR_DIM_NUM(input));
            gcmASSERT(0);
        }

        for (j = 0; j < 4; j++)
        {
            if (resolved_dim[j] == current_axis)
                break;
        }

        if (j == 4)
            resolved_dim[resolved_dim_count++] = current_axis;
    }

    input_axis = 0;

    {
        vx_uint32 dst_elementCount = 0;

        status = vxoTensor_GetTensorElementCount(output, &dst_elementCount);

        if (dst_elementCount == 1) /* reudce mean all*/
        {
            vx_uint32  reshpTensor_Sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {1};
            vx_uint32  reshpTensor_Dims = 2;

            resolved_dim_count = 0;

            vxoElementOptimization_GetTensorShape(input, reshpTensor_Sizes, &reshpTensor_Dims);

            for (i = 0; i < reshpTensor_Dims; i++)
            {
                if (reshpTensor_Sizes[i] != 1)
                {
                    resolved_dim[resolved_dim_count++] = i;
                }
                else
                    break;
            }

            input = vxoTensor_ReshapeTensor((vx_tensor)parameters[0], (vx_int32*)reshpTensor_Sizes, reshpTensor_Dims);

            tensor_mean_layer->base.temp_tensors[tmpTensorIndex++] = input;
            tensor_mean_layer->base.num_temp_tensors = tmpTensorIndex;
        }
    }

    if(context->evisNoInst.supportEVIS)
    {
        shExe_flag = (vx_bool)((inputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32 && resolved_dim_count == 2)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_INT8 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_UINT8 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 1));
    }
    else
    {
        shExe_flag = (vx_bool)((inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32 && resolved_dim_count == 2)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32 && resolved_dim_count == 1)
                           || (inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16 && resolved_dim_count == 2)
                           || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8 && resolved_dim_count == 2)
                           || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8 && resolved_dim_count == 1));
    }

    if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vx_tensor_create_params_t tensor_create_params;
        vx_uint32 dims          = TENSOR_DIM_NUM(input);
        vx_uint32 width         = TENSOR_VIEW_SIZE_INDEX(input, 0);
        vx_uint32 height        = dims > 1 ? TENSOR_VIEW_SIZE_INDEX(input, 1) : 1;
        vx_uint32 depth         = dims > 2 ? TENSOR_VIEW_SIZE_INDEX(input, 2) : 1;
        vx_uint32 batch         = dims > 3 ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;
        vx_uint32 sizes[4]      = {width, height, depth, batch};
        vx_uint32 new_sizes[4]  = {width, height, depth, batch};
        vx_uint32 perm_array[4] = {0, 1, 2, 3};
        vx_tensor transTensor   = NULL;
        vx_tensor dst           = NULL;
        vx_bool   enable_trans  = vx_false_e;
        vx_bool   enable_axis  = vx_false_e;

        if (resolved_dim_count == 1)
        {
            enable_axis = vx_true_e;
        }

        if (resolved_dim[0] + resolved_dim[1] != 1 && resolved_dim_count == 2)
        {
            enable_trans = vx_true_e;

            if (resolved_dim[0] + resolved_dim[1] == 2)
            {
                perm_array[0] = 0;
                perm_array[1] = 2;
                perm_array[2] = 1;
                perm_array[3] = 3;
            }
            else if (resolved_dim[0] + resolved_dim[1] == 3)
            {
                if (abs(resolved_dim[0] - resolved_dim[1]) == 3)
                {
                    perm_array[0] = 0;
                    perm_array[1] = 3;
                    perm_array[2] = 1;
                    perm_array[3] = 2;
                }
                else
                {
                    perm_array[0] = 1;
                    perm_array[1] = 2;
                    perm_array[2] = 0;
                    perm_array[3] = 3;
                }
            }
            else if (resolved_dim[0] + resolved_dim[1] == 4)
            {
                perm_array[0] = 1;
                perm_array[1] = 3;
                perm_array[2] = 0;
                perm_array[3] = 2;
            }
            else if (resolved_dim[0] + resolved_dim[1] == 5)
            {
                perm_array[0] = 2;
                perm_array[1] = 3;
                perm_array[2] = 0;
                perm_array[3] = 1;
            }
        }
        else if (resolved_dim[0] != 0 && resolved_dim_count == 1)
        {

            if(node->base.context->evisNoInst.supportEVIS)
            {
                if (resolved_dim[0] == 3)
                {
                    enable_trans = vx_true_e;
                    input_axis = 0;
                    perm_array[0] = 3;
                    perm_array[1] = 0;
                    perm_array[2] = 1;
                    perm_array[3] = 2;
                }
                else
                {
                    transTensor = input;
                    input_axis = resolved_dim[0];
                }
            }
            else
            {
                enable_trans = vx_true_e;
                input_axis = 0;
                if (resolved_dim[0] == 1)
                {
                    perm_array[0] = 1;
                    perm_array[1] = 0;
                    perm_array[2] = 2;
                    perm_array[3] = 3;
                }
                else if (resolved_dim[0] == 2)
                {
                    perm_array[0] = 2;
                    perm_array[1] = 0;
                    perm_array[2] = 1;
                    perm_array[3] = 3;
                }
                else if (resolved_dim[0] == 3)
                {
                    perm_array[0] = 3;
                    perm_array[1] = 0;
                    perm_array[2] = 1;
                    perm_array[3] = 2;
                }
            }
        }
        else
        {
            transTensor = input;
        }

        if (enable_trans)
        {
            for (i = 0; i < 4; i ++)
            {
                new_sizes[i] = sizes[perm_array[i]];
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = dims;
            tensor_create_params.sizes = new_sizes;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(input);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(input);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(input);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(input);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input);
            }

            transTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);

            tensor_mean_layer->base.temp_tensors[tmpTensorIndex++] = transTensor;

            if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP_TRANSPOSE) &&
                vxnneIsTPSupportFormat(context, input, VX_NULL, transTensor))
            {
                vx_op_param_s conv = {0};
                vx_uint32 dnum = 4;

                status = vxnneOperation_Initialize(&tensor_mean_layer->tensor_mean_trans_tp_operation.base,
                    &tensor_mean_layer->base,
                    VXNNE_OPERATION_TARGET_TP,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    VX_NULL,
                    vxnneOperation_TP_Deinitialize,
                    1,
                    0);
                if (status != VX_SUCCESS) goto exit;

                conv.pad_x_left = 0;
                conv.pad_y_top = 0;
                conv.pool_size_x = 0;
                conv.pool_size_y = 0;
                conv.pool_stride = 1;
                conv.enable_relu = vx_false_e;
                conv.conv_rounding_type = 0;
                conv.pad_mode = VX_PAD_CONSTANT;
                conv.pad_const = 0;
                conv.tpType = TP_TRANSPOSE;
                conv.other_ref = (vx_reference)input;
                conv.data_buff = gcvNULL;
                conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
                conv.tp_value->u32[0] = dnum;
                conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(vx_uint32) * dnum);
                vxMemCopy(conv.tp_value->p8[0], perm_array, sizeof(vx_uint32) * dnum);

                vxMemCopy(&tensor_mean_layer->tensor_mean_trans_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));

                vxnneLayer_SetOperation(
                    &tensor_mean_layer->base,
                    &tensor_mean_layer->tensor_mean_trans_tp_operation.base,
                    operationIdx++);

                tensor_mean_layer->tensor_mean_trans_tp_operation.input  = input;
                tensor_mean_layer->tensor_mean_trans_tp_operation.output = transTensor;

                vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_trans_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_trans_tp_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            }
            else
            {
                vxnne_shader_executable shaderExecutable = VX_NULL;

                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable = vxnneTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, perm_array, dims, transTensor);
                }
                else
                {
                    shaderExecutable = vxnneGPUTensorTransposeShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_TRANSPOSE, &node->kernelAttributes.borderMode, input, perm_array, dims, transTensor);
                }

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&tensor_mean_layer->tensor_mean_trans_sh_operation,
                    &tensor_mean_layer->base,
                    VXNNE_OPERATOR_TENSOR_TRANS,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_trans_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_trans_sh_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &tensor_mean_layer->base,
                    &tensor_mean_layer->tensor_mean_trans_sh_operation.base,
                    operationIdx++);
            }
        }

        if (resolved_dim_count == 2)
        {
            vx_uint32 output_dims         = dims < 3 ? 3 : dims;

            sizes[0] = 1;
            sizes[1] = 1;
            sizes[2] = new_sizes[2];
            sizes[3] = new_sizes[3];
            dst = vxoTensor_ReshapeTensor(output, (vx_int32*)sizes, output_dims);

            tensor_mean_layer->base.temp_tensors[tmpTensorIndex++] = dst;
        }
        else if (resolved_dim_count == 1)
        {
            vx_uint32 output_dims         = dims < 3 ? 3 : dims;

            sizes[0] = new_sizes[0];
            sizes[1] = new_sizes[1];
            sizes[2] = new_sizes[2];
            sizes[3] = new_sizes[3];
            sizes[input_axis] = 1;
            dst = vxoTensor_ReshapeTensor(output, (vx_int32*)sizes, output_dims);

            tensor_mean_layer->base.temp_tensors[tmpTensorIndex++] = dst;
        }

        if (resolved_dim_count == 2)
        {
            vxnne_shader_executable shaderExecutable = NULL;
            vx_bool   enable_tf_quantize  = (vx_bool)(inputFormat == VX_TYPE_UINT8);
            vx_bool   enable_int16_sh     = (vx_bool)(inputFormat == VX_TYPE_INT16);
            vx_uint32 stride              = 1;
            vx_scalar stride_s            = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &stride);
            vx_scalar poolSizeX           = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &new_sizes[0]);
            vx_scalar poolSizeY           = vxCreateScalar(node->base.context, VX_TYPE_UINT32, &new_sizes[1]);
            vx_uint32 pad_x_left          = 0;
            vx_uint32 pad_y_top           = 0;
            vx_uint32 batch               = new_sizes[3];

            if (stride_s == NULL || poolSizeX == NULL || poolSizeY == NULL)
            {
                if (stride_s)  vxReleaseScalar(&stride_s);
                if (poolSizeX) vxReleaseScalar(&poolSizeX);
                if (poolSizeY) vxReleaseScalar(&poolSizeY);
                status = VX_FAILURE;

                goto exit;
            }

            if(node->base.context->evisNoInst.supportEVIS)
            {
                if(enable_tf_quantize)
                    shaderExecutable = vxnneGetAvgPooling_UInt8ShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING_UINT8, &node->kernelAttributes.borderMode, transTensor, NULL, stride_s, poolSizeX, poolSizeY, pad_x_left, pad_y_top, NULL, VX_NN_ACTIVATION_NONE, dst);

                else if(enable_int16_sh)
                    shaderExecutable = vxnneGetAvgPooling_Int16ShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING_INT16, &node->kernelAttributes.borderMode,
                    transTensor, NULL, stride_s, poolSizeX, poolSizeY, pad_x_left, pad_y_top, NULL, VX_NN_ACTIVATION_NONE, dst);
                else
                    shaderExecutable = vxnneGetAvgPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING, &node->kernelAttributes.borderMode,
                    transTensor, NULL, stride_s, poolSizeX, poolSizeY, pad_x_left, pad_y_top, NULL, dst);
            }
            else
            {
                shaderExecutable = vxnneGetGPUAvgPoolingShaderExecutable(node->base.context, VXNNE_KERNEL_AVGPOOLING, &node->kernelAttributes.borderMode,
                    transTensor, NULL, stride_s, stride_s, poolSizeX, poolSizeY, pad_x_left, pad_y_top, pad_x_left, pad_y_top, NULL, dst);
            }

            if (stride_s)  vxReleaseScalar(&stride_s);
            if (poolSizeX) vxReleaseScalar(&poolSizeX);
            if (poolSizeY) vxReleaseScalar(&poolSizeY);

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&tensor_mean_layer->tensor_mean_pool_sh_operation,
                &tensor_mean_layer->base,
                VXNNE_OPERATOR_POOLING,
                batch,
                shaderExecutable);

            if (status != VX_SUCCESS)
            {
                goto exit;
            }

            vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_pool_sh_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_pool_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_mean_layer->base,
                &tensor_mean_layer->tensor_mean_pool_sh_operation.base,
                operationIdx++);
        }

        if (enable_axis)
        {
            vxnne_shader_executable shaderExecutable = NULL;
            vx_float32              axis_coef        = 1.0f / (vx_float32)(TENSOR_VIEW_SIZE_INDEX(transTensor,input_axis));
            vx_uint32               batch            = new_sizes[3];

            if(node->base.context->evisNoInst.supportEVIS)
            {
                shaderExecutable = vxnneGetTensorMeanAxisShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MEAN_AXIS0, &node->kernelAttributes.borderMode, axis_coef, transTensor, dst, input_axis);
            }
            else
            {
                shaderExecutable = vxnneGetGPUTensorMeanAxis0ShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_MEAN_AXIS0, &node->kernelAttributes.borderMode, axis_coef, transTensor, dst);
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&tensor_mean_layer->tensor_mean_axis0_sh_operation,
                &tensor_mean_layer->base,
                VXNNE_OPERATOR_TENSOR_MEAN,
                batch,
                shaderExecutable);

            if (status != VX_SUCCESS)
            {
                goto exit;
            }

            vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_axis0_sh_operation.base, (vx_reference)transTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_axis0_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_mean_layer->base,
                &tensor_mean_layer->tensor_mean_axis0_sh_operation.base,
                operationIdx++);
        }

        tensor_mean_layer->base.num_temp_tensors = tmpTensorIndex;
    }
    else
    {
        vxnneOperation_Initialize(&tensor_mean_layer->tensor_mean_sw_operation.base,
            &tensor_mean_layer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_MEAN,
            vxnneExecuteSWTensorMean,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &tensor_mean_layer->base,
            &tensor_mean_layer->tensor_mean_sw_operation.base,
            0);

        tensor_mean_layer->tensor_mean_sw_operation.input     = input;
        tensor_mean_layer->tensor_mean_sw_operation.axis      = axis;
        tensor_mean_layer->tensor_mean_sw_operation.keep_dims = keep_dims;
        tensor_mean_layer->tensor_mean_sw_operation.output    = output;

        vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_mean_layer->tensor_mean_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
    }

    node->layer = &tensor_mean_layer->base;
    return status;

exit:
    if (tensor_mean_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_mean_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorMean_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
*                                                 TENSOR STRIDE SLICE
***************************************************************************************************************************/

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorStrideSlice(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorStrideSlice_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorStrideSlice_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_int32 vxoNNTensorStrideSlice_GetAxisValue(vx_int32 value, vx_uint32 dimension_size)
{
    vx_int32 axis_vlaue = 0;
    if (value < 0)
        axis_vlaue = value + dimension_size;
    else
        axis_vlaue = value;
    return axis_vlaue;
}

VX_PRIVATE_API vx_int32 vxoNNTensorStrideSlice_MaskStartValue(vx_int32 stride, vx_uint32 dimension_size)
{
    vx_int32 start_vlaue = 0;
    if (stride > 0)
        start_vlaue = 0;
    else
        start_vlaue = dimension_size - 1;
    return start_vlaue;
}

VX_PRIVATE_API vx_int32 vxoNNTensorStrideSlice_MaskStopValue(vx_int32 stride, vx_uint32 dimension_size)
{
    vx_int32 stop_vlaue = 0;
    if (stride > 0)
        stop_vlaue = dimension_size;
    else
        stop_vlaue = -1;
    return stop_vlaue;
}

VX_PRIVATE_API vx_int32 vxoNNTensorStrideSlice_ClampStop(vx_int32 stride, vx_int32 stop, vx_uint32 dimension_size)
{
    vx_int32 stop_vlaue = 0;
    if (stride > 0)
    {
        stop_vlaue = gcmCLAMP(stop, 0, (vx_int32)dimension_size);
    }
    else
    {
        stop_vlaue = gcmCLAMP(stop, -1, (vx_int32)dimension_size - 1);
    }
    return stop_vlaue;
}

VX_PRIVATE_API vx_status vxoNNTensorStrideSlice_getStartStopStride(vx_tensor input, vx_tensor begin_dims, vx_tensor end_dims, vx_tensor stride_dims, vx_scalar begin_mask, vx_scalar end_mask, vx_scalar shrink_axis_mask, vx_int32 start[4], vx_int32 stop[4], vx_int32 stride[4])
{
    vx_uint8_ptr begin_dims_base = VX_NULL, end_dims_base = VX_NULL, stride_dims_base = VX_NULL;

    vx_uint32 i = 0;
    vx_int32 int32_value = 0;
    vx_uint32 srcIdx = 0;

    vxoTensor_GetTensorViewMemory(begin_dims, (gctPOINTER*)&begin_dims_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(end_dims, (gctPOINTER*)&end_dims_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(stride_dims, (gctPOINTER*)&stride_dims_base, VX_NULL);

    for (i = 0; i < 4; i ++)
    {
        start[i]    = 0;
        stop[i]     = 1;
        stride[i]   = 1;
    }

    for (i = 0; i < TENSOR_VIEW_SIZE_INDEX(stride_dims, 0); ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(stride_dims), TENSOR_QUANT_TYPE(stride_dims), srcIdx++, stride_dims_base,
            TENSOR_POS(stride_dims), TENSOR_TF_ZEROPOINT(stride_dims), TENSOR_TF_SCALE(stride_dims));

        stride[i] = int32_value;
    }

    srcIdx = 0;
    for (i = 0; i < TENSOR_VIEW_SIZE_INDEX(begin_dims, 0); ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(begin_dims), TENSOR_QUANT_TYPE(begin_dims), srcIdx++, begin_dims_base,
            TENSOR_POS(begin_dims), TENSOR_TF_ZEROPOINT(begin_dims), TENSOR_TF_SCALE(begin_dims));

        start[i] = vxoNNTensorStrideSlice_GetAxisValue(int32_value, TENSOR_VIEW_SIZE_INDEX(input, i));
    }

    srcIdx = 0;
    for (i = 0; i < TENSOR_VIEW_SIZE_INDEX(end_dims, 0); ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(end_dims), TENSOR_QUANT_TYPE(end_dims), srcIdx++, end_dims_base,
            TENSOR_POS(end_dims), TENSOR_TF_ZEROPOINT(end_dims), TENSOR_TF_SCALE(end_dims));

        stop[i] = vxoNNTensorStrideSlice_GetAxisValue(int32_value, TENSOR_VIEW_SIZE_INDEX(input, i));
    }

    /*if the ith bit of mask is set, the start or stop will be the fullest possible range in that dimension.*/
    for (i = 0; i < 4; i ++)
    {
        if (begin_mask->value->n32 & (1 << i))
        {
            start[i] = vxoNNTensorStrideSlice_MaskStartValue(stride[i], TENSOR_VIEW_SIZE_INDEX(input, i));
        }

        start[i] = gcmCLAMP(start[i], 0, (vx_int32)(TENSOR_VIEW_SIZE_INDEX(input, i) - 1));

        if (shrink_axis_mask->value->n32 & (1 << i))
        {
            stop[i] = start[i] + 1;
        }

        if (end_mask->value->n32 & (1 << i))
        {
            stop[i] = vxoNNTensorStrideSlice_MaskStopValue(stride[i], TENSOR_VIEW_SIZE_INDEX(input, i));
        }

        stop[i] = vxoNNTensorStrideSlice_ClampStop(stride[i], stop[i], TENSOR_VIEW_SIZE_INDEX(input, i));
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxoNNTensorStrideSlice_getReverseAxis(vx_int32 start[4], vx_int32 stop[4], vx_int32 stride[4], vx_uint32 reverseAxis[4], vx_uint32 *numOfAxis, vx_uint32 *tensor_size)
{
    vx_uint32 i   = 0;
    vx_uint32 idx = 0;

    for (i = 0; i < 4; i ++)
    {
        reverseAxis[idx] = 0xcdcdcdcd;

        if (stride[i] < 0)
        {
            vx_uint32 start_axis    = tensor_size[i] - start[i] - 1;
            vx_uint32 stop_axis     = tensor_size[i] - stop[i] - 2;

            stride[i]   = abs(stride[i]);
            start[i]    = start_axis;
            stop[i]     = stop_axis;

            reverseAxis[idx] = i;
            idx++;
        }
    }

    *numOfAxis = idx;

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneExecuteSWTensorStrideSlice(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_stride_slice_operation transOperation = (vxnne_tensor_stride_slice_operation)operation;
    vx_tensor input = (vx_tensor)transOperation->input;
    vx_tensor begin_dims = (vx_tensor)transOperation->begin_dims;
    vx_tensor end_dims = (vx_tensor)transOperation->end_dims;
    vx_tensor stride_dims = (vx_tensor)transOperation->strides;
    vx_scalar begin_mask = (vx_scalar)transOperation->begin_mask;
    vx_scalar end_mask = (vx_scalar)transOperation->end_mask;
    vx_scalar shrink_axis_mask = (vx_scalar)transOperation->shrink_axis_mask;
    vx_tensor output = (vx_tensor)transOperation->output;
    vx_uint8_ptr input_base = VX_NULL, output_base = VX_NULL, begin_dims_base = VX_NULL, end_dims_base = VX_NULL, stride_dims_base = VX_NULL;

    vx_uint32 i = 0;
    vx_int32 int32_value = 0;
    vx_float32 float32_value = 0;
    vx_uint32 srcIdx = 0, dstIdx = 0;

    vx_int32 start_w = 0, start_h = 0, start_c = 0, start_n = 0;
    vx_int32 stop_w = 1, stop_h = 1, stop_c = 1, stop_n = 1;
    vx_int32 stride_w = 1, stride_h = 1, stride_c = 1, stride_n = 1;
    vx_int32 in_w = 0, in_h = 0, in_c = 0, in_n = 0;

    vx_uint32 shrink_axis_w = shrink_axis_mask->value->n32 & (1 << 0);
    vx_uint32 shrink_axis_h = shrink_axis_mask->value->n32 & (1 << 1);
    vx_uint32 shrink_axis_c = shrink_axis_mask->value->n32 & (1 << 2);
    vx_uint32 shrink_axis_n = shrink_axis_mask->value->n32 & (1 << 3);

    vxoTensor_GetTensorViewMemory(begin_dims, (gctPOINTER*)&begin_dims_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(end_dims, (gctPOINTER*)&end_dims_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(stride_dims, (gctPOINTER*)&stride_dims_base, VX_NULL);

    for (i = 0; i < stride_dims->dims[0]; ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(stride_dims), TENSOR_QUANT_TYPE(stride_dims),
            srcIdx++, stride_dims_base, TENSOR_POS(stride_dims), TENSOR_TF_ZEROPOINT(stride_dims), TENSOR_TF_SCALE(stride_dims));
        if (i == 0)
            stride_w = int32_value;
        if (i == 1)
            stride_h = int32_value;
        if (i == 2)
            stride_c = int32_value;
        if (i == 3)
            stride_n = int32_value;
    }

    srcIdx = 0;
    for (i = 0; i < begin_dims->dims[0]; ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(begin_dims), TENSOR_QUANT_TYPE(begin_dims),
            srcIdx++, begin_dims_base, TENSOR_POS(begin_dims), TENSOR_TF_ZEROPOINT(begin_dims), TENSOR_TF_SCALE(begin_dims));
        if (i == 0)
        {
            start_w = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[0]);
        }

        if (i == 1)
        {
            start_h = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[1]);
        }

        if (i == 2)
        {
            start_c = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[2]);
        }

        if (i == 3)
        {
            start_n = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[3]);
        }
    }

    srcIdx = 0;
    for (i = 0; i < end_dims->dims[0]; ++i)
    {
        int32_value = (vx_int32)vxnneGetDataExt(TENSOR_DATA_TYPE(end_dims), TENSOR_QUANT_TYPE(end_dims),
            srcIdx++, end_dims_base, TENSOR_POS(end_dims), TENSOR_TF_ZEROPOINT(end_dims), TENSOR_TF_SCALE(end_dims));
        if (i == 0)
        {
            stop_w = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[0]);
        }

        if (i == 1)
        {
            stop_h = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[1]);
        }

        if (i == 2)
        {
            stop_c = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[2]);
        }

        if (i == 3)
        {
            stop_n = vxoNNTensorStrideSlice_GetAxisValue(int32_value, input->dims[3]);
        }
    }

    /*if the ith bit of mask is set, the start or stop will be the fullest possible range in that dimension.*/
    if (begin_mask->value->n32 & 1 << 0)
    {
        start_w = vxoNNTensorStrideSlice_MaskStartValue(stride_w, input->dims[0]);
    }

    if (begin_mask->value->n32 & 1 << 1)
    {
        start_h = vxoNNTensorStrideSlice_MaskStartValue(stride_h, input->dims[1]);
    }

    if (begin_mask->value->n32 & 1 << 2)
    {
        start_c = vxoNNTensorStrideSlice_MaskStartValue(stride_c, input->dims[2]);
    }

    if (begin_mask->value->n32 & 1 << 3)
    {
        start_n = vxoNNTensorStrideSlice_MaskStartValue(stride_n, input->dims[3]);
    }

    start_w = gcmCLAMP(start_w, 0, (vx_int32)(input->dims[0] - 1));
    start_h = gcmCLAMP(start_h, 0, (vx_int32)(input->dims[1] - 1));
    start_c = gcmCLAMP(start_c, 0, (vx_int32)(input->dims[2] - 1));
    start_n = gcmCLAMP(start_n, 0, (vx_int32)(input->dims[3] - 1));

    if (shrink_axis_w)
    {
        stop_w = start_w + 1;
    }

    if (shrink_axis_h)
    {
        stop_h = start_h + 1;
    }

    if (shrink_axis_c)
    {
        stop_c = start_c + 1;
    }

    if (shrink_axis_n)
    {
        stop_n = start_n + 1;
    }

    if (end_mask->value->n32 & (1 << 0))
    {
        stop_w = vxoNNTensorStrideSlice_MaskStopValue(stride_w, input->dims[0]);
    }

    if (end_mask->value->n32 & (1 << 1))
    {
        stop_h = vxoNNTensorStrideSlice_MaskStopValue(stride_h, input->dims[1]);
    }

    if (end_mask->value->n32 & (1 << 2))
    {
        stop_c = vxoNNTensorStrideSlice_MaskStopValue(stride_c, input->dims[2]);
    }

    if (end_mask->value->n32 & (1 << 3))
    {
        stop_n = vxoNNTensorStrideSlice_MaskStopValue(stride_n, input->dims[3]);
    }

    stop_w = vxoNNTensorStrideSlice_ClampStop(stride_w, stop_w, input->dims[0]);
    stop_h = vxoNNTensorStrideSlice_ClampStop(stride_h, stop_h, input->dims[1]);
    stop_c = vxoNNTensorStrideSlice_ClampStop(stride_c, stop_c, input->dims[2]);
    stop_n = vxoNNTensorStrideSlice_ClampStop(stride_n, stop_n, input->dims[3]);

    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&input_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&output_base, VX_NULL);

    for (in_n = start_n; !((stride_n > 0) ? (in_n >= stop_n) : (in_n <= stop_n)); in_n += stride_n)
    {
        for (in_c = start_c; !((stride_c > 0) ? (in_c >= stop_c) : (in_c <= stop_c)); in_c += stride_c)
        {
            for (in_h = start_h; !((stride_h > 0) ? (in_h >= stop_h) : (in_h <= stop_h)); in_h += stride_h)
            {
                for (in_w = start_w; !((stride_w > 0) ? (in_w >= stop_w) : (in_w <= stop_w)); in_w += stride_w)
                {
                    srcIdx = ((in_n * input->dims[2] + in_c) * input->dims[1] + in_h) * input->dims[0] + in_w;
                    float32_value = vxnneGetDataExt(TENSOR_DATA_TYPE(input), TENSOR_QUANT_TYPE(input),
                        srcIdx, input_base, TENSOR_POS(input), TENSOR_TF_ZEROPOINT(input), TENSOR_TF_SCALE(input));
                    vxnneSaveDataExt(TENSOR_DATA_TYPE(output), TENSOR_QUANT_TYPE(output), dstIdx++, float32_value, output_base,
                        TENSOR_POS(output), TENSOR_TF_ZEROPOINT(output), TENSOR_TF_SCALE(output), TENSOR_ROUNDING_MODE(output));
                }
            }
        }
    }

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_bool vxnneExecuteSWTSS_FullPositiveSeq(vx_tensor dims, vx_int32 mask)
{
    vx_uint32 i = 0;

    if (mask != 0)
        return vx_false_e;
    for (i = 0; i < TENSOR_SIZE_INDEX(dims, 0); i++)
    {
        if ((vx_int32)VX_GET_DATA_FROM_TENSOR(dims, i) < 0)
            return vx_false_e;
    }

    return vx_true_e;
}

#define STRIDED_SLICE_CHECK_TP_SUPPORT \
        vxnneExecuteSWTSS_FullPositiveSeq(begin_dims, begin_mask->value->n32) && /* only support positive sequence currently*/ \
        vxnneExecuteSWTSS_FullPositiveSeq(end_dims, end_mask->value->n32) && \
        vxnneExecuteSWTSS_FullPositiveSeq(stride_dims, shrink_axis_mask->value->n32)

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorStrideSlice_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;

    vx_tensor input                 = (vx_tensor)parameters[0];
    vx_tensor begin_dims            = (vx_tensor)parameters[1];
    vx_tensor end_dims              = (vx_tensor)parameters[2];
    vx_tensor stride_dims           = (vx_tensor)parameters[3];
    vx_scalar begin_mask            = (vx_scalar)parameters[4];
    vx_scalar end_mask              = (vx_scalar)parameters[5];
    vx_scalar shrink_axis_mask      = (vx_scalar)parameters[6];
    vx_tensor output                = (vx_tensor)parameters[7];
    vx_uint32 batchCount            = 1;
    vx_int32  start[4]              = {0};
    vx_int32  stop[4]               = {1};
    vx_int32  stride[4]             = {1};
    vx_uint32 reverseAxis[4]        = {0xcdcd};
    vx_uint32 numOfAxis             = 0;
    vx_uint32 opIdx                 = 0;
    vx_uint32 input_size[4]         = {0};
    vx_uint32 i                     = 0;
    vx_enum   inputFormat           = TENSOR_DATA_TYPE(input);
    vx_enum   outputFormat          = TENSOR_DATA_TYPE(output);
    vx_uint32 batch                 = TENSOR_VIEW_DIM_NUM(input) > 3 ? TENSOR_VIEW_SIZE_INDEX(input, 3) : 1;
    vx_bool   enable_sh_crop        = vx_false_e;
    vx_bool   enable_sh_reverse     = vx_false_e;
    vx_bool   shExe_flag            = vx_false_e;

    vxnne_tensor_stride_slice_layer tensor_stride_slice_layer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_stride_slice_layer_s), (gctPOINTER*)&tensor_stride_slice_layer);
    if (!tensor_stride_slice_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_stride_slice_layer, sizeof(vxnne_tensor_stride_slice_layer_s));

    vxnneLayer_Initialize(&tensor_stride_slice_layer->base,
        "TensorStrideSlice",
        node,
        vxmOPERATION_COUNT(tensor_stride_slice_layer),
        tensor_stride_slice_layer->operations,
        VX_NULL);

    for (i = 0; i < TENSOR_DIM_NUM(input); i++)
    {
        input_size[i] = TENSOR_VIEW_SIZE_INDEX(input, i);
    }

    vxoNNTensorStrideSlice_getStartStopStride(input, begin_dims, end_dims, stride_dims, begin_mask, end_mask, shrink_axis_mask, start, stop, stride);

    enable_sh_crop = (vx_bool)((inputFormat != VX_TYPE_FLOAT32 && outputFormat != VX_TYPE_FLOAT32) && gcoMATH_Absolute((vx_float32)stride[0]) == gcoMATH_Absolute((vx_float32)stride[1]) && gcoMATH_Absolute((vx_float32)stride[0]) == gcoMATH_Absolute((vx_float32)stride[2]) && gcoMATH_Absolute((vx_float32)stride[0]) == 1 && batch == 1);


    vxoNNTensorStrideSlice_getReverseAxis(start, stop, stride, reverseAxis, &numOfAxis, input_size);

    enable_sh_reverse    = (vx_bool)(numOfAxis > 0 && numOfAxis < 4);

    shExe_flag = (vx_bool)((_IsSameType(input, output) && batch == 1) || enable_sh_crop);

    if (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(node->base.context, input, VX_NULL, output) && STRIDED_SLICE_CHECK_TP_SUPPORT
        )
    {
        vx_op_param_s conv = { 0 };

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;

        vxnneOperation_Initialize(&tensor_stride_slice_layer->tensor_stride_slice_tp_operation.base,
            &tensor_stride_slice_layer->base,
            VXNNE_OPERATION_TARGET_TP,
            VXNNE_OPERATOR_TENSOR_STRIDE_SLICE,
            VX_NULL,
            vxnneOperation_TP_Deinitialize,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &tensor_stride_slice_layer->base,
            &tensor_stride_slice_layer->tensor_stride_slice_tp_operation.base,
            0);

        tensor_stride_slice_layer->tensor_stride_slice_tp_operation.input = input;
        tensor_stride_slice_layer->tensor_stride_slice_tp_operation.output = output;

        vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_tp_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        conv.tpType = TP_TENSOR_STRIDED_SLICE;
        conv.other_ref = (vx_reference)input;
        conv.data_buff = gcvNULL;

        conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
        /*
         * u32[0]: begin_dims[0]
         * u32[1]: begin_dims[1]
         * u32[2]: end_dims[0]
         * u32[3]: end_dims[1]
         * u32[4]: stride_dims[0]
         * u32[5]: stride_dims[1]
         */
        conv.tp_value->u32[0] = (vx_uint32)VX_GET_DATA_FROM_TENSOR(begin_dims, 0);
        conv.tp_value->u32[1] = TENSOR_VIEW_SIZE_INDEX(begin_dims, 0) > 1 ? (vx_uint32)VX_GET_DATA_FROM_TENSOR(begin_dims, 1) : 0;
        conv.tp_value->u32[2] = (vx_uint32)VX_GET_DATA_FROM_TENSOR(end_dims, 0);
        conv.tp_value->u32[3] = TENSOR_VIEW_SIZE_INDEX(end_dims, 0) > 1 ? (vx_uint32)VX_GET_DATA_FROM_TENSOR(end_dims, 1) : 1;
        conv.tp_value->u32[4] = (vx_uint32)VX_GET_DATA_FROM_TENSOR(stride_dims, 0); /* stride x*/
        conv.tp_value->u32[5] = TENSOR_VIEW_SIZE_INDEX(stride_dims, 0) > 1 ? (vx_uint32)VX_GET_DATA_FROM_TENSOR(stride_dims, 1) : 1; /* stride y*/

        memcpy(&tensor_stride_slice_layer->tensor_stride_slice_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));
    }
    else if(shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
    {
        vxnne_shader_executable shaderExecutable = VX_NULL;
        vx_tensor tmpTensor = NULL;

        if (enable_sh_reverse)
        {
            vx_uint32 sizes[4] = {1};
            vx_uint32 dims     = TENSOR_DIM_NUM(output);
            vx_uint32 idx      = 0;
            vx_tensor_create_params_t tensor_create_params;
            vx_context context = vxGetContext((vx_reference)node);

            for (idx = 0; idx < dims; idx++)
            {
                sizes[idx] = TENSOR_VIEW_SIZE_INDEX(input, idx);
            }

            gcoOS_MemFill(&tensor_create_params, 0, sizeof(vx_tensor_create_params_t));
            tensor_create_params.num_of_dims = dims;
            tensor_create_params.sizes = sizes;
            tensor_create_params.data_format = TENSOR_DATA_TYPE(input);
            tensor_create_params.quant_format = TENSOR_QUANT_TYPE(input);
            if (tensor_create_params.quant_format == VX_QUANT_DYNAMIC_FIXED_POINT)
            {
                tensor_create_params.quant_data.dfp.fixed_point_pos = TENSOR_POS(input);
            }
            else
            {
                tensor_create_params.quant_data.affine.scale = TENSOR_TF_SCALE(input);
                tensor_create_params.quant_data.affine.zeroPoint = TENSOR_TF_ZEROPOINT(input);
            }

            tmpTensor = vxoTensor_CreateTensor(node->base.context, node->graph, &tensor_create_params, vx_true_e);

            tensor_stride_slice_layer->base.temp_tensors[0]  = tmpTensor;
            tensor_stride_slice_layer->base.num_temp_tensors = 1;


            if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP_REVERSE) &&
                vxnneIsTPSupportFormat(context, input, VX_NULL, tmpTensor))
            {
                vx_op_param_s conv = {0};
                vx_int32 axis[VX_CONTEXT_TENSOR_MAX_DIMENSION] = {0};

                for (i = 0; i < numOfAxis; i++)
                {
                    axis[i] = reverseAxis[i];
                }

                status = vxnneOperation_Initialize(&tensor_stride_slice_layer->tensor_reverse_tp_operation.base,
                    &tensor_stride_slice_layer->base,
                    VXNNE_OPERATION_TARGET_TP,
                    VXNNE_OPERATOR_TENSOR_REVERSE,
                    VX_NULL,
                    vxnneOperation_TP_Deinitialize,
                    batchCount,
                    0);
                if (status != VX_SUCCESS) goto exit;
                vxnneLayer_SetOperation(&tensor_stride_slice_layer->base, &tensor_stride_slice_layer->tensor_reverse_tp_operation.base, opIdx++);
                tensor_stride_slice_layer->tensor_reverse_tp_operation.input  = input;
                tensor_stride_slice_layer->tensor_reverse_tp_operation.output = tmpTensor;

                vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_reverse_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_reverse_tp_operation.base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                conv.pad_x_left = 0;
                conv.pad_y_top = 0;
                conv.pool_size_x = 0;
                conv.pool_size_y = 0;
                conv.pool_stride = 1;
                conv.enable_relu = vx_false_e;
                conv.conv_rounding_type = 0;
                conv.pad_mode = VX_PAD_CONSTANT;
                conv.pad_const = 0;
                conv.tpType = TP_REVERSE;
                conv.data_buff = gcvNULL;
                conv.other_ref = (vx_reference)input;
                conv.tp_value = (vx_tp_value_cmd)vxAllocateAndZeroMemory(sizeof(vx_tp_value_cmd_s));
                conv.tp_value->u32[0] = numOfAxis;
                conv.tp_value->p8[0] = (vx_uint8_ptr)vxAllocateAndZeroMemory(sizeof(axis));
                vxMemCopy(conv.tp_value->p8[0], axis, sizeof(axis));

                vxMemCopy(&tensor_stride_slice_layer->tensor_reverse_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));
            }
            else
            {
                if(node->base.context->evisNoInst.supportEVIS)
                {
                    shaderExecutable = vxnneGetReverseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_REVERSE, &node->kernelAttributes.borderMode,
                        input, tmpTensor, numOfAxis, reverseAxis);
                }
                else
                {
                    shaderExecutable = vxnneGetGPUReverseShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_REVERSE, &node->kernelAttributes.borderMode,
                        input, tmpTensor, numOfAxis, reverseAxis);
                }

                if (!shaderExecutable)
                {
                    status = VX_FAILURE;
                    goto exit;
                }
                status = vxnneShaderOperation_Initialize(&tensor_stride_slice_layer->tensor_reverse_sh_operation,
                    &tensor_stride_slice_layer->base,
                    VXNNE_OPERATOR_TENSOR_STRIDE_SLICE,
                    batchCount,
                    shaderExecutable);

                if (status != VX_SUCCESS)
                    goto exit;

                vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_reverse_sh_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
                vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_reverse_sh_operation.base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_OUTPUT);

                vxnneLayer_SetOperation(
                    &tensor_stride_slice_layer->base,
                    &tensor_stride_slice_layer->tensor_reverse_sh_operation.base,
                    opIdx++);
            }
        }
        else
        {
            tmpTensor = input;
        }

        if (enable_sh_crop)
        {
            if(node->base.context->evisNoInst.supportEVIS)
            {
                shaderExecutable = vxnneGetTensorCropShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_CROP, &node->kernelAttributes.borderMode, start, stop, tmpTensor, output);
            }
            else
            {
                shaderExecutable = vxnneGetGPUTensorCropShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_CROP, &node->kernelAttributes.borderMode, start, stop, tmpTensor, output);
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&tensor_stride_slice_layer->tensor_crop_sh_operation,
                &tensor_stride_slice_layer->base,
                VXNNE_OPERATOR_TENSOR_STRIDE_SLICE,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_crop_sh_operation.base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_crop_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_stride_slice_layer->base,
                &tensor_stride_slice_layer->tensor_crop_sh_operation.base,
                opIdx++);
        }
        else
        {
            if(node->base.context->evisNoInst.supportEVIS)
            {
                shaderExecutable = vxnneGetTensorStridedSliceShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_STRIDE_SLICE, &node->kernelAttributes.borderMode, start, stop, stride, tmpTensor, output);
            }
            else
            {
                shaderExecutable = vxnneGetGPUTensorStridedSliceShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_STRIDE_SLICE, &node->kernelAttributes.borderMode, start, stop, stride, tmpTensor, output);
            }

            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }
            status = vxnneShaderOperation_Initialize(&tensor_stride_slice_layer->tensor_stride_slice_sh_operation,
                &tensor_stride_slice_layer->base,
                VXNNE_OPERATOR_TENSOR_STRIDE_SLICE,
                batchCount,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_sh_operation.base, (vx_reference)tmpTensor, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_sh_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

            vxnneLayer_SetOperation(
                &tensor_stride_slice_layer->base,
                &tensor_stride_slice_layer->tensor_stride_slice_sh_operation.base,
                opIdx++);
        }
    }
    else
    {
        vxnneOperation_Initialize(&tensor_stride_slice_layer->tensor_stride_slice_sw_operation.base,
            &tensor_stride_slice_layer->base,
            VXNNE_OPERATION_TARGET_SW,
            VXNNE_OPERATOR_TENSOR_STRIDE_SLICE,
            vxnneExecuteSWTensorStrideSlice,
            VX_NULL,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &tensor_stride_slice_layer->base,
            &tensor_stride_slice_layer->tensor_stride_slice_sw_operation.base,
            0);

        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.input       = input;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.begin_dims  = begin_dims;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.end_dims    = end_dims;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.strides     = stride_dims;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.begin_mask  = begin_mask;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.end_mask    = end_mask;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.shrink_axis_mask = shrink_axis_mask;
        tensor_stride_slice_layer->tensor_stride_slice_sw_operation.output      = output;

        vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_stride_slice_layer->tensor_stride_slice_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

    }

    node->layer = &tensor_stride_slice_layer->base;
    return status;

exit:
    if (tensor_stride_slice_layer)
    {
        if (tensor_stride_slice_layer->tensor_reverse_tp_operation.base.parameter.tp_value)
        {
            if (tensor_stride_slice_layer->tensor_reverse_tp_operation.base.parameter.tp_value->p8[0])
            {
                gcoOS_Free(NULL, (gctPOINTER)tensor_stride_slice_layer->tensor_reverse_tp_operation.base.parameter.tp_value->p8[0]);
            }
            gcoOS_Free(NULL, (gctPOINTER)tensor_stride_slice_layer->tensor_reverse_tp_operation.base.parameter.tp_value);
        }
        gcoOS_Free(NULL, (gctPOINTER)tensor_stride_slice_layer);
    }
    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorStrideSlice_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

/***************************************************************************************************************************
*                                                 TENSOR SQUEEZE
***************************************************************************************************************************/

VX_PRIVATE_API vx_status VX_CALLBACK vxoBaseKernel_NNTensorSqueeze(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSqueeze_ValidateInput(vx_node node, vx_uint32 index)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSqueeze_ValidateOutput(vx_node node, vx_uint32 index, vx_meta_format_s *ptr)
{
    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status vxnneExecuteSWTensorSqueeze(struct _vxnne_operation_s *operation)
{
    vxnne_tensor_squeeze_operation transOperation = (vxnne_tensor_squeeze_operation)operation;

    vx_tensor input        = (vx_tensor)transOperation->input;
    /*vx_tensor squeeze_dims = (vx_tensor)transOperation->squeeze_dims;*/
    vx_tensor output       = (vx_tensor)transOperation->output;
    vx_size size = 0;
    vx_uint8_ptr input_base = VX_NULL, output_base = VX_NULL;

    vxoTensor_GetTensorWholeSize(input, &size);
    vxoTensor_GetTensorViewMemory(input, (gctPOINTER*)&input_base, VX_NULL);
    vxoTensor_GetTensorViewMemory(output, (gctPOINTER*)&output_base, VX_NULL);

    memcpy(output_base, input_base, (vx_uint32)size);

    return VX_SUCCESS;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSqueeze_Initializer(vx_node node, const vx_reference parameters[], vx_uint32 num)
{
    vx_status status = VX_SUCCESS;
    vx_context context = vxGetContext((vx_reference)node);

    vx_tensor input         = (vx_tensor)parameters[0];
    vx_tensor squeeze_dims  = (vx_tensor)parameters[1];
    vx_tensor output        = (vx_tensor)parameters[2];
    vx_uint32 batchCount = TENSOR_SIZE_INDEX(input, 3);
    vx_enum   inputFormat   = TENSOR_DATA_TYPE(input);
    vx_enum   outputFormat  = TENSOR_DATA_TYPE(output);
    vx_bool   shExe_flag    = vx_false_e;

    vxnne_tensor_squeeze_layer tensor_squeeze_layer = VX_NULL;

    /* destroy the existing layer */
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
        node->layer = VX_NULL;
    }

    gcoOS_Allocate(gcvNULL, sizeof(vxnne_tensor_squeeze_layer_s), (gctPOINTER*)&tensor_squeeze_layer);
    if (!tensor_squeeze_layer)
    {
        status = VX_ERROR_NO_MEMORY;
        vxError("allocate memory fail at function %s line %d", __FUNCTION__, __LINE__);
        goto exit;
    }

    gcoOS_ZeroMemory(tensor_squeeze_layer, sizeof(vxnne_tensor_squeeze_layer_s));

    vxnneLayer_Initialize(&tensor_squeeze_layer->base,
        "TensorSqueeze",
        node,
        vxmOPERATION_COUNT(tensor_squeeze_layer),
        tensor_squeeze_layer->operations,
        VX_NULL);

    if(context->evisNoInst.supportEVIS)
    {
        shExe_flag = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                            || (inputFormat == VX_TYPE_INT16 && outputFormat == VX_TYPE_INT16)
                            || (inputFormat == VX_TYPE_INT8 && outputFormat == VX_TYPE_INT8)
                            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16));
    }
    else
    {
        shExe_flag = (vx_bool)((inputFormat == VX_TYPE_FLOAT16 && outputFormat == VX_TYPE_FLOAT16)
                            || (inputFormat == VX_TYPE_FLOAT32 && outputFormat == VX_TYPE_FLOAT32)
                            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_UINT8)
                            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT32)
                            || (inputFormat == VX_TYPE_UINT8 && outputFormat == VX_TYPE_FLOAT16));
    }

    if (vxoContext_IsFeatureAvailable(context, VX_NN_FEATURE_TP) &&
        vxnneIsTPSupportFormat(context, input, VX_NULL, output))
    {
        vx_op_param_s conv = { 0 };

        conv.pad_x_left = 0;
        conv.pad_y_top = 0;
        conv.pool_size_x = 0;
        conv.pool_size_y = 0;
        conv.pool_stride = 1;
        conv.enable_relu = vx_false_e;
        conv.pad_mode = VX_PAD_CONSTANT;
        conv.pad_const = 0;

        vxnneOperation_Initialize(&tensor_squeeze_layer->tensor_squeeze_tp_operation.base,
            &tensor_squeeze_layer->base,
            VXNNE_OPERATION_TARGET_TP,
            VXNNE_OPERATOR_TENSOR_SQUEEZE,
            VX_NULL,
            vxnneOperation_TP_Deinitialize,
            batchCount,
            0);

        vxnneLayer_SetOperation(
            &tensor_squeeze_layer->base,
            &tensor_squeeze_layer->tensor_squeeze_tp_operation.base,
            0);

        tensor_squeeze_layer->tensor_squeeze_tp_operation.input = input;
        tensor_squeeze_layer->tensor_squeeze_tp_operation.output = output;

        vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_tp_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
        vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_tp_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);

        conv.tpType = TP_TENSOR_SQUEEZE;
        conv.other_ref = (vx_reference)input;
        conv.data_buff = gcvNULL;

        memcpy(&tensor_squeeze_layer->tensor_squeeze_tp_operation.base.parameter, &conv, sizeof(vx_op_param_s));
    }
    else
    {
        if (shExe_flag && (vxoContext_IsFeatureAvailable(node->base.context, VX_NN_FEATURE_SHADER)))
        {
            vxnne_shader_executable shaderExecutable = VX_NULL;
            vx_tensor  src          = NULL;
            vx_tensor  dst          = NULL;
            vx_uint32 sizes[VX_CONTEXT_TENSOR_MAX_DIMENSION];
            vx_uint32 dims = 0;

            vxoElementOptimization_GetTensorShape(input, sizes, &dims);

            src        = vxoTensor_ReshapeTensor(input, (vx_int32*)sizes, dims);
            dst        = vxoTensor_ReshapeTensor(output, (vx_int32*)sizes, dims);

            tensor_squeeze_layer->base.temp_tensors[0]  = src;
            tensor_squeeze_layer->base.temp_tensors[1]  = dst;
            tensor_squeeze_layer->base.num_temp_tensors = 2;

            if(node->base.context->evisNoInst.supportEVIS)
            {
                shaderExecutable = vxnneTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, src, dst);
            }
            else
            {
                shaderExecutable = vxnneGPUTensorCopyShaderExecutable(node->base.context, VXNNE_KERNEL_TENSOR_COPY, &node->kernelAttributes.borderMode, src, dst);
            }
            if (!shaderExecutable)
            {
                status = VX_FAILURE;
                goto exit;
            }

            status = vxnneShaderOperation_Initialize(&tensor_squeeze_layer->tensor_squeeze_sh_operation,
                &tensor_squeeze_layer->base,
                VXNNE_OPERATOR_TENSOR_COPY,
                1,
                shaderExecutable);

            if (status != VX_SUCCESS)
                goto exit;

            vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_sh_operation.base, (vx_reference)src, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_sh_operation.base, (vx_reference)dst, VXNNE_OPERATION_REFENRENCE_OUTPUT);
            vxnneLayer_SetOperation(&tensor_squeeze_layer->base, &tensor_squeeze_layer->tensor_squeeze_sh_operation.base, 0);
        }
        else
        {
            vxnneOperation_Initialize(&tensor_squeeze_layer->tensor_squeeze_sw_operation.base,
                &tensor_squeeze_layer->base,
                VXNNE_OPERATION_TARGET_SW,
                VXNNE_OPERATOR_TENSOR_SQUEEZE,
                vxnneExecuteSWTensorSqueeze,
                VX_NULL,
                batchCount,
                0);

            vxnneLayer_SetOperation(
                &tensor_squeeze_layer->base,
                &tensor_squeeze_layer->tensor_squeeze_sw_operation.base,
                0);

            tensor_squeeze_layer->tensor_squeeze_sw_operation.input         = input;
            tensor_squeeze_layer->tensor_squeeze_sw_operation.squeeze_dims  = squeeze_dims;
            tensor_squeeze_layer->tensor_squeeze_sw_operation.output        = output;

            vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_sw_operation.base, (vx_reference)input, VXNNE_OPERATION_REFENRENCE_INPUT);
            vxnneOperation_AddReference(&tensor_squeeze_layer->tensor_squeeze_sw_operation.base, (vx_reference)output, VXNNE_OPERATION_REFENRENCE_OUTPUT);
        }
    }

    node->layer = &tensor_squeeze_layer->base;
    return status;

exit:
    if (tensor_squeeze_layer)
        gcoOS_Free(NULL, (gctPOINTER)tensor_squeeze_layer);

    return status;
}

VX_PRIVATE_API vx_status VX_CALLBACK vxoNNTensorSqueeze_Deinitializer(vx_node node, const vx_reference *parameters, vx_uint32 num)
{
    if (node->layer)
    {
        vxnneLayer_Free(node->layer);
    }

    return VX_SUCCESS;
}

